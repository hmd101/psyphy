{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"psyphy","text":"<p> Psychophysical modeling and adaptive trial placement.</p>"},{"location":"#install","title":"Install","text":"<pre><code># in your project root\npython -m venv .venv\nsource .venv/bin/activate\npip install -e .\n</code></pre>"},{"location":"#what-is-in-psyphy","title":"What is in psyphy?","text":"<ul> <li>Models: WPPM, priors, noise, task likelihoods</li> <li>Inference: MAP, Langevin, Laplace</li> <li>Posterior: wrappers and diagnostics</li> <li>Trial placement: Placement strategies (acquisition functions) (e.g., grid, staircase, info gain)</li> <li>Session: end-to-end orchestration of an experiment</li> </ul>"},{"location":"CONTRIBUTING/","title":"Contributing","text":""},{"location":"CONTRIBUTING/#dev-setup","title":"Dev setup","text":"<pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install -r requirements.txt\npip install -e .\n</code></pre>"},{"location":"CONTRIBUTING/#lint-test","title":"Lint &amp; test","text":"<pre><code>ruff check .\nblack --check .\npytest -q\n</code></pre>"},{"location":"CONTRIBUTING/#docs-build-serve","title":"Docs: build &amp; serve","text":"<p>Install doc dependencies:</p> <pre><code>pip install mkdocs mkdocs-material mkdocstrings[python]\n</code></pre> <p>Build/serve locally (auto-reload):</p> <pre><code>mkdocs serve\n</code></pre> <p>Deploy (manual):</p> <pre><code>mkdocs gh-deploy --clean\n</code></pre> <p>We use NumPy-style docstrings for API reference via mkdocstrings.</p>"},{"location":"reference/","title":"API Reference","text":"<p>This page has moved. See the structured API pages:</p> <ul> <li>API Overview</li> <li>Data</li> <li>Model</li> <li>Inference</li> <li>Posterior</li> <li>Trial Placement</li> <li>Session</li> <li>Utils</li> </ul>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#quick-start","title":"Quick start","text":"<pre><code>import jax\nimport jax.numpy as jnp\nimport optax\n\nfrom psyphy.data.dataset import ResponseData\nfrom psyphy.model import WPPM, Prior, OddityTask, GaussianNoise\nfrom psyphy.inference.map_optimizer import MAPOptimizer\nfrom psyphy.trial_placement.grid import GridPlacement\nfrom psyphy.session.experiment_session import ExperimentSession\n\n# -----------------------\n# 1) Define the model\n# -----------------------\nprior_dist = Prior.default(input_dim=2)\ndecision_task = OddityTask(slope=1.5)\nobserver_noise = GaussianNoise(sigma=1.0)\n\nwppm_model = WPPM(\n    input_dim=2,\n    prior=prior_dist,\n    task=decision_task,\n    noise=observer_noise,\n)\n\n# -----------------------\n# 2) Choose optimizer and MAP settings\n#    (explicitly configure Optax SGD + momentum)\n# -----------------------\nlearning_rate = 5e-4\nmomentum = 0.9\nsgd_momentum = optax.sgd(learning_rate=learning_rate, momentum=momentum)\n\nmap_estimator = MAPOptimizer(\n    steps=500,\n    optimizer=sgd_momentum,\n)\n\n# -----------------------\n# 3) Trial placement strategy (stubbed)\n# -----------------------\ntrial_placement = GridPlacement(grid_points=[(0.0, 0.0)])  # MVP placeholder\n\n# -----------------------\n# 4) Orchestrate an experiment session\n# -----------------------\nsession = ExperimentSession(wppm_model, map_estimator, trial_placement)\n\n# Initialize posterior (before any data)\nposterior = session.initialize()\n\n# Propose a batch of trials and collect responses (simulated/user-provided)\nproposed_batch = session.next_batch(batch_size=5)\n# responses = run_subject(proposed_batch) \n# session.data.add_batch(responses, proposed_batch)\n\n# Update posterior after adding data\nposterior = session.update()\n\n# Predict threshold contour around a reference (placeholder unit circle in MVP)\nreference_point = jnp.array([0.0, 0.0])\nthreshold_contour = posterior.predict_thresholds(\n    reference=reference_point,\n    criterion=0.667,\n    directions=16,\n)\n</code></pre>"},{"location":"usage/#alternative-offline-fit-without-the-session","title":"Alternative: Offline fit without the session","text":"<p>If you already have data and just want to fit and predict without the experiment orchestrator:</p> <pre><code>from psyphy.data.dataset import ResponseData\nfrom psyphy.model import WPPM, Prior, TwoAFC\nfrom psyphy.inference.map_optimizer import MAPOptimizer\nimport optax\nimport jax.numpy as jnp\n\n# Prepare data\n# Create an empty container for trials (reference, probe, response)\ndata = ResponseData()\n\n# Add one trial:\n# - ref: reference stimulus (shape: (input_dim,))\n# - probe: probe stimulus (same shape as ref)\n# - resp: binary response in {0, 1}; TwoAFC log-likelihood treats 1 as \"correct\"\ndata.add_trial(ref=jnp.array([0.0, 0.0]), probe=jnp.array([0.1, 0.0]), resp=1)\n\n# Add another trial (subject responded 0 = \"incorrect\")\ndata.add_trial(ref=jnp.array([0.0, 0.0]), probe=jnp.array([0.0, 0.1]), resp=0)\n\n# Model\nmodel = WPPM(input_dim=2, prior=Prior.default(2), task=TwoAFC())\n\n# Optimizer config (SGD + momentum)\nopt = optax.sgd(learning_rate=5e-4, momentum=0.9)\nposterior = MAPOptimizer(steps=500, optimizer=opt).fit(model, data)\n\n# Predictions\np = posterior.predict_prob((jnp.array([0.0, 0.0]), jnp.array([0.05, 0.05])))\ncontour = posterior.predict_thresholds(reference=jnp.array([0.0, 0.0]))\n</code></pre>"},{"location":"examples/mvp/offline_fit_mvp/","title":"MVP Offline Fit","text":"<p>This example fits the MVP WPPM model to synthetic 2D data and saves two plots:</p> <ul> <li>Thresholds around the reference (ground-truth, init, fitted)</li> <li>Learning curve (negative log posterior vs steps)</li> </ul> <p>Generated plots (from running <code>docs/examples/mvp/offline_fit_mvp.py</code>):</p> <p></p> <p></p> <p>Run the underlying script:</p> <pre><code>python docs/examples/mvp/offline_fit_mvp.py\n</code></pre> <p>The script writes plots into <code>docs/examples/mvp/plots/</code> and this page embeds them.</p>"},{"location":"reference/data/","title":"Data","text":""},{"location":"reference/data/#package","title":"Package","text":""},{"location":"reference/data/#psyphy.data","title":"data","text":""},{"location":"reference/data/#psyphy.data--psyphydata","title":"psyphy.data","text":"<p>submodule for handling psychophysical experiment data.</p> <p>Includes: - dataset: ResponseData, TrialBatch, loaders - transforms: color/model space conversions - io: save/load datasets</p> <p>Classes:</p> Name Description <code>ResponseData</code> <p>Container for psychophysical trial data.</p> <code>TrialBatch</code> <p>Container for a proposed batch of trials</p>"},{"location":"reference/data/#psyphy.data.ResponseData","title":"ResponseData","text":"<pre><code>ResponseData()\n</code></pre> <p>Container for psychophysical trial data.</p> <p>Attributes:</p> Name Type Description <code>refs</code> <code>List[Any]</code> <p>List of reference stimuli.</p> <code>probes</code> <code>List[Any]</code> <p>List of probe stimuli.</p> <code>responses</code> <code>List[int]</code> <p>List of subject responses (e.g., 0/1 or categorical).</p> <p>Methods:</p> Name Description <code>add_batch</code> <p>Append responses for a batch of trials.</p> <code>add_trial</code> <p>append a single trial.</p> <code>to_numpy</code> <p>Return refs, probes, responses as numpy arrays.</p> Source code in <code>src/psyphy/data/dataset.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.refs: List[Any] = []\n    self.probes: List[Any] = []\n    self.responses: List[int] = []\n</code></pre>"},{"location":"reference/data/#psyphy.data.ResponseData.probes","title":"probes","text":"<pre><code>probes: List[Any] = []\n</code></pre>"},{"location":"reference/data/#psyphy.data.ResponseData.refs","title":"refs","text":"<pre><code>refs: List[Any] = []\n</code></pre>"},{"location":"reference/data/#psyphy.data.ResponseData.responses","title":"responses","text":"<pre><code>responses: List[int] = []\n</code></pre>"},{"location":"reference/data/#psyphy.data.ResponseData.add_batch","title":"add_batch","text":"<pre><code>add_batch(\n    responses: List[int], trial_batch: TrialBatch\n) -&gt; None\n</code></pre> <p>Append responses for a batch of trials.</p> <p>Parameters:</p> Name Type Description Default <code>responses</code> <code>List[int]</code> <p>Responses corresponding to each (ref, probe) in the trial batch.</p> required <code>trial_batch</code> <code>TrialBatch</code> <p>The batch of proposed trials.</p> required Source code in <code>src/psyphy/data/dataset.py</code> <pre><code>def add_batch(self, responses: List[int], trial_batch: TrialBatch) -&gt; None:\n    \"\"\"\n    Append responses for a batch of trials.\n\n    Parameters\n    ----------\n    responses : List[int]\n        Responses corresponding to each (ref, probe) in the trial batch.\n    trial_batch : TrialBatch\n        The batch of proposed trials.\n    \"\"\"\n    for (ref, probe), resp in zip(trial_batch.stimuli, responses):\n        self.add_trial(ref, probe, resp)\n</code></pre>"},{"location":"reference/data/#psyphy.data.ResponseData.add_trial","title":"add_trial","text":"<pre><code>add_trial(ref: Any, probe: Any, resp: int) -&gt; None\n</code></pre> <p>append a single trial.</p> <p>Parameters:</p> Name Type Description Default <code>ref</code> <code>Any</code> <p>Reference stimulus (numpy array, list, etc.)</p> required <code>probe</code> <code>Any</code> <p>Probe stimulus</p> required <code>resp</code> <code>int</code> <p>Subject response (binary or categorical)</p> required Source code in <code>src/psyphy/data/dataset.py</code> <pre><code>def add_trial(self, ref: Any, probe: Any, resp: int) -&gt; None:\n    \"\"\"\n    append a single trial.\n\n    Parameters\n    ----------\n    ref : Any\n        Reference stimulus (numpy array, list, etc.)\n    probe : Any\n        Probe stimulus\n    resp : int\n        Subject response (binary or categorical)\n    \"\"\"\n    self.refs.append(ref)\n    self.probes.append(probe)\n    self.responses.append(resp)\n</code></pre>"},{"location":"reference/data/#psyphy.data.ResponseData.to_numpy","title":"to_numpy","text":"<pre><code>to_numpy() -&gt; Tuple[ndarray, ndarray, ndarray]\n</code></pre> <p>Return refs, probes, responses as numpy arrays.</p> <p>Returns:</p> Name Type Description <code>refs</code> <code>ndarray</code> <code>probes</code> <code>ndarray</code> <code>responses</code> <code>ndarray</code> Source code in <code>src/psyphy/data/dataset.py</code> <pre><code>def to_numpy(self) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Return refs, probes, responses as numpy arrays.\n\n    Returns\n    -------\n    refs : np.ndarray\n    probes : np.ndarray\n    responses : np.ndarray\n    \"\"\"\n    return (\n        np.array(self.refs),\n        np.array(self.probes),\n        np.array(self.responses),\n    )\n</code></pre>"},{"location":"reference/data/#psyphy.data.TrialBatch","title":"TrialBatch","text":"<pre><code>TrialBatch(stimuli: List[Tuple[Any, Any]])\n</code></pre> <p>Container for a proposed batch of trials</p> <p>Attributes:</p> Name Type Description <code>stimuli</code> <code>List[Tuple[Any, Any]]</code> <p>Each trial is a (reference, probe) tuple.</p> <p>Methods:</p> Name Description <code>from_stimuli</code> <p>Construct a TrialBatch from a list of stimuli (ref, probe) pairs.</p> Source code in <code>src/psyphy/data/dataset.py</code> <pre><code>def __init__(self, stimuli: List[Tuple[Any, Any]]) -&gt; None:\n    self.stimuli = list(stimuli)\n</code></pre>"},{"location":"reference/data/#psyphy.data.TrialBatch.stimuli","title":"stimuli","text":"<pre><code>stimuli = list(stimuli)\n</code></pre>"},{"location":"reference/data/#psyphy.data.TrialBatch.from_stimuli","title":"from_stimuli","text":"<pre><code>from_stimuli(pairs: List[Tuple[Any, Any]]) -&gt; TrialBatch\n</code></pre> <p>Construct a TrialBatch from a list of stimuli (ref, probe) pairs.</p> Source code in <code>src/psyphy/data/dataset.py</code> <pre><code>@classmethod\ndef from_stimuli(cls, pairs: List[Tuple[Any, Any]]) -&gt; TrialBatch:\n    \"\"\"\n    Construct a TrialBatch from a list of stimuli (ref, probe) pairs.\n    \"\"\"\n    return cls(pairs)\n</code></pre>"},{"location":"reference/data/#data-containers-for-response-data-and-proposed-next-trials","title":"Data Containers for Response Data and Proposed Next Trials","text":""},{"location":"reference/data/#psyphy.data.dataset","title":"dataset","text":"dataset.py <p>Core data containers for psyphy.</p> <p>defines: - ResponseData: container for psychophysical trial data - TrialBatch: container for a proposed batch of trials</p> Notes <ul> <li>Data is stored in standard NumPy (mutable!) arrays or Python lists.</li> <li>Use numpy for I/O and analysis.</li> <li>Convert to jax.numpy (jnp) (immutable!) arrays only when passing into WPPM   or inference engines that require JAX/Optax.</li> </ul> <p>Classes:</p> Name Description <code>ResponseData</code> <p>Container for psychophysical trial data.</p> <code>TrialBatch</code> <p>Container for a proposed batch of trials</p>"},{"location":"reference/data/#psyphy.data.dataset.ResponseData","title":"ResponseData","text":"<pre><code>ResponseData()\n</code></pre> <p>Container for psychophysical trial data.</p> <p>Attributes:</p> Name Type Description <code>refs</code> <code>List[Any]</code> <p>List of reference stimuli.</p> <code>probes</code> <code>List[Any]</code> <p>List of probe stimuli.</p> <code>responses</code> <code>List[int]</code> <p>List of subject responses (e.g., 0/1 or categorical).</p> <p>Methods:</p> Name Description <code>add_batch</code> <p>Append responses for a batch of trials.</p> <code>add_trial</code> <p>append a single trial.</p> <code>to_numpy</code> <p>Return refs, probes, responses as numpy arrays.</p> Source code in <code>src/psyphy/data/dataset.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.refs: List[Any] = []\n    self.probes: List[Any] = []\n    self.responses: List[int] = []\n</code></pre>"},{"location":"reference/data/#psyphy.data.dataset.ResponseData.probes","title":"probes","text":"<pre><code>probes: List[Any] = []\n</code></pre>"},{"location":"reference/data/#psyphy.data.dataset.ResponseData.refs","title":"refs","text":"<pre><code>refs: List[Any] = []\n</code></pre>"},{"location":"reference/data/#psyphy.data.dataset.ResponseData.responses","title":"responses","text":"<pre><code>responses: List[int] = []\n</code></pre>"},{"location":"reference/data/#psyphy.data.dataset.ResponseData.add_batch","title":"add_batch","text":"<pre><code>add_batch(\n    responses: List[int], trial_batch: TrialBatch\n) -&gt; None\n</code></pre> <p>Append responses for a batch of trials.</p> <p>Parameters:</p> Name Type Description Default <code>responses</code> <code>List[int]</code> <p>Responses corresponding to each (ref, probe) in the trial batch.</p> required <code>trial_batch</code> <code>TrialBatch</code> <p>The batch of proposed trials.</p> required Source code in <code>src/psyphy/data/dataset.py</code> <pre><code>def add_batch(self, responses: List[int], trial_batch: TrialBatch) -&gt; None:\n    \"\"\"\n    Append responses for a batch of trials.\n\n    Parameters\n    ----------\n    responses : List[int]\n        Responses corresponding to each (ref, probe) in the trial batch.\n    trial_batch : TrialBatch\n        The batch of proposed trials.\n    \"\"\"\n    for (ref, probe), resp in zip(trial_batch.stimuli, responses):\n        self.add_trial(ref, probe, resp)\n</code></pre>"},{"location":"reference/data/#psyphy.data.dataset.ResponseData.add_trial","title":"add_trial","text":"<pre><code>add_trial(ref: Any, probe: Any, resp: int) -&gt; None\n</code></pre> <p>append a single trial.</p> <p>Parameters:</p> Name Type Description Default <code>ref</code> <code>Any</code> <p>Reference stimulus (numpy array, list, etc.)</p> required <code>probe</code> <code>Any</code> <p>Probe stimulus</p> required <code>resp</code> <code>int</code> <p>Subject response (binary or categorical)</p> required Source code in <code>src/psyphy/data/dataset.py</code> <pre><code>def add_trial(self, ref: Any, probe: Any, resp: int) -&gt; None:\n    \"\"\"\n    append a single trial.\n\n    Parameters\n    ----------\n    ref : Any\n        Reference stimulus (numpy array, list, etc.)\n    probe : Any\n        Probe stimulus\n    resp : int\n        Subject response (binary or categorical)\n    \"\"\"\n    self.refs.append(ref)\n    self.probes.append(probe)\n    self.responses.append(resp)\n</code></pre>"},{"location":"reference/data/#psyphy.data.dataset.ResponseData.to_numpy","title":"to_numpy","text":"<pre><code>to_numpy() -&gt; Tuple[ndarray, ndarray, ndarray]\n</code></pre> <p>Return refs, probes, responses as numpy arrays.</p> <p>Returns:</p> Name Type Description <code>refs</code> <code>ndarray</code> <code>probes</code> <code>ndarray</code> <code>responses</code> <code>ndarray</code> Source code in <code>src/psyphy/data/dataset.py</code> <pre><code>def to_numpy(self) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Return refs, probes, responses as numpy arrays.\n\n    Returns\n    -------\n    refs : np.ndarray\n    probes : np.ndarray\n    responses : np.ndarray\n    \"\"\"\n    return (\n        np.array(self.refs),\n        np.array(self.probes),\n        np.array(self.responses),\n    )\n</code></pre>"},{"location":"reference/data/#psyphy.data.dataset.TrialBatch","title":"TrialBatch","text":"<pre><code>TrialBatch(stimuli: List[Tuple[Any, Any]])\n</code></pre> <p>Container for a proposed batch of trials</p> <p>Attributes:</p> Name Type Description <code>stimuli</code> <code>List[Tuple[Any, Any]]</code> <p>Each trial is a (reference, probe) tuple.</p> <p>Methods:</p> Name Description <code>from_stimuli</code> <p>Construct a TrialBatch from a list of stimuli (ref, probe) pairs.</p> Source code in <code>src/psyphy/data/dataset.py</code> <pre><code>def __init__(self, stimuli: List[Tuple[Any, Any]]) -&gt; None:\n    self.stimuli = list(stimuli)\n</code></pre>"},{"location":"reference/data/#psyphy.data.dataset.TrialBatch.stimuli","title":"stimuli","text":"<pre><code>stimuli = list(stimuli)\n</code></pre>"},{"location":"reference/data/#psyphy.data.dataset.TrialBatch.from_stimuli","title":"from_stimuli","text":"<pre><code>from_stimuli(pairs: List[Tuple[Any, Any]]) -&gt; TrialBatch\n</code></pre> <p>Construct a TrialBatch from a list of stimuli (ref, probe) pairs.</p> Source code in <code>src/psyphy/data/dataset.py</code> <pre><code>@classmethod\ndef from_stimuli(cls, pairs: List[Tuple[Any, Any]]) -&gt; TrialBatch:\n    \"\"\"\n    Construct a TrialBatch from a list of stimuli (ref, probe) pairs.\n    \"\"\"\n    return cls(pairs)\n</code></pre>"},{"location":"reference/data/#transforms-eg-from-rgb-to-model-space","title":"Transforms (e.g., from RGB to model space)","text":""},{"location":"reference/data/#psyphy.data.transforms","title":"transforms","text":"transforms.py <p>color space transformations.</p> <p>functions (MVP stubs): - to_model_space(rgb): map RGB stimulus values into model space - to_rgb(model_coords): map from model space back to RGB</p> <p>Future extensions: - other color spaces</p> <p>Functions:</p> Name Description <code>model_to_stimuli</code> <p>Map from model space coordinates to RGB values.</p> <code>stimuli_to_model_space</code> <p>Map RGB stimulus values to model space coordinates.</p> <p>Attributes:</p> Name Type Description <code>ArrayLike</code>"},{"location":"reference/data/#psyphy.data.transforms.ArrayLike","title":"ArrayLike","text":"<pre><code>ArrayLike = Union[Sequence[float], ndarray]\n</code></pre>"},{"location":"reference/data/#psyphy.data.transforms.model_to_stimuli","title":"model_to_stimuli","text":"<pre><code>model_to_stimuli(model_coords: ArrayLike) -&gt; ndarray\n</code></pre> <p>Map from model space coordinates to RGB values.</p> <p>Parameters:</p> Name Type Description Default <code>model_coords</code> <code>array - like</code> <p>Model space coordinates.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>RGB values (MVP: identical to input).</p> Source code in <code>src/psyphy/data/transforms.py</code> <pre><code>def model_to_stimuli(model_coords: ArrayLike) -&gt; np.ndarray:\n    \"\"\"\n    Map from model space coordinates to RGB values.\n\n    Parameters\n    ----------\n    model_coords : array-like\n        Model space coordinates.\n\n    Returns\n    -------\n    np.ndarray\n        RGB values (MVP: identical to input).\n    \"\"\"\n    return np.array(model_coords)\n</code></pre>"},{"location":"reference/data/#psyphy.data.transforms.stimuli_to_model_space","title":"stimuli_to_model_space","text":"<pre><code>stimuli_to_model_space(rgb: ArrayLike) -&gt; ndarray\n</code></pre> <p>Map RGB stimulus values to model space coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>rgb</code> <code>array - like</code> <p>RGB values, shape (3,) or similar.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Model-space coordinates (MVP: identical to input).</p> Source code in <code>src/psyphy/data/transforms.py</code> <pre><code>def stimuli_to_model_space(rgb: ArrayLike) -&gt; np.ndarray:\n    \"\"\"\n    Map RGB stimulus values to model space coordinates.\n\n    Parameters\n    ----------\n    rgb : array-like\n        RGB values, shape (3,) or similar.\n\n    Returns\n    -------\n    np.ndarray\n        Model-space coordinates (MVP: identical to input).\n    \"\"\"\n    return np.array(rgb)\n</code></pre>"},{"location":"reference/data/#io","title":"I/O","text":""},{"location":"reference/data/#psyphy.data.io","title":"io","text":"io.py <p>I/O utilities for saving and loading psyphy data.</p> <p>Supports: - CSV for human-readable trial logs - Pickle (.pkl) for Posterior and ResponseData checkpoints?</p> Notes <ul> <li>Data is stored in NumPy arrays (via ResponseData.to_numpy()).</li> <li>Convert to jax.numpy when passing into models.</li> </ul> <p>Functions:</p> Name Description <code>load_posterior</code> <p>Load a Posterior object from pickle.</p> <code>load_responses_csv</code> <p>Load ResponseData from a CSV file.</p> <code>save_posterior</code> <p>Save a Posterior object to disk using pickle.</p> <code>save_responses_csv</code> <p>Save ResponseData to a CSV file.</p> <p>Attributes:</p> Name Type Description <code>PathLike</code>"},{"location":"reference/data/#psyphy.data.io.PathLike","title":"PathLike","text":"<pre><code>PathLike = Union[str, Path]\n</code></pre>"},{"location":"reference/data/#psyphy.data.io.load_posterior","title":"load_posterior","text":"<pre><code>load_posterior(path: PathLike) -&gt; object\n</code></pre> <p>Load a Posterior object from pickle.</p> Source code in <code>src/psyphy/data/io.py</code> <pre><code>def load_posterior(path: PathLike) -&gt; object:\n    \"\"\"\n    Load a Posterior object from pickle.\n    \"\"\"\n    with open(path, \"rb\") as f:\n        return pickle.load(f)\n</code></pre>"},{"location":"reference/data/#psyphy.data.io.load_responses_csv","title":"load_responses_csv","text":"<pre><code>load_responses_csv(path: PathLike) -&gt; ResponseData\n</code></pre> <p>Load ResponseData from a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str or Path</code> required <p>Returns:</p> Type Description <code>ResponseData</code> Source code in <code>src/psyphy/data/io.py</code> <pre><code>def load_responses_csv(path: PathLike) -&gt; ResponseData:\n    \"\"\"\n    Load ResponseData from a CSV file.\n\n    Parameters\n    ----------\n    path : str or Path\n\n    Returns\n    -------\n    ResponseData\n    \"\"\"\n    data = ResponseData()\n    with open(path) as f:\n        reader = csv.DictReader(f)\n        for row in reader:\n            ref = ast.literal_eval(row[\"ref\"])\n            probe = ast.literal_eval(row[\"probe\"])\n            resp = int(row[\"response\"])\n            data.add_trial(ref, probe, resp)\n    return data\n</code></pre>"},{"location":"reference/data/#psyphy.data.io.save_posterior","title":"save_posterior","text":"<pre><code>save_posterior(posterior: object, path: PathLike) -&gt; None\n</code></pre> <p>Save a Posterior object to disk using pickle.</p> Source code in <code>src/psyphy/data/io.py</code> <pre><code>def save_posterior(posterior: object, path: PathLike) -&gt; None:\n    \"\"\"\n    Save a Posterior object to disk using pickle.\n    \"\"\"\n    with open(path, \"wb\") as f:\n        pickle.dump(posterior, f)\n</code></pre>"},{"location":"reference/data/#psyphy.data.io.save_responses_csv","title":"save_responses_csv","text":"<pre><code>save_responses_csv(\n    data: ResponseData, path: PathLike\n) -&gt; None\n</code></pre> <p>Save ResponseData to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ResponseData</code> required <code>path</code> <code>str or Path</code> required Source code in <code>src/psyphy/data/io.py</code> <pre><code>def save_responses_csv(data: ResponseData, path: PathLike) -&gt; None:\n    \"\"\"\n    Save ResponseData to a CSV file.\n\n    Parameters\n    ----------\n    data : ResponseData\n    path : str or Path\n    \"\"\"\n    refs, probes, resps = data.to_numpy()\n    with open(path, \"w\", newline=\"\") as f:\n        writer = csv.writer(f)\n        writer.writerow([\"ref\", \"probe\", \"response\"])\n        for r, p, y in zip(refs, probes, resps):\n            writer.writerow([r.tolist(), p.tolist(), int(y)])\n</code></pre>"},{"location":"reference/inference/","title":"Inference","text":""},{"location":"reference/inference/#package","title":"Package","text":""},{"location":"reference/inference/#psyphy.inference","title":"inference","text":""},{"location":"reference/inference/#psyphy.inference--inference","title":"inference","text":"<p>Inference engines for WPPM.</p> <p>This subpackage provides different strategies for fitting model parameters to data and returning posterior objects.</p> MVP implementations <ul> <li>MAPOptimizer : maximum a posteriori fit with Optax optimizers.</li> <li>LaplaceApproximation : approximate posterior covariance around MAP.</li> <li>LangevinSampler : skeleton for sampling-based inference.</li> </ul> Future extensions <ul> <li>adjusted MC samplers, e.g., MALA (for Bayesian posterior inference).</li> </ul> <p>Classes:</p> Name Description <code>InferenceEngine</code> <p>Abstract interface for inference engines.</p> <code>LangevinSampler</code> <p>Langevin sampler (stub).</p> <code>LaplaceApproximation</code> <p>Laplace approximation around MAP estimate.</p> <code>MAPOptimizer</code> <p>MAP (Maximum A Posteriori) optimizer.</p>"},{"location":"reference/inference/#psyphy.inference.InferenceEngine","title":"InferenceEngine","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract interface for inference engines.</p> <p>Methods:</p> Name Description <code>fit</code> <p>Fit model parameters to data and return a Posterior object.</p>"},{"location":"reference/inference/#psyphy.inference.InferenceEngine.fit","title":"fit","text":"<pre><code>fit(model: Any, data: Any) -&gt; Any\n</code></pre> <p>Fit model parameters to data.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>WPPM</code> <p>Psychophysical model to fit.</p> required <code>data</code> <code>ResponseData</code> <p>Observed trials.</p> required <p>Returns:</p> Type Description <code>Posterior</code> <p>Posterior object wrapping fitted params and model reference.</p> Source code in <code>src/psyphy/inference/base.py</code> <pre><code>@abstractmethod\ndef fit(self, model: Any, data: Any) -&gt; Any:\n    \"\"\"\n    Fit model parameters to data.\n\n    Parameters\n    ----------\n    model : WPPM\n        Psychophysical model to fit.\n    data : ResponseData\n        Observed trials.\n\n    Returns\n    -------\n    Posterior\n        Posterior object wrapping fitted params and model reference.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/inference/#psyphy.inference.LangevinSampler","title":"LangevinSampler","text":"<pre><code>LangevinSampler(\n    steps: int = 1000,\n    step_size: float = 0.001,\n    temperature: float = 1.0,\n)\n</code></pre> <p>Langevin sampler (stub).</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of Langevin steps.</p> <code>1000</code> <code>step_size</code> <code>float</code> <p>Integration step size.</p> <code>1e-3</code> <code>temperature</code> <code>float</code> <p>Noise scale (temperature).</p> <code>1.0</code> <p>Methods:</p> Name Description <code>fit</code> <p>Fit model parameters with Langevin dynamics (stub).</p> <p>Attributes:</p> Name Type Description <code>step_size</code> <code>steps</code> <code>temperature</code> Source code in <code>src/psyphy/inference/langevin.py</code> <pre><code>def __init__(self, steps: int = 1000, step_size: float = 1e-3, temperature: float = 1.0):\n    self.steps = steps\n    self.step_size = step_size\n    self.temperature = temperature\n</code></pre>"},{"location":"reference/inference/#psyphy.inference.LangevinSampler.step_size","title":"step_size","text":"<pre><code>step_size = step_size\n</code></pre>"},{"location":"reference/inference/#psyphy.inference.LangevinSampler.steps","title":"steps","text":"<pre><code>steps = steps\n</code></pre>"},{"location":"reference/inference/#psyphy.inference.LangevinSampler.temperature","title":"temperature","text":"<pre><code>temperature = temperature\n</code></pre>"},{"location":"reference/inference/#psyphy.inference.LangevinSampler.fit","title":"fit","text":"<pre><code>fit(model, data) -&gt; Posterior\n</code></pre> <p>Fit model parameters with Langevin dynamics (stub).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>WPPM</code> <p>Model instance.</p> required <code>data</code> <code>ResponseData</code> <p>Observed trials.</p> required <p>Returns:</p> Type Description <code>Posterior</code> <p>Posterior wrapper (MVP: params from init).</p> Source code in <code>src/psyphy/inference/langevin.py</code> <pre><code>def fit(self, model, data) -&gt; Posterior:\n    \"\"\"\n    Fit model parameters with Langevin dynamics (stub).\n\n    Parameters\n    ----------\n    model : WPPM\n        Model instance.\n    data : ResponseData\n        Observed trials.\n\n    Returns\n    -------\n    Posterior\n        Posterior wrapper (MVP: params from init).\n    \"\"\"\n    return Posterior(params=model.init_params(None), model=model)\n</code></pre>"},{"location":"reference/inference/#psyphy.inference.LaplaceApproximation","title":"LaplaceApproximation","text":"<p>Laplace approximation around MAP estimate.</p> <p>Methods:</p> Name Description <code>from_map</code> <p>Construct a Gaussian approximation centered at MAP.</p>"},{"location":"reference/inference/#psyphy.inference.LaplaceApproximation.from_map","title":"from_map","text":"<pre><code>from_map(map_posterior: Posterior) -&gt; Posterior\n</code></pre> <p>Return posterior approximation from MAP.</p> <p>Parameters:</p> Name Type Description Default <code>map_posterior</code> <code>Posterior</code> <p>Posterior object from MAP optimization.</p> required <p>Returns:</p> Type Description <code>Posterior</code> <p>Same posterior object (MVP).</p> Source code in <code>src/psyphy/inference/laplace.py</code> <pre><code>def from_map(self, map_posterior: Posterior) -&gt; Posterior:\n    \"\"\"\n    Return posterior approximation from MAP.\n\n    Parameters\n    ----------\n    map_posterior : Posterior\n        Posterior object from MAP optimization.\n\n    Returns\n    -------\n    Posterior\n        Same posterior object (MVP).\n    \"\"\"\n    return map_posterior\n</code></pre>"},{"location":"reference/inference/#psyphy.inference.MAPOptimizer","title":"MAPOptimizer","text":"<pre><code>MAPOptimizer(\n    steps: int = 500,\n    optimizer: GradientTransformation | None = None,\n)\n</code></pre> <p>               Bases: <code>InferenceEngine</code></p> <p>MAP (Maximum A Posteriori) optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of optimization steps.</p> <code>500</code> <code>optimizer</code> <code>GradientTransformation</code> <p>Optax optimizer to use. Default: SGD with momentum.</p> <code>None</code> Notes <ul> <li>Loss function = negative log posterior.</li> <li>Gradients computed with jax.grad.</li> </ul> <p>Methods:</p> Name Description <code>fit</code> <p>Fit model parameters with MAP optimization.</p> <p>Attributes:</p> Name Type Description <code>optimizer</code> <code>steps</code> Source code in <code>src/psyphy/inference/map_optimizer.py</code> <pre><code>def __init__(self, steps: int = 500, optimizer: optax.GradientTransformation | None = None):\n    self.steps = steps\n    self.optimizer = optimizer or optax.sgd(learning_rate=5e-5, momentum=0.9)\n</code></pre>"},{"location":"reference/inference/#psyphy.inference.MAPOptimizer.optimizer","title":"optimizer","text":"<pre><code>optimizer = optimizer or sgd(\n    learning_rate=5e-05, momentum=0.9\n)\n</code></pre>"},{"location":"reference/inference/#psyphy.inference.MAPOptimizer.steps","title":"steps","text":"<pre><code>steps = steps\n</code></pre>"},{"location":"reference/inference/#psyphy.inference.MAPOptimizer.fit","title":"fit","text":"<pre><code>fit(model, data) -&gt; Posterior\n</code></pre> <p>Fit model parameters with MAP optimization.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>WPPM</code> <p>Model instance.</p> required <code>data</code> <code>ResponseData</code> <p>Observed trials.</p> required <p>Returns:</p> Type Description <code>Posterior</code> <p>Posterior wrapper around MAP params and model.</p> Source code in <code>src/psyphy/inference/map_optimizer.py</code> <pre><code>def fit(self, model, data) -&gt; Posterior:\n    \"\"\"\n    Fit model parameters with MAP optimization.\n\n    Parameters\n    ----------\n    model : WPPM\n        Model instance.\n    data : ResponseData\n        Observed trials.\n\n    Returns\n    -------\n    Posterior\n        Posterior wrapper around MAP params and model.\n    \"\"\"\n\n    def loss_fn(params):\n        return -model.log_posterior_from_data(params, data)\n\n    params = model.init_params(jax.random.PRNGKey(0))\n    opt_state = self.optimizer.init(params)\n\n    @jax.jit\n    def step(params, opt_state):\n        loss, grads = jax.value_and_grad(loss_fn)(params)\n        updates, opt_state = self.optimizer.update(grads, opt_state, params)\n        params = optax.apply_updates(params, updates)\n        return params, opt_state, loss\n\n    for _ in range(self.steps):\n        params, opt_state, loss = step(params, opt_state)\n\n    return Posterior(params=params, model=model)\n</code></pre>"},{"location":"reference/inference/#base","title":"Base","text":""},{"location":"reference/inference/#psyphy.inference.base","title":"base","text":"base.py <p>Abstract base class for inference engines.</p> <p>All inference engines must implement a <code>fit(model, data)</code> method that returns a Posterior object.</p> <p>All inference engines (MAPOptimizer, LangevinSampler, LaplaceApproximation) subclass from this base.</p> <p>Classes:</p> Name Description <code>InferenceEngine</code> <p>Abstract interface for inference engines.</p>"},{"location":"reference/inference/#psyphy.inference.base.InferenceEngine","title":"InferenceEngine","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract interface for inference engines.</p> <p>Methods:</p> Name Description <code>fit</code> <p>Fit model parameters to data and return a Posterior object.</p>"},{"location":"reference/inference/#psyphy.inference.base.InferenceEngine.fit","title":"fit","text":"<pre><code>fit(model: Any, data: Any) -&gt; Any\n</code></pre> <p>Fit model parameters to data.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>WPPM</code> <p>Psychophysical model to fit.</p> required <code>data</code> <code>ResponseData</code> <p>Observed trials.</p> required <p>Returns:</p> Type Description <code>Posterior</code> <p>Posterior object wrapping fitted params and model reference.</p> Source code in <code>src/psyphy/inference/base.py</code> <pre><code>@abstractmethod\ndef fit(self, model: Any, data: Any) -&gt; Any:\n    \"\"\"\n    Fit model parameters to data.\n\n    Parameters\n    ----------\n    model : WPPM\n        Psychophysical model to fit.\n    data : ResponseData\n        Observed trials.\n\n    Returns\n    -------\n    Posterior\n        Posterior object wrapping fitted params and model reference.\n    \"\"\"\n    ...\n</code></pre>"},{"location":"reference/inference/#map-optimizer","title":"MAP Optimizer","text":""},{"location":"reference/inference/#psyphy.inference.map_optimizer","title":"map_optimizer","text":"map_optimizer.py <p>MAP (Maximum A Posteriori) optimizer using Optax.</p> <p>MVP implementation: - Uses gradient ascent on log posterior. - Defaults to SGD with momentum, but any Optax optimizer can be passed in.</p> Connections <ul> <li>Calls WPPM.log_posterior_from_data(params, data) as the objective.</li> <li>Returns a Posterior object wrapping the MAP estimate.</li> </ul> <p>Classes:</p> Name Description <code>MAPOptimizer</code> <p>MAP (Maximum A Posteriori) optimizer.</p>"},{"location":"reference/inference/#psyphy.inference.map_optimizer.MAPOptimizer","title":"MAPOptimizer","text":"<pre><code>MAPOptimizer(\n    steps: int = 500,\n    optimizer: GradientTransformation | None = None,\n)\n</code></pre> <p>               Bases: <code>InferenceEngine</code></p> <p>MAP (Maximum A Posteriori) optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of optimization steps.</p> <code>500</code> <code>optimizer</code> <code>GradientTransformation</code> <p>Optax optimizer to use. Default: SGD with momentum.</p> <code>None</code> Notes <ul> <li>Loss function = negative log posterior.</li> <li>Gradients computed with jax.grad.</li> </ul> <p>Methods:</p> Name Description <code>fit</code> <p>Fit model parameters with MAP optimization.</p> <p>Attributes:</p> Name Type Description <code>optimizer</code> <code>steps</code> Source code in <code>src/psyphy/inference/map_optimizer.py</code> <pre><code>def __init__(self, steps: int = 500, optimizer: optax.GradientTransformation | None = None):\n    self.steps = steps\n    self.optimizer = optimizer or optax.sgd(learning_rate=5e-5, momentum=0.9)\n</code></pre>"},{"location":"reference/inference/#psyphy.inference.map_optimizer.MAPOptimizer.optimizer","title":"optimizer","text":"<pre><code>optimizer = optimizer or sgd(\n    learning_rate=5e-05, momentum=0.9\n)\n</code></pre>"},{"location":"reference/inference/#psyphy.inference.map_optimizer.MAPOptimizer.steps","title":"steps","text":"<pre><code>steps = steps\n</code></pre>"},{"location":"reference/inference/#psyphy.inference.map_optimizer.MAPOptimizer.fit","title":"fit","text":"<pre><code>fit(model, data) -&gt; Posterior\n</code></pre> <p>Fit model parameters with MAP optimization.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>WPPM</code> <p>Model instance.</p> required <code>data</code> <code>ResponseData</code> <p>Observed trials.</p> required <p>Returns:</p> Type Description <code>Posterior</code> <p>Posterior wrapper around MAP params and model.</p> Source code in <code>src/psyphy/inference/map_optimizer.py</code> <pre><code>def fit(self, model, data) -&gt; Posterior:\n    \"\"\"\n    Fit model parameters with MAP optimization.\n\n    Parameters\n    ----------\n    model : WPPM\n        Model instance.\n    data : ResponseData\n        Observed trials.\n\n    Returns\n    -------\n    Posterior\n        Posterior wrapper around MAP params and model.\n    \"\"\"\n\n    def loss_fn(params):\n        return -model.log_posterior_from_data(params, data)\n\n    params = model.init_params(jax.random.PRNGKey(0))\n    opt_state = self.optimizer.init(params)\n\n    @jax.jit\n    def step(params, opt_state):\n        loss, grads = jax.value_and_grad(loss_fn)(params)\n        updates, opt_state = self.optimizer.update(grads, opt_state, params)\n        params = optax.apply_updates(params, updates)\n        return params, opt_state, loss\n\n    for _ in range(self.steps):\n        params, opt_state, loss = step(params, opt_state)\n\n    return Posterior(params=params, model=model)\n</code></pre>"},{"location":"reference/inference/#langevin-samplers","title":"Langevin Samplers","text":""},{"location":"reference/inference/#psyphy.inference.langevin","title":"langevin","text":"langevin.py <p>Langevin samplers for posterior inference.</p> <p>Implements: - Overdamped (unadjusted) Langevin Algorithm (ULA) - Underdamped Langevin (with BAOAB splitting scheme?)</p> <p>Used for posterior-aware trial placement (InfoGain).</p> <p>MVP implementation: - Stub that returns an initial Posterior. - Future: implement underdamped Langevin dynamics (e.g. BAOAB integrator).</p> <p>Classes:</p> Name Description <code>LangevinSampler</code> <p>Langevin sampler (stub).</p>"},{"location":"reference/inference/#psyphy.inference.langevin.LangevinSampler","title":"LangevinSampler","text":"<pre><code>LangevinSampler(\n    steps: int = 1000,\n    step_size: float = 0.001,\n    temperature: float = 1.0,\n)\n</code></pre> <p>Langevin sampler (stub).</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of Langevin steps.</p> <code>1000</code> <code>step_size</code> <code>float</code> <p>Integration step size.</p> <code>1e-3</code> <code>temperature</code> <code>float</code> <p>Noise scale (temperature).</p> <code>1.0</code> <p>Methods:</p> Name Description <code>fit</code> <p>Fit model parameters with Langevin dynamics (stub).</p> <p>Attributes:</p> Name Type Description <code>step_size</code> <code>steps</code> <code>temperature</code> Source code in <code>src/psyphy/inference/langevin.py</code> <pre><code>def __init__(self, steps: int = 1000, step_size: float = 1e-3, temperature: float = 1.0):\n    self.steps = steps\n    self.step_size = step_size\n    self.temperature = temperature\n</code></pre>"},{"location":"reference/inference/#psyphy.inference.langevin.LangevinSampler.step_size","title":"step_size","text":"<pre><code>step_size = step_size\n</code></pre>"},{"location":"reference/inference/#psyphy.inference.langevin.LangevinSampler.steps","title":"steps","text":"<pre><code>steps = steps\n</code></pre>"},{"location":"reference/inference/#psyphy.inference.langevin.LangevinSampler.temperature","title":"temperature","text":"<pre><code>temperature = temperature\n</code></pre>"},{"location":"reference/inference/#psyphy.inference.langevin.LangevinSampler.fit","title":"fit","text":"<pre><code>fit(model, data) -&gt; Posterior\n</code></pre> <p>Fit model parameters with Langevin dynamics (stub).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>WPPM</code> <p>Model instance.</p> required <code>data</code> <code>ResponseData</code> <p>Observed trials.</p> required <p>Returns:</p> Type Description <code>Posterior</code> <p>Posterior wrapper (MVP: params from init).</p> Source code in <code>src/psyphy/inference/langevin.py</code> <pre><code>def fit(self, model, data) -&gt; Posterior:\n    \"\"\"\n    Fit model parameters with Langevin dynamics (stub).\n\n    Parameters\n    ----------\n    model : WPPM\n        Model instance.\n    data : ResponseData\n        Observed trials.\n\n    Returns\n    -------\n    Posterior\n        Posterior wrapper (MVP: params from init).\n    \"\"\"\n    return Posterior(params=model.init_params(None), model=model)\n</code></pre>"},{"location":"reference/inference/#laplace-approximation","title":"Laplace Approximation","text":""},{"location":"reference/inference/#psyphy.inference.laplace","title":"laplace","text":"laplace.py <p>Laplace approximation to posterior.</p> <p>Approximates posterior with a Gaussian:     N(mean = MAP, covariance = H^-1 at MAP)</p> <p>Provides posterior.sample() cheaply. Useful for InfoGainPlacement when only MAP fit is available.</p> <p>MVP implementation: - Stub that just returns the MAP posterior. - Future: compute covariance from Hessian at MAP params.</p> <p>Classes:</p> Name Description <code>LaplaceApproximation</code> <p>Laplace approximation around MAP estimate.</p>"},{"location":"reference/inference/#psyphy.inference.laplace.LaplaceApproximation","title":"LaplaceApproximation","text":"<p>Laplace approximation around MAP estimate.</p> <p>Methods:</p> Name Description <code>from_map</code> <p>Construct a Gaussian approximation centered at MAP.</p>"},{"location":"reference/inference/#psyphy.inference.laplace.LaplaceApproximation.from_map","title":"from_map","text":"<pre><code>from_map(map_posterior: Posterior) -&gt; Posterior\n</code></pre> <p>Return posterior approximation from MAP.</p> <p>Parameters:</p> Name Type Description Default <code>map_posterior</code> <code>Posterior</code> <p>Posterior object from MAP optimization.</p> required <p>Returns:</p> Type Description <code>Posterior</code> <p>Same posterior object (MVP).</p> Source code in <code>src/psyphy/inference/laplace.py</code> <pre><code>def from_map(self, map_posterior: Posterior) -&gt; Posterior:\n    \"\"\"\n    Return posterior approximation from MAP.\n\n    Parameters\n    ----------\n    map_posterior : Posterior\n        Posterior object from MAP optimization.\n\n    Returns\n    -------\n    Posterior\n        Same posterior object (MVP).\n    \"\"\"\n    return map_posterior\n</code></pre>"},{"location":"reference/model/","title":"Model","text":""},{"location":"reference/model/#package","title":"Package","text":""},{"location":"reference/model/#psyphy.model","title":"model","text":""},{"location":"reference/model/#psyphy.model--psyphymodel","title":"psyphy.model","text":"<p>Model-layer API: everything model-related in one place.</p> Includes <ul> <li>WPPM (core model)</li> <li>Priors (Prior)</li> <li>Tasks (TaskLikelihood base, OddityTask, TwoAFC)</li> <li>Noise models (GaussianNoise, StudentTNoise)</li> </ul> <p>All functions/classes use JAX arrays (jax.numpy as jnp) for autodiff and optimization with Optax.</p> Typical usage <pre><code>from psyphy.model import WPPM, Prior, OddityTask, GaussianNoise\n</code></pre> <p>Classes:</p> Name Description <code>GaussianNoise</code> <code>OddityTask</code> <p>Three-alternative forced-choice oddity task (MVP placeholder) (\"pick the odd-one out).</p> <code>Prior</code> <p>Prior distribution over WPPM parameters</p> <code>StudentTNoise</code> <code>TaskLikelihood</code> <p>Abstract base class for task likelihoods</p> <code>TwoAFC</code> <p>2-alternative forced-choice task (MVP placeholder).</p> <code>WPPM</code> <p>Wishart Process Psychophysical Model (WPPM).</p>"},{"location":"reference/model/#psyphy.model.GaussianNoise","title":"GaussianNoise","text":"<pre><code>GaussianNoise(sigma: float = 1.0)\n</code></pre> <p>Methods:</p> Name Description <code>log_prob</code> <p>Attributes:</p> Name Type Description <code>sigma</code> <code>float</code>"},{"location":"reference/model/#psyphy.model.GaussianNoise.sigma","title":"sigma","text":"<pre><code>sigma: float = 1.0\n</code></pre>"},{"location":"reference/model/#psyphy.model.GaussianNoise.log_prob","title":"log_prob","text":"<pre><code>log_prob(residual: float) -&gt; float\n</code></pre> Source code in <code>src/psyphy/model/noise.py</code> <pre><code>def log_prob(self, residual: float) -&gt; float:\n    _ = residual\n    return -0.5\n</code></pre>"},{"location":"reference/model/#psyphy.model.OddityTask","title":"OddityTask","text":"<pre><code>OddityTask(slope: float = 1.5)\n</code></pre> <p>               Bases: <code>TaskLikelihood</code></p> <p>Three-alternative forced-choice oddity task (MVP placeholder) (\"pick the odd-one out).</p> <p>Methods:</p> Name Description <code>loglik</code> <code>predict</code> <p>Attributes:</p> Name Type Description <code>chance_level</code> <code>float</code> <code>performance_range</code> <code>float</code> <code>slope</code> Source code in <code>src/psyphy/model/task.py</code> <pre><code>def __init__(self, slope: float = 1.5) -&gt; None:\n    self.slope = float(slope)\n    self.chance_level: float = 1.0 / 3.0\n    self.performance_range: float = 1.0 - self.chance_level\n</code></pre>"},{"location":"reference/model/#psyphy.model.OddityTask.chance_level","title":"chance_level","text":"<pre><code>chance_level: float = 1.0 / 3.0\n</code></pre>"},{"location":"reference/model/#psyphy.model.OddityTask.performance_range","title":"performance_range","text":"<pre><code>performance_range: float = 1.0 - chance_level\n</code></pre>"},{"location":"reference/model/#psyphy.model.OddityTask.slope","title":"slope","text":"<pre><code>slope = float(slope)\n</code></pre>"},{"location":"reference/model/#psyphy.model.OddityTask.loglik","title":"loglik","text":"<pre><code>loglik(\n    params: Any, data: Any, model: Any, noise: Any\n) -&gt; ndarray\n</code></pre> Source code in <code>src/psyphy/model/task.py</code> <pre><code>def loglik(self, params: Any, data: Any, model: Any, noise: Any) -&gt; jnp.ndarray:\n    refs, probes, responses = data.to_numpy()\n    ps = jnp.array([self.predict(params, (r, p), model, noise) for r, p in zip(refs, probes)])\n    eps = 1e-9\n    return jnp.sum(jnp.where(responses == 1, jnp.log(ps + eps), jnp.log(1.0 - ps + eps)))\n</code></pre>"},{"location":"reference/model/#psyphy.model.OddityTask.predict","title":"predict","text":"<pre><code>predict(\n    params: Any, stimuli: Stimulus, model: Any, noise: Any\n) -&gt; ndarray\n</code></pre> Source code in <code>src/psyphy/model/task.py</code> <pre><code>def predict(self, params: Any, stimuli: Stimulus, model: Any, noise: Any) -&gt; jnp.ndarray:\n    d = model.discriminability(params, stimuli)\n    g = 0.5 * (jnp.tanh(self.slope * d) + 1.0)\n    return self.chance_level + self.performance_range * g\n</code></pre>"},{"location":"reference/model/#psyphy.model.Prior","title":"Prior","text":"<pre><code>Prior(\n    input_dim: int,\n    scale: float = 0.5,\n    variance_scale: float = 1.0,\n    lengthscale: float = 1.0,\n    extra_embedding_dims: int = 0,\n)\n</code></pre> <p>Prior distribution over WPPM parameters</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimensionality of the model space (same as WPPM.input_dim)</p> required <code>scale</code> <code>float</code> <p>Stddev of Gaussian prior for log_diag entries (MVP only).</p> <code>0.5</code> <code>variance_scale</code> <code>float</code> <p>Forward-compatible stub for Full WPPM mode. Will scale covariance magnitudes</p> <code>1.0</code> <code>lengthscale</code> <code>float</code> <p>Forward-compatible stub for Full WPPM mode; controls smoothness of covariance field: - small lengthscale --&gt; rapid variation across space - large lengthscale --&gt; smoother field, long-range correlations.</p> <code>1.0</code> <code>extra_embedding_dims</code> <code>int</code> <p>Forward-compatible stub for Full WPPM mode. Will expand embedding space.</p> <code>0</code> <p>Methods:</p> Name Description <code>default</code> <p>Convenience constructor with MVP defaults.</p> <code>log_prob</code> <p>Compute log prior density (up to a constant)</p> <code>sample_params</code> <p>Sample initial parameters from the prior.</p> <p>Attributes:</p> Name Type Description <code>extra_embedding_dims</code> <code>int</code> <code>input_dim</code> <code>int</code> <code>lengthscale</code> <code>float</code> <code>scale</code> <code>float</code> <code>variance_scale</code> <code>float</code>"},{"location":"reference/model/#psyphy.model.Prior.extra_embedding_dims","title":"extra_embedding_dims","text":"<pre><code>extra_embedding_dims: int = 0\n</code></pre>"},{"location":"reference/model/#psyphy.model.Prior.input_dim","title":"input_dim","text":"<pre><code>input_dim: int\n</code></pre>"},{"location":"reference/model/#psyphy.model.Prior.lengthscale","title":"lengthscale","text":"<pre><code>lengthscale: float = 1.0\n</code></pre>"},{"location":"reference/model/#psyphy.model.Prior.scale","title":"scale","text":"<pre><code>scale: float = 0.5\n</code></pre>"},{"location":"reference/model/#psyphy.model.Prior.variance_scale","title":"variance_scale","text":"<pre><code>variance_scale: float = 1.0\n</code></pre>"},{"location":"reference/model/#psyphy.model.Prior.default","title":"default","text":"<pre><code>default(input_dim: int, scale: float = 0.5) -&gt; 'Prior'\n</code></pre> <p>Convenience constructor with MVP defaults.</p> Source code in <code>src/psyphy/model/prior.py</code> <pre><code>@classmethod\ndef default(cls, input_dim: int, scale: float = 0.5) -&gt; \"Prior\":\n    \"\"\"Convenience constructor with MVP defaults.\"\"\"\n    return cls(input_dim=input_dim, scale=scale)\n</code></pre>"},{"location":"reference/model/#psyphy.model.Prior.log_prob","title":"log_prob","text":"<pre><code>log_prob(params: Params) -&gt; ndarray\n</code></pre> <p>Compute log prior density (up to a constant)</p> <p>MVP:     Isotropic Gaussian on log_diag Full WPPM mode:     Will implement structured prior over basis weights and     lengthscale-regularized covariance fields</p> Source code in <code>src/psyphy/model/prior.py</code> <pre><code>def log_prob(self, params: Params) -&gt; jnp.ndarray:\n    \"\"\"\n    Compute log prior density (up to a constant)\n\n    MVP:\n        Isotropic Gaussian on log_diag\n    Full WPPM mode:\n        Will implement structured prior over basis weights and\n        lengthscale-regularized covariance fields\n    \"\"\"\n    log_diag = params[\"log_diag\"]\n    var = self.scale**2\n    return -0.5 * jnp.sum((log_diag**2) / var)\n</code></pre>"},{"location":"reference/model/#psyphy.model.Prior.sample_params","title":"sample_params","text":"<pre><code>sample_params(key: KeyArray) -&gt; Params\n</code></pre> <p>Sample initial parameters from the prior.</p> <p>MVP:     Returns {\"log_diag\": shape (input_dim,)}. Full WPPM mode:     Will also include basis weights, structured covariance params,     and hyperparameters for GP (variance_scale, lengthscale).</p> Source code in <code>src/psyphy/model/prior.py</code> <pre><code>def sample_params(self, key: jr.KeyArray) -&gt; Params:\n    \"\"\"\n    Sample initial parameters from the prior.\n\n    MVP:\n        Returns {\"log_diag\": shape (input_dim,)}.\n    Full WPPM mode:\n        Will also include basis weights, structured covariance params,\n        and hyperparameters for GP (variance_scale, lengthscale).\n    \"\"\"\n    log_diag = jr.normal(key, shape=(self.input_dim,)) * self.scale\n    return {\"log_diag\": log_diag}\n</code></pre>"},{"location":"reference/model/#psyphy.model.StudentTNoise","title":"StudentTNoise","text":"<pre><code>StudentTNoise(df: float = 3.0, scale: float = 1.0)\n</code></pre> <p>Methods:</p> Name Description <code>log_prob</code> <p>Attributes:</p> Name Type Description <code>df</code> <code>float</code> <code>scale</code> <code>float</code>"},{"location":"reference/model/#psyphy.model.StudentTNoise.df","title":"df","text":"<pre><code>df: float = 3.0\n</code></pre>"},{"location":"reference/model/#psyphy.model.StudentTNoise.scale","title":"scale","text":"<pre><code>scale: float = 1.0\n</code></pre>"},{"location":"reference/model/#psyphy.model.StudentTNoise.log_prob","title":"log_prob","text":"<pre><code>log_prob(residual: float) -&gt; float\n</code></pre> Source code in <code>src/psyphy/model/noise.py</code> <pre><code>def log_prob(self, residual: float) -&gt; float:\n    _ = residual\n    return -0.5\n</code></pre>"},{"location":"reference/model/#psyphy.model.TaskLikelihood","title":"TaskLikelihood","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for task likelihoods</p> <p>Methods:</p> Name Description <code>loglik</code> <p>Compute log-likelihood of observed responses under this task</p> <code>predict</code> <p>Predict probability of correct response for a stimulus.</p>"},{"location":"reference/model/#psyphy.model.TaskLikelihood.loglik","title":"loglik","text":"<pre><code>loglik(\n    params: Any, data: Any, model: Any, noise: Any\n) -&gt; ndarray\n</code></pre> <p>Compute log-likelihood of observed responses under this task</p> Source code in <code>src/psyphy/model/task.py</code> <pre><code>@abstractmethod\ndef loglik(self, params: Any, data: Any, model: Any, noise: Any) -&gt; jnp.ndarray:\n    \"\"\"Compute log-likelihood of observed responses under this task\"\"\"\n    ...\n</code></pre>"},{"location":"reference/model/#psyphy.model.TaskLikelihood.predict","title":"predict","text":"<pre><code>predict(\n    params: Any, stimuli: Stimulus, model: Any, noise: Any\n) -&gt; ndarray\n</code></pre> <p>Predict probability of correct response for a stimulus.</p> Source code in <code>src/psyphy/model/task.py</code> <pre><code>@abstractmethod\ndef predict(self, params: Any, stimuli: Stimulus, model: Any, noise: Any) -&gt; jnp.ndarray:\n    \"\"\"Predict probability of correct response for a stimulus.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/model/#psyphy.model.TwoAFC","title":"TwoAFC","text":"<pre><code>TwoAFC(slope: float = 2.0)\n</code></pre> <p>               Bases: <code>TaskLikelihood</code></p> <p>2-alternative forced-choice task (MVP placeholder).</p> <p>Methods:</p> Name Description <code>loglik</code> <code>predict</code> <p>Attributes:</p> Name Type Description <code>chance_level</code> <code>float</code> <code>performance_range</code> <code>float</code> <code>slope</code> Source code in <code>src/psyphy/model/task.py</code> <pre><code>def __init__(self, slope: float = 2.0) -&gt; None:\n    self.slope = float(slope)\n    self.chance_level: float = 0.5\n    self.performance_range: float = 1.0 - self.chance_level\n</code></pre>"},{"location":"reference/model/#psyphy.model.TwoAFC.chance_level","title":"chance_level","text":"<pre><code>chance_level: float = 0.5\n</code></pre>"},{"location":"reference/model/#psyphy.model.TwoAFC.performance_range","title":"performance_range","text":"<pre><code>performance_range: float = 1.0 - chance_level\n</code></pre>"},{"location":"reference/model/#psyphy.model.TwoAFC.slope","title":"slope","text":"<pre><code>slope = float(slope)\n</code></pre>"},{"location":"reference/model/#psyphy.model.TwoAFC.loglik","title":"loglik","text":"<pre><code>loglik(\n    params: Any, data: Any, model: Any, noise: Any\n) -&gt; ndarray\n</code></pre> Source code in <code>src/psyphy/model/task.py</code> <pre><code>def loglik(self, params: Any, data: Any, model: Any, noise: Any) -&gt; jnp.ndarray:\n    refs, probes, responses = data.to_numpy()\n    ps = jnp.array([self.predict(params, (r, p), model, noise) for r, p in zip(refs, probes)])\n    eps = 1e-9\n    return jnp.sum(jnp.where(responses == 1, jnp.log(ps + eps), jnp.log(1.0 - ps + eps)))\n</code></pre>"},{"location":"reference/model/#psyphy.model.TwoAFC.predict","title":"predict","text":"<pre><code>predict(\n    params: Any, stimuli: Stimulus, model: Any, noise: Any\n) -&gt; ndarray\n</code></pre> Source code in <code>src/psyphy/model/task.py</code> <pre><code>def predict(self, params: Any, stimuli: Stimulus, model: Any, noise: Any) -&gt; jnp.ndarray:\n    d = model.discriminability(params, stimuli)\n    return self.chance_level + self.performance_range * jnp.tanh(self.slope * d)\n</code></pre>"},{"location":"reference/model/#psyphy.model.WPPM","title":"WPPM","text":"<pre><code>WPPM(\n    input_dim: int,\n    prior: Prior,\n    task: TaskLikelihood,\n    noise: Any | None = None,\n    *,\n    extra_dims: int = 0,\n    variance_scale: float = 1.0,\n    lengthscale: float = 1.0,\n    diag_term: float = 1e-06\n)\n</code></pre> <p>Wishart Process Psychophysical Model (WPPM).</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimensionality of the input stimulus space (e.g., 2 for isoluminant plane, 3 for RGB). Both reference and probe live in R^{input_dim}.</p> required <code>prior</code> <code>Prior</code> <p>Prior distribution over model parameters. MVP uses a simple Gaussian prior over diagonal log-variances (see Prior.sample_params()).</p> required <code>task</code> <code>TaskLikelihood</code> <p>Psychophysical task mapping that defines how discriminability translates to p(correct) and how log-likelihood of responses is computed. (e.g., OddityTask, TwoAFC)</p> required <code>noise</code> <code>Any</code> <p>Noise model describing internal representation noise (e.g., GaussianNoise). Not used in MVP mapping but passed to the task interface for future MC sims.</p> <code>None</code> Forward-compatible hyperparameters (MVP stubs) <p>extra_dims : int, default=0     Additional embedding dimensions for basis expansions (unused in MVP). variance_scale : float, default=1.0     Global scaling factor for covariance magnitude (unused in MVP). lengthscale : float, default=1.0     Smoothness/length-scale for spatial covariance variation (unused in MVP).     (formerly \"decay_rate\") diag_term : float, default=1e-6     Small positive value added to the covariance diagonal for numerical stability.     MVP uses this in matrix solves; the research model will also use it.</p> <p>Methods:</p> Name Description <code>discriminability</code> <p>Compute scalar discriminability d &gt;= 0 for a (reference, probe) pair</p> <code>init_params</code> <p>Sample initial parameters from the prior.</p> <code>local_covariance</code> <p>Return local covariance \u03a3(x) at stimulus location x.</p> <code>log_likelihood</code> <p>Compute the log-likelihood for arrays of trials.</p> <code>log_likelihood_from_data</code> <p>Compute log-likelihood directly from a ResponseData object.</p> <code>log_posterior_from_data</code> <p>Convenience helper if you want log posterior in one call (MVP).</p> <code>predict_prob</code> <p>Predict probability of a correct response for a single stimulus.</p> <p>Attributes:</p> Name Type Description <code>diag_term</code> <code>extra_dims</code> <code>input_dim</code> <code>lengthscale</code> <code>noise</code> <code>prior</code> <code>task</code> <code>variance_scale</code> Source code in <code>src/psyphy/model/wppm.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    prior: Prior,\n    task: TaskLikelihood,\n    noise: Any | None = None,\n    *,\n    extra_dims: int = 0,\n    variance_scale: float = 1.0,\n    lengthscale: float = 1.0,\n    diag_term: float = 1e-6,\n) -&gt; None:\n    # --- core components ---\n    self.input_dim = int(input_dim)   # stimulus-space dimensionality\n    self.prior = prior                # prior over parameter PyTree\n    self.task = task                  # task mapping and likelihood\n    self.noise = noise                # noise model \n\n    # --- forward-compatible hyperparameters (stubs in MVP) ---\n    self.extra_dims = int(extra_dims)\n    self.variance_scale = float(variance_scale)\n    self.lengthscale = float(lengthscale)\n    self.diag_term = float(diag_term)\n</code></pre>"},{"location":"reference/model/#psyphy.model.WPPM.diag_term","title":"diag_term","text":"<pre><code>diag_term = float(diag_term)\n</code></pre>"},{"location":"reference/model/#psyphy.model.WPPM.extra_dims","title":"extra_dims","text":"<pre><code>extra_dims = int(extra_dims)\n</code></pre>"},{"location":"reference/model/#psyphy.model.WPPM.input_dim","title":"input_dim","text":"<pre><code>input_dim = int(input_dim)\n</code></pre>"},{"location":"reference/model/#psyphy.model.WPPM.lengthscale","title":"lengthscale","text":"<pre><code>lengthscale = float(lengthscale)\n</code></pre>"},{"location":"reference/model/#psyphy.model.WPPM.noise","title":"noise","text":"<pre><code>noise = noise\n</code></pre>"},{"location":"reference/model/#psyphy.model.WPPM.prior","title":"prior","text":"<pre><code>prior = prior\n</code></pre>"},{"location":"reference/model/#psyphy.model.WPPM.task","title":"task","text":"<pre><code>task = task\n</code></pre>"},{"location":"reference/model/#psyphy.model.WPPM.variance_scale","title":"variance_scale","text":"<pre><code>variance_scale = float(variance_scale)\n</code></pre>"},{"location":"reference/model/#psyphy.model.WPPM.discriminability","title":"discriminability","text":"<pre><code>discriminability(\n    params: Params, stimulus: Stimulus\n) -&gt; ndarray\n</code></pre> <p>Compute scalar discriminability d &gt;= 0 for a (reference, probe) pair</p> <p>MVP:     d = sqrt( (probe - ref)^T \u03a3(ref)^{-1} (probe - ref) )     with \u03a3(ref) the local covariance at the reference,     - We add <code>diag_term * I</code> for numerical stability before inversion Future (full WPPM mode):     d is implicit via Monte Carlo simulation of internal noisy responses     under the task's decision rule (no closed form). In that case, tasks     will directly implement predict/loglik with MC, and this method may be     used only for diagnostics.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Model parameters.</p> required <code>stimulus</code> <code>tuple</code> <p>(reference, probe) arrays of shape (input_dim,).</p> required <p>Returns:</p> Name Type Description <code>d</code> <code>ndarray</code> <p>Nonnegative scalar discriminability.</p> Source code in <code>src/psyphy/model/wppm.py</code> <pre><code>def discriminability(self, params: Params, stimulus: Stimulus) -&gt; jnp.ndarray:\n    \"\"\"\n    Compute scalar discriminability d &gt;= 0 for a (reference, probe) pair\n\n    MVP:\n        d = sqrt( (probe - ref)^T \u03a3(ref)^{-1} (probe - ref) )\n        with \u03a3(ref) the local covariance at the reference,\n        - We add `diag_term * I` for numerical stability before inversion\n    Future (full WPPM mode):\n        d is implicit via Monte Carlo simulation of internal noisy responses\n        under the task's decision rule (no closed form). In that case, tasks\n        will directly implement predict/loglik with MC, and this method may be\n        used only for diagnostics.\n\n    Parameters\n    ----------\n    params : dict\n        Model parameters.\n    stimulus : tuple\n        (reference, probe) arrays of shape (input_dim,).\n\n    Returns\n    -------\n    d : jnp.ndarray\n        Nonnegative scalar discriminability.\n    \"\"\"\n    ref, probe = stimulus\n    delta = probe - ref                                # difference vector in input space\n    Sigma = self.local_covariance(params, ref)         # local covariance at reference\n    # Add jitter for stable solve; diag_term is configurable\n    jitter = self.diag_term * jnp.eye(self.input_dim)\n    # Solve (\u03a3 + jitter)^{-1} delta using a PD-aware solver\n    x = jax.scipy.linalg.solve(Sigma + jitter, delta, assume_a=\"pos\")\n    d2 = jnp.dot(delta, x)                             # quadratic form\n    # Guard against tiny negative values from numerical error\n    return jnp.sqrt(jnp.maximum(d2, 0.0))\n</code></pre>"},{"location":"reference/model/#psyphy.model.WPPM.init_params","title":"init_params","text":"<pre><code>init_params(key: KeyArray) -&gt; Params\n</code></pre> <p>Sample initial parameters from the prior.</p> <p>MVP parameters:     {\"log_diag\": shape (input_dim,)} which defines a constant diagonal covariance across the space.</p> <p>Returns:</p> Name Type Description <code>params</code> <code>dict[str, ndarray]</code> Source code in <code>src/psyphy/model/wppm.py</code> <pre><code>def init_params(self, key: jr.KeyArray) -&gt; Params:\n    \"\"\"\n    Sample initial parameters from the prior.\n\n    MVP parameters:\n        {\"log_diag\": shape (input_dim,)}\n    which defines a constant diagonal covariance across the space.\n\n    Returns\n    -------\n    params : dict[str, jnp.ndarray]\n    \"\"\"\n    return self.prior.sample_params(key)\n</code></pre>"},{"location":"reference/model/#psyphy.model.WPPM.local_covariance","title":"local_covariance","text":"<pre><code>local_covariance(params: Params, x: ndarray) -&gt; ndarray\n</code></pre> <p>Return local covariance \u03a3(x) at stimulus location x.</p> <p>MVP:     \u03a3(x) = diag(exp(log_diag)), constant across x.     - Positive-definite because exp(log_diag) &gt; 0. Future (full WPPM mode):     \u03a3(x) varies smoothly with x via basis expansions and a Wishart-process     prior controlled by (extra_dims, variance_scale, lengthscale). Those     hyperparameters are exposed here but not used in MVP.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>model parameters (MVP expects \"log_diag\": (input_dim,)).</p> required <code>x</code> <code>ndarray</code> <p>Stimulus location (unused in MVP because \u03a3 is constant).</p> required <p>Returns:</p> Type Description <code>\u03a3 : jnp.ndarray, shape (input_dim, input_dim)</code> Source code in <code>src/psyphy/model/wppm.py</code> <pre><code>def local_covariance(self, params: Params, x: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"\n    Return local covariance \u03a3(x) at stimulus location x.\n\n    MVP:\n        \u03a3(x) = diag(exp(log_diag)), constant across x.\n        - Positive-definite because exp(log_diag) &gt; 0.\n    Future (full WPPM mode):\n        \u03a3(x) varies smoothly with x via basis expansions and a Wishart-process\n        prior controlled by (extra_dims, variance_scale, lengthscale). Those\n        hyperparameters are exposed here but not used in MVP.\n\n    Parameters\n    ----------\n    params : dict\n        model parameters (MVP expects \"log_diag\": (input_dim,)).\n    x : jnp.ndarray\n        Stimulus location (unused in MVP because \u03a3 is constant).\n\n    Returns\n    -------\n    \u03a3 : jnp.ndarray, shape (input_dim, input_dim)\n    \"\"\"\n    log_diag = params[\"log_diag\"]               # unconstrained diagonal log-variances\n    diag = jnp.exp(log_diag)                    # enforce positivity\n    return jnp.diag(diag)                       # constant diagonal covariance\n</code></pre>"},{"location":"reference/model/#psyphy.model.WPPM.log_likelihood","title":"log_likelihood","text":"<pre><code>log_likelihood(\n    params: Params,\n    refs: ndarray,\n    probes: ndarray,\n    responses: ndarray,\n) -&gt; ndarray\n</code></pre> <p>Compute the log-likelihood for arrays of trials.</p> <p>IMPORTANT:     We delegate to the TaskLikelihood to avoid duplicating Bernoulli (MPV)     or MC likelihood logic in multiple places. This keeps responsibilities     clean and makes adding new tasks straightforward.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Model parameters.</p> required <code>refs</code> <code>(ndarray, shape(N, input_dim))</code> required <code>probes</code> <code>(ndarray, shape(N, input_dim))</code> required <code>responses</code> <code>(ndarray, shape(N))</code> <p>Typically 0/1; task may support richer encodings.</p> required <p>Returns:</p> Name Type Description <code>loglik</code> <code>ndarray</code> <p>Scalar log-likelihood (task-only; add prior outside if needed)</p> Source code in <code>src/psyphy/model/wppm.py</code> <pre><code>def log_likelihood(self, params: Params, refs: jnp.ndarray, probes: jnp.ndarray, responses: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"\n    Compute the log-likelihood for arrays of trials.\n\n    IMPORTANT:\n        We delegate to the TaskLikelihood to avoid duplicating Bernoulli (MPV)\n        or MC likelihood logic in multiple places. This keeps responsibilities\n        clean and makes adding new tasks straightforward.\n\n    Parameters\n    ----------\n    params : dict\n        Model parameters.\n    refs : jnp.ndarray, shape (N, input_dim)\n    probes : jnp.ndarray, shape (N, input_dim)\n    responses : jnp.ndarray, shape (N,)\n        Typically 0/1; task may support richer encodings.\n\n    Returns\n    -------\n    loglik : jnp.ndarray\n        Scalar log-likelihood (task-only; add prior outside if needed)\n    \"\"\"\n    # We need a ResponseData-like object. To keep this method usable from\n    # array inputs, we construct one on the fly. If you already have a\n    # ResponseData instance, prefer `log_likelihood_from_data`.\n    from psyphy.data.dataset import ResponseData  # local import to avoid cycles\n    data = ResponseData()\n    # ResponseData.add_trial(ref, probe, resp)\n    for r, p, y in zip(refs, probes, responses):\n        data.add_trial(r, p, int(y))\n    return self.task.loglik(params, data, self, self.noise)\n</code></pre>"},{"location":"reference/model/#psyphy.model.WPPM.log_likelihood_from_data","title":"log_likelihood_from_data","text":"<pre><code>log_likelihood_from_data(\n    params: Params, data: Any\n) -&gt; ndarray\n</code></pre> <p>Compute log-likelihood directly from a ResponseData object.</p> <p>Why delegate to the task?     - The task knows the decision rule (oddity, 2AFC, ...).     - The task can use the model (this WPPM) to fetch discriminabilities     - and the task can use the noise model if it needs MC simulation</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Model parameters.</p> required <code>data</code> <code>ResponseData</code> <p>Collected trial data.</p> required <p>Returns:</p> Name Type Description <code>loglik</code> <code>ndarray</code> <p>scalar log-likelihood (task-only; add prior outside if needed)</p> Source code in <code>src/psyphy/model/wppm.py</code> <pre><code>def log_likelihood_from_data(self, params: Params, data: Any) -&gt; jnp.ndarray:\n    \"\"\"\n    Compute log-likelihood directly from a ResponseData object.\n\n    Why delegate to the task?\n        - The task knows the decision rule (oddity, 2AFC, ...).\n        - The task can use the model (this WPPM) to fetch discriminabilities\n        - and the task can use the noise model if it needs MC simulation\n\n    Parameters\n    ----------\n    params : dict\n        Model parameters.\n    data : ResponseData\n        Collected trial data.\n\n    Returns\n    -------\n    loglik : jnp.ndarray\n        scalar log-likelihood (task-only; add prior outside if needed)\n    \"\"\"\n    return self.task.loglik(params, data, self, self.noise)\n</code></pre>"},{"location":"reference/model/#psyphy.model.WPPM.log_posterior_from_data","title":"log_posterior_from_data","text":"<pre><code>log_posterior_from_data(\n    params: Params, data: Any\n) -&gt; ndarray\n</code></pre> <p>Convenience helper if you want log posterior in one call (MVP).</p> <p>This simply adds the prior log-probability to the task log-likelihood. Inference engines (e.g., MAP optimizer) typically optimize this quantity.</p> <p>Returns:</p> Type Description <code>jnp.ndarray : scalar log posterior = loglik(params | data) + log_prior(params)</code> Source code in <code>src/psyphy/model/wppm.py</code> <pre><code>def log_posterior_from_data(self, params: Params, data: Any) -&gt; jnp.ndarray:\n    \"\"\"\n    Convenience helper if you want log posterior in one call (MVP).\n\n    This simply adds the prior log-probability to the task log-likelihood.\n    Inference engines (e.g., MAP optimizer) typically optimize this quantity.\n\n    Returns\n    -------\n    jnp.ndarray : scalar log posterior = loglik(params | data) + log_prior(params)\n    \"\"\"\n    return self.log_likelihood_from_data(params, data) + self.prior.log_prob(params)\n</code></pre>"},{"location":"reference/model/#psyphy.model.WPPM.predict_prob","title":"predict_prob","text":"<pre><code>predict_prob(params: Params, stimulus: Stimulus) -&gt; ndarray\n</code></pre> <p>Predict probability of a correct response for a single stimulus.</p> <p>Design choice:     WPPM computes discriminability &amp; covariance; the TASK defines how     that translates to performance. We therefore delegate to:         task.predict(params, stimulus, model=self, noise=self.noise)</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> required <code>stimulus</code> <code>(reference, probe)</code> required <p>Returns:</p> Name Type Description <code>p_correct</code> <code>ndarray</code> Source code in <code>src/psyphy/model/wppm.py</code> <pre><code>def predict_prob(self, params: Params, stimulus: Stimulus) -&gt; jnp.ndarray:\n    \"\"\"\n    Predict probability of a correct response for a single stimulus.\n\n    Design choice:\n        WPPM computes discriminability &amp; covariance; the TASK defines how\n        that translates to performance. We therefore delegate to:\n            task.predict(params, stimulus, model=self, noise=self.noise)\n\n    Parameters\n    ----------\n    params : dict\n    stimulus : (reference, probe)\n\n    Returns\n    -------\n    p_correct : jnp.ndarray\n    \"\"\"\n    return self.task.predict(params, stimulus, self, self.noise)\n</code></pre>"},{"location":"reference/model/#wishart-psyochophysical-process-model-wppm","title":"Wishart Psyochophysical Process Model (WPPM)","text":""},{"location":"reference/model/#psyphy.model.wppm","title":"wppm","text":"wppm.py <p>Wishart Process Psychophysical Model (WPPM) \u2014 MVP-style implementation with forward-compatible hooks for the full WPPM model.</p> Goals <p>1) MVP that runs today:    - Local covariance \u03a3(x) is diagonal and constant across the space.    - Discriminability is Mahalanobis distance under \u03a3(reference).    - Task mapping (e.g., Oddity, 2AFC) converts discriminability -&gt; p(correct).    - Likelihood is delegated to the TaskLikelihood (no Bernoulli code here).</p> <p>2) Forward compatibility with full WPPM model:    - Expose hyperparameters needed to for example use Model config used in Hong et al.:        * extra_dims: embedding size for basis expansions (unused in MVP)        * variance_scale: global covariance scale (unused in MVP)        * lengthscale: smoothness/length-scale for covariance field (unused in MVP)        * diag_term: numerical stabilizer added to covariance diagonals (used in MVP)    - Later, replace <code>local_covariance</code> with a basis-expansion Wishart process      and swap discriminability/likelihood with MC observer simulation.</p> <p>All numerics use JAX (jax.numpy as jnp) to support autodiff and Optax optimizers</p> <p>Classes:</p> Name Description <code>WPPM</code> <p>Wishart Process Psychophysical Model (WPPM).</p> <p>Attributes:</p> Name Type Description <code>Params</code> <code>Stimulus</code>"},{"location":"reference/model/#psyphy.model.wppm.Params","title":"Params","text":"<pre><code>Params = Dict[str, ndarray]\n</code></pre>"},{"location":"reference/model/#psyphy.model.wppm.Stimulus","title":"Stimulus","text":"<pre><code>Stimulus = Tuple[ndarray, ndarray]\n</code></pre>"},{"location":"reference/model/#psyphy.model.wppm.WPPM","title":"WPPM","text":"<pre><code>WPPM(\n    input_dim: int,\n    prior: Prior,\n    task: TaskLikelihood,\n    noise: Any | None = None,\n    *,\n    extra_dims: int = 0,\n    variance_scale: float = 1.0,\n    lengthscale: float = 1.0,\n    diag_term: float = 1e-06\n)\n</code></pre> <p>Wishart Process Psychophysical Model (WPPM).</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimensionality of the input stimulus space (e.g., 2 for isoluminant plane, 3 for RGB). Both reference and probe live in R^{input_dim}.</p> required <code>prior</code> <code>Prior</code> <p>Prior distribution over model parameters. MVP uses a simple Gaussian prior over diagonal log-variances (see Prior.sample_params()).</p> required <code>task</code> <code>TaskLikelihood</code> <p>Psychophysical task mapping that defines how discriminability translates to p(correct) and how log-likelihood of responses is computed. (e.g., OddityTask, TwoAFC)</p> required <code>noise</code> <code>Any</code> <p>Noise model describing internal representation noise (e.g., GaussianNoise). Not used in MVP mapping but passed to the task interface for future MC sims.</p> <code>None</code> Forward-compatible hyperparameters (MVP stubs) <p>extra_dims : int, default=0     Additional embedding dimensions for basis expansions (unused in MVP). variance_scale : float, default=1.0     Global scaling factor for covariance magnitude (unused in MVP). lengthscale : float, default=1.0     Smoothness/length-scale for spatial covariance variation (unused in MVP).     (formerly \"decay_rate\") diag_term : float, default=1e-6     Small positive value added to the covariance diagonal for numerical stability.     MVP uses this in matrix solves; the research model will also use it.</p> <p>Methods:</p> Name Description <code>discriminability</code> <p>Compute scalar discriminability d &gt;= 0 for a (reference, probe) pair</p> <code>init_params</code> <p>Sample initial parameters from the prior.</p> <code>local_covariance</code> <p>Return local covariance \u03a3(x) at stimulus location x.</p> <code>log_likelihood</code> <p>Compute the log-likelihood for arrays of trials.</p> <code>log_likelihood_from_data</code> <p>Compute log-likelihood directly from a ResponseData object.</p> <code>log_posterior_from_data</code> <p>Convenience helper if you want log posterior in one call (MVP).</p> <code>predict_prob</code> <p>Predict probability of a correct response for a single stimulus.</p> <p>Attributes:</p> Name Type Description <code>diag_term</code> <code>extra_dims</code> <code>input_dim</code> <code>lengthscale</code> <code>noise</code> <code>prior</code> <code>task</code> <code>variance_scale</code> Source code in <code>src/psyphy/model/wppm.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    prior: Prior,\n    task: TaskLikelihood,\n    noise: Any | None = None,\n    *,\n    extra_dims: int = 0,\n    variance_scale: float = 1.0,\n    lengthscale: float = 1.0,\n    diag_term: float = 1e-6,\n) -&gt; None:\n    # --- core components ---\n    self.input_dim = int(input_dim)   # stimulus-space dimensionality\n    self.prior = prior                # prior over parameter PyTree\n    self.task = task                  # task mapping and likelihood\n    self.noise = noise                # noise model \n\n    # --- forward-compatible hyperparameters (stubs in MVP) ---\n    self.extra_dims = int(extra_dims)\n    self.variance_scale = float(variance_scale)\n    self.lengthscale = float(lengthscale)\n    self.diag_term = float(diag_term)\n</code></pre>"},{"location":"reference/model/#psyphy.model.wppm.WPPM.diag_term","title":"diag_term","text":"<pre><code>diag_term = float(diag_term)\n</code></pre>"},{"location":"reference/model/#psyphy.model.wppm.WPPM.extra_dims","title":"extra_dims","text":"<pre><code>extra_dims = int(extra_dims)\n</code></pre>"},{"location":"reference/model/#psyphy.model.wppm.WPPM.input_dim","title":"input_dim","text":"<pre><code>input_dim = int(input_dim)\n</code></pre>"},{"location":"reference/model/#psyphy.model.wppm.WPPM.lengthscale","title":"lengthscale","text":"<pre><code>lengthscale = float(lengthscale)\n</code></pre>"},{"location":"reference/model/#psyphy.model.wppm.WPPM.noise","title":"noise","text":"<pre><code>noise = noise\n</code></pre>"},{"location":"reference/model/#psyphy.model.wppm.WPPM.prior","title":"prior","text":"<pre><code>prior = prior\n</code></pre>"},{"location":"reference/model/#psyphy.model.wppm.WPPM.task","title":"task","text":"<pre><code>task = task\n</code></pre>"},{"location":"reference/model/#psyphy.model.wppm.WPPM.variance_scale","title":"variance_scale","text":"<pre><code>variance_scale = float(variance_scale)\n</code></pre>"},{"location":"reference/model/#psyphy.model.wppm.WPPM.discriminability","title":"discriminability","text":"<pre><code>discriminability(\n    params: Params, stimulus: Stimulus\n) -&gt; ndarray\n</code></pre> <p>Compute scalar discriminability d &gt;= 0 for a (reference, probe) pair</p> <p>MVP:     d = sqrt( (probe - ref)^T \u03a3(ref)^{-1} (probe - ref) )     with \u03a3(ref) the local covariance at the reference,     - We add <code>diag_term * I</code> for numerical stability before inversion Future (full WPPM mode):     d is implicit via Monte Carlo simulation of internal noisy responses     under the task's decision rule (no closed form). In that case, tasks     will directly implement predict/loglik with MC, and this method may be     used only for diagnostics.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Model parameters.</p> required <code>stimulus</code> <code>tuple</code> <p>(reference, probe) arrays of shape (input_dim,).</p> required <p>Returns:</p> Name Type Description <code>d</code> <code>ndarray</code> <p>Nonnegative scalar discriminability.</p> Source code in <code>src/psyphy/model/wppm.py</code> <pre><code>def discriminability(self, params: Params, stimulus: Stimulus) -&gt; jnp.ndarray:\n    \"\"\"\n    Compute scalar discriminability d &gt;= 0 for a (reference, probe) pair\n\n    MVP:\n        d = sqrt( (probe - ref)^T \u03a3(ref)^{-1} (probe - ref) )\n        with \u03a3(ref) the local covariance at the reference,\n        - We add `diag_term * I` for numerical stability before inversion\n    Future (full WPPM mode):\n        d is implicit via Monte Carlo simulation of internal noisy responses\n        under the task's decision rule (no closed form). In that case, tasks\n        will directly implement predict/loglik with MC, and this method may be\n        used only for diagnostics.\n\n    Parameters\n    ----------\n    params : dict\n        Model parameters.\n    stimulus : tuple\n        (reference, probe) arrays of shape (input_dim,).\n\n    Returns\n    -------\n    d : jnp.ndarray\n        Nonnegative scalar discriminability.\n    \"\"\"\n    ref, probe = stimulus\n    delta = probe - ref                                # difference vector in input space\n    Sigma = self.local_covariance(params, ref)         # local covariance at reference\n    # Add jitter for stable solve; diag_term is configurable\n    jitter = self.diag_term * jnp.eye(self.input_dim)\n    # Solve (\u03a3 + jitter)^{-1} delta using a PD-aware solver\n    x = jax.scipy.linalg.solve(Sigma + jitter, delta, assume_a=\"pos\")\n    d2 = jnp.dot(delta, x)                             # quadratic form\n    # Guard against tiny negative values from numerical error\n    return jnp.sqrt(jnp.maximum(d2, 0.0))\n</code></pre>"},{"location":"reference/model/#psyphy.model.wppm.WPPM.init_params","title":"init_params","text":"<pre><code>init_params(key: KeyArray) -&gt; Params\n</code></pre> <p>Sample initial parameters from the prior.</p> <p>MVP parameters:     {\"log_diag\": shape (input_dim,)} which defines a constant diagonal covariance across the space.</p> <p>Returns:</p> Name Type Description <code>params</code> <code>dict[str, ndarray]</code> Source code in <code>src/psyphy/model/wppm.py</code> <pre><code>def init_params(self, key: jr.KeyArray) -&gt; Params:\n    \"\"\"\n    Sample initial parameters from the prior.\n\n    MVP parameters:\n        {\"log_diag\": shape (input_dim,)}\n    which defines a constant diagonal covariance across the space.\n\n    Returns\n    -------\n    params : dict[str, jnp.ndarray]\n    \"\"\"\n    return self.prior.sample_params(key)\n</code></pre>"},{"location":"reference/model/#psyphy.model.wppm.WPPM.local_covariance","title":"local_covariance","text":"<pre><code>local_covariance(params: Params, x: ndarray) -&gt; ndarray\n</code></pre> <p>Return local covariance \u03a3(x) at stimulus location x.</p> <p>MVP:     \u03a3(x) = diag(exp(log_diag)), constant across x.     - Positive-definite because exp(log_diag) &gt; 0. Future (full WPPM mode):     \u03a3(x) varies smoothly with x via basis expansions and a Wishart-process     prior controlled by (extra_dims, variance_scale, lengthscale). Those     hyperparameters are exposed here but not used in MVP.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>model parameters (MVP expects \"log_diag\": (input_dim,)).</p> required <code>x</code> <code>ndarray</code> <p>Stimulus location (unused in MVP because \u03a3 is constant).</p> required <p>Returns:</p> Type Description <code>\u03a3 : jnp.ndarray, shape (input_dim, input_dim)</code> Source code in <code>src/psyphy/model/wppm.py</code> <pre><code>def local_covariance(self, params: Params, x: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"\n    Return local covariance \u03a3(x) at stimulus location x.\n\n    MVP:\n        \u03a3(x) = diag(exp(log_diag)), constant across x.\n        - Positive-definite because exp(log_diag) &gt; 0.\n    Future (full WPPM mode):\n        \u03a3(x) varies smoothly with x via basis expansions and a Wishart-process\n        prior controlled by (extra_dims, variance_scale, lengthscale). Those\n        hyperparameters are exposed here but not used in MVP.\n\n    Parameters\n    ----------\n    params : dict\n        model parameters (MVP expects \"log_diag\": (input_dim,)).\n    x : jnp.ndarray\n        Stimulus location (unused in MVP because \u03a3 is constant).\n\n    Returns\n    -------\n    \u03a3 : jnp.ndarray, shape (input_dim, input_dim)\n    \"\"\"\n    log_diag = params[\"log_diag\"]               # unconstrained diagonal log-variances\n    diag = jnp.exp(log_diag)                    # enforce positivity\n    return jnp.diag(diag)                       # constant diagonal covariance\n</code></pre>"},{"location":"reference/model/#psyphy.model.wppm.WPPM.log_likelihood","title":"log_likelihood","text":"<pre><code>log_likelihood(\n    params: Params,\n    refs: ndarray,\n    probes: ndarray,\n    responses: ndarray,\n) -&gt; ndarray\n</code></pre> <p>Compute the log-likelihood for arrays of trials.</p> <p>IMPORTANT:     We delegate to the TaskLikelihood to avoid duplicating Bernoulli (MPV)     or MC likelihood logic in multiple places. This keeps responsibilities     clean and makes adding new tasks straightforward.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Model parameters.</p> required <code>refs</code> <code>(ndarray, shape(N, input_dim))</code> required <code>probes</code> <code>(ndarray, shape(N, input_dim))</code> required <code>responses</code> <code>(ndarray, shape(N))</code> <p>Typically 0/1; task may support richer encodings.</p> required <p>Returns:</p> Name Type Description <code>loglik</code> <code>ndarray</code> <p>Scalar log-likelihood (task-only; add prior outside if needed)</p> Source code in <code>src/psyphy/model/wppm.py</code> <pre><code>def log_likelihood(self, params: Params, refs: jnp.ndarray, probes: jnp.ndarray, responses: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"\n    Compute the log-likelihood for arrays of trials.\n\n    IMPORTANT:\n        We delegate to the TaskLikelihood to avoid duplicating Bernoulli (MPV)\n        or MC likelihood logic in multiple places. This keeps responsibilities\n        clean and makes adding new tasks straightforward.\n\n    Parameters\n    ----------\n    params : dict\n        Model parameters.\n    refs : jnp.ndarray, shape (N, input_dim)\n    probes : jnp.ndarray, shape (N, input_dim)\n    responses : jnp.ndarray, shape (N,)\n        Typically 0/1; task may support richer encodings.\n\n    Returns\n    -------\n    loglik : jnp.ndarray\n        Scalar log-likelihood (task-only; add prior outside if needed)\n    \"\"\"\n    # We need a ResponseData-like object. To keep this method usable from\n    # array inputs, we construct one on the fly. If you already have a\n    # ResponseData instance, prefer `log_likelihood_from_data`.\n    from psyphy.data.dataset import ResponseData  # local import to avoid cycles\n    data = ResponseData()\n    # ResponseData.add_trial(ref, probe, resp)\n    for r, p, y in zip(refs, probes, responses):\n        data.add_trial(r, p, int(y))\n    return self.task.loglik(params, data, self, self.noise)\n</code></pre>"},{"location":"reference/model/#psyphy.model.wppm.WPPM.log_likelihood_from_data","title":"log_likelihood_from_data","text":"<pre><code>log_likelihood_from_data(\n    params: Params, data: Any\n) -&gt; ndarray\n</code></pre> <p>Compute log-likelihood directly from a ResponseData object.</p> <p>Why delegate to the task?     - The task knows the decision rule (oddity, 2AFC, ...).     - The task can use the model (this WPPM) to fetch discriminabilities     - and the task can use the noise model if it needs MC simulation</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Model parameters.</p> required <code>data</code> <code>ResponseData</code> <p>Collected trial data.</p> required <p>Returns:</p> Name Type Description <code>loglik</code> <code>ndarray</code> <p>scalar log-likelihood (task-only; add prior outside if needed)</p> Source code in <code>src/psyphy/model/wppm.py</code> <pre><code>def log_likelihood_from_data(self, params: Params, data: Any) -&gt; jnp.ndarray:\n    \"\"\"\n    Compute log-likelihood directly from a ResponseData object.\n\n    Why delegate to the task?\n        - The task knows the decision rule (oddity, 2AFC, ...).\n        - The task can use the model (this WPPM) to fetch discriminabilities\n        - and the task can use the noise model if it needs MC simulation\n\n    Parameters\n    ----------\n    params : dict\n        Model parameters.\n    data : ResponseData\n        Collected trial data.\n\n    Returns\n    -------\n    loglik : jnp.ndarray\n        scalar log-likelihood (task-only; add prior outside if needed)\n    \"\"\"\n    return self.task.loglik(params, data, self, self.noise)\n</code></pre>"},{"location":"reference/model/#psyphy.model.wppm.WPPM.log_posterior_from_data","title":"log_posterior_from_data","text":"<pre><code>log_posterior_from_data(\n    params: Params, data: Any\n) -&gt; ndarray\n</code></pre> <p>Convenience helper if you want log posterior in one call (MVP).</p> <p>This simply adds the prior log-probability to the task log-likelihood. Inference engines (e.g., MAP optimizer) typically optimize this quantity.</p> <p>Returns:</p> Type Description <code>jnp.ndarray : scalar log posterior = loglik(params | data) + log_prior(params)</code> Source code in <code>src/psyphy/model/wppm.py</code> <pre><code>def log_posterior_from_data(self, params: Params, data: Any) -&gt; jnp.ndarray:\n    \"\"\"\n    Convenience helper if you want log posterior in one call (MVP).\n\n    This simply adds the prior log-probability to the task log-likelihood.\n    Inference engines (e.g., MAP optimizer) typically optimize this quantity.\n\n    Returns\n    -------\n    jnp.ndarray : scalar log posterior = loglik(params | data) + log_prior(params)\n    \"\"\"\n    return self.log_likelihood_from_data(params, data) + self.prior.log_prob(params)\n</code></pre>"},{"location":"reference/model/#psyphy.model.wppm.WPPM.predict_prob","title":"predict_prob","text":"<pre><code>predict_prob(params: Params, stimulus: Stimulus) -&gt; ndarray\n</code></pre> <p>Predict probability of a correct response for a single stimulus.</p> <p>Design choice:     WPPM computes discriminability &amp; covariance; the TASK defines how     that translates to performance. We therefore delegate to:         task.predict(params, stimulus, model=self, noise=self.noise)</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> required <code>stimulus</code> <code>(reference, probe)</code> required <p>Returns:</p> Name Type Description <code>p_correct</code> <code>ndarray</code> Source code in <code>src/psyphy/model/wppm.py</code> <pre><code>def predict_prob(self, params: Params, stimulus: Stimulus) -&gt; jnp.ndarray:\n    \"\"\"\n    Predict probability of a correct response for a single stimulus.\n\n    Design choice:\n        WPPM computes discriminability &amp; covariance; the TASK defines how\n        that translates to performance. We therefore delegate to:\n            task.predict(params, stimulus, model=self, noise=self.noise)\n\n    Parameters\n    ----------\n    params : dict\n    stimulus : (reference, probe)\n\n    Returns\n    -------\n    p_correct : jnp.ndarray\n    \"\"\"\n    return self.task.predict(params, stimulus, self, self.noise)\n</code></pre>"},{"location":"reference/model/#priors","title":"Priors","text":""},{"location":"reference/model/#psyphy.model.prior","title":"prior","text":"prior.py <p>Prior distributions for WPPM parameters</p> <p>MVP implementation: - Gaussian prior over diagonal log-variances</p> <p>Forward compatibility (Full WPPM mode): - Exposes hyperparameters that will be used when the full Wishart Process   covariance field is implemented:     * variance_scale : global scaling factor for covariance magnitude     * lengthscale    : smoothness/length-scale controlling spatial variation     * extra_embedding_dims : embedding dimension for basis expansions</p> Connections <ul> <li>WPPM calls Prior.sample_params() to initialize model parameters</li> <li>WPPM adds Prior.log_prob(params) to task log-likelihoods to form the log posterior</li> <li>In Full WPPM mode, Prior will generate structured parameters for basis expansions   and lengthscale-controlled smooth covariance fields</li> </ul> <p>Classes:</p> Name Description <code>Prior</code> <p>Prior distribution over WPPM parameters</p> <p>Attributes:</p> Name Type Description <code>Params</code>"},{"location":"reference/model/#psyphy.model.prior.Params","title":"Params","text":"<pre><code>Params = Dict[str, ndarray]\n</code></pre>"},{"location":"reference/model/#psyphy.model.prior.Prior","title":"Prior","text":"<pre><code>Prior(\n    input_dim: int,\n    scale: float = 0.5,\n    variance_scale: float = 1.0,\n    lengthscale: float = 1.0,\n    extra_embedding_dims: int = 0,\n)\n</code></pre> <p>Prior distribution over WPPM parameters</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimensionality of the model space (same as WPPM.input_dim)</p> required <code>scale</code> <code>float</code> <p>Stddev of Gaussian prior for log_diag entries (MVP only).</p> <code>0.5</code> <code>variance_scale</code> <code>float</code> <p>Forward-compatible stub for Full WPPM mode. Will scale covariance magnitudes</p> <code>1.0</code> <code>lengthscale</code> <code>float</code> <p>Forward-compatible stub for Full WPPM mode; controls smoothness of covariance field: - small lengthscale --&gt; rapid variation across space - large lengthscale --&gt; smoother field, long-range correlations.</p> <code>1.0</code> <code>extra_embedding_dims</code> <code>int</code> <p>Forward-compatible stub for Full WPPM mode. Will expand embedding space.</p> <code>0</code> <p>Methods:</p> Name Description <code>default</code> <p>Convenience constructor with MVP defaults.</p> <code>log_prob</code> <p>Compute log prior density (up to a constant)</p> <code>sample_params</code> <p>Sample initial parameters from the prior.</p> <p>Attributes:</p> Name Type Description <code>extra_embedding_dims</code> <code>int</code> <code>input_dim</code> <code>int</code> <code>lengthscale</code> <code>float</code> <code>scale</code> <code>float</code> <code>variance_scale</code> <code>float</code>"},{"location":"reference/model/#psyphy.model.prior.Prior.extra_embedding_dims","title":"extra_embedding_dims","text":"<pre><code>extra_embedding_dims: int = 0\n</code></pre>"},{"location":"reference/model/#psyphy.model.prior.Prior.input_dim","title":"input_dim","text":"<pre><code>input_dim: int\n</code></pre>"},{"location":"reference/model/#psyphy.model.prior.Prior.lengthscale","title":"lengthscale","text":"<pre><code>lengthscale: float = 1.0\n</code></pre>"},{"location":"reference/model/#psyphy.model.prior.Prior.scale","title":"scale","text":"<pre><code>scale: float = 0.5\n</code></pre>"},{"location":"reference/model/#psyphy.model.prior.Prior.variance_scale","title":"variance_scale","text":"<pre><code>variance_scale: float = 1.0\n</code></pre>"},{"location":"reference/model/#psyphy.model.prior.Prior.default","title":"default","text":"<pre><code>default(input_dim: int, scale: float = 0.5) -&gt; 'Prior'\n</code></pre> <p>Convenience constructor with MVP defaults.</p> Source code in <code>src/psyphy/model/prior.py</code> <pre><code>@classmethod\ndef default(cls, input_dim: int, scale: float = 0.5) -&gt; \"Prior\":\n    \"\"\"Convenience constructor with MVP defaults.\"\"\"\n    return cls(input_dim=input_dim, scale=scale)\n</code></pre>"},{"location":"reference/model/#psyphy.model.prior.Prior.log_prob","title":"log_prob","text":"<pre><code>log_prob(params: Params) -&gt; ndarray\n</code></pre> <p>Compute log prior density (up to a constant)</p> <p>MVP:     Isotropic Gaussian on log_diag Full WPPM mode:     Will implement structured prior over basis weights and     lengthscale-regularized covariance fields</p> Source code in <code>src/psyphy/model/prior.py</code> <pre><code>def log_prob(self, params: Params) -&gt; jnp.ndarray:\n    \"\"\"\n    Compute log prior density (up to a constant)\n\n    MVP:\n        Isotropic Gaussian on log_diag\n    Full WPPM mode:\n        Will implement structured prior over basis weights and\n        lengthscale-regularized covariance fields\n    \"\"\"\n    log_diag = params[\"log_diag\"]\n    var = self.scale**2\n    return -0.5 * jnp.sum((log_diag**2) / var)\n</code></pre>"},{"location":"reference/model/#psyphy.model.prior.Prior.sample_params","title":"sample_params","text":"<pre><code>sample_params(key: KeyArray) -&gt; Params\n</code></pre> <p>Sample initial parameters from the prior.</p> <p>MVP:     Returns {\"log_diag\": shape (input_dim,)}. Full WPPM mode:     Will also include basis weights, structured covariance params,     and hyperparameters for GP (variance_scale, lengthscale).</p> Source code in <code>src/psyphy/model/prior.py</code> <pre><code>def sample_params(self, key: jr.KeyArray) -&gt; Params:\n    \"\"\"\n    Sample initial parameters from the prior.\n\n    MVP:\n        Returns {\"log_diag\": shape (input_dim,)}.\n    Full WPPM mode:\n        Will also include basis weights, structured covariance params,\n        and hyperparameters for GP (variance_scale, lengthscale).\n    \"\"\"\n    log_diag = jr.normal(key, shape=(self.input_dim,)) * self.scale\n    return {\"log_diag\": log_diag}\n</code></pre>"},{"location":"reference/model/#noise","title":"Noise","text":""},{"location":"reference/model/#psyphy.model.noise","title":"noise","text":"<p>Classes:</p> Name Description <code>GaussianNoise</code> <code>StudentTNoise</code>"},{"location":"reference/model/#psyphy.model.noise.GaussianNoise","title":"GaussianNoise","text":"<pre><code>GaussianNoise(sigma: float = 1.0)\n</code></pre> <p>Methods:</p> Name Description <code>log_prob</code> <p>Attributes:</p> Name Type Description <code>sigma</code> <code>float</code>"},{"location":"reference/model/#psyphy.model.noise.GaussianNoise.sigma","title":"sigma","text":"<pre><code>sigma: float = 1.0\n</code></pre>"},{"location":"reference/model/#psyphy.model.noise.GaussianNoise.log_prob","title":"log_prob","text":"<pre><code>log_prob(residual: float) -&gt; float\n</code></pre> Source code in <code>src/psyphy/model/noise.py</code> <pre><code>def log_prob(self, residual: float) -&gt; float:\n    _ = residual\n    return -0.5\n</code></pre>"},{"location":"reference/model/#psyphy.model.noise.StudentTNoise","title":"StudentTNoise","text":"<pre><code>StudentTNoise(df: float = 3.0, scale: float = 1.0)\n</code></pre> <p>Methods:</p> Name Description <code>log_prob</code> <p>Attributes:</p> Name Type Description <code>df</code> <code>float</code> <code>scale</code> <code>float</code>"},{"location":"reference/model/#psyphy.model.noise.StudentTNoise.df","title":"df","text":"<pre><code>df: float = 3.0\n</code></pre>"},{"location":"reference/model/#psyphy.model.noise.StudentTNoise.scale","title":"scale","text":"<pre><code>scale: float = 1.0\n</code></pre>"},{"location":"reference/model/#psyphy.model.noise.StudentTNoise.log_prob","title":"log_prob","text":"<pre><code>log_prob(residual: float) -&gt; float\n</code></pre> Source code in <code>src/psyphy/model/noise.py</code> <pre><code>def log_prob(self, residual: float) -&gt; float:\n    _ = residual\n    return -0.5\n</code></pre>"},{"location":"reference/model/#tasks","title":"Tasks","text":""},{"location":"reference/model/#psyphy.model.task","title":"task","text":"task.py <p>Task likelihoods for different psychophysical experimetns.</p> <p>Each TaskLikelihood defines: - predict(params, stimuli, model, noise)     Map discriminability (computed by model) to probability of correct response.</p> <ul> <li>loglik(params, data, model, noise)     Compute log-likelihood of observed responses under this task.</li> </ul> <p>MVP implementation: - OddityTask (3AFC) and TwoAFC. - Both use simple sigmoid-like mappings of discriminability -&gt; performance - loglik implemented as Bernoulli log-prob with these predictions</p> <p>Forward compatibility (Full WPPM mode): - Tasks will call into WPPM for discriminability computed via Monte Carlo   observer simulations, not closed forms. - Noise models will be used explicitly to generate internal noisy reps. - This ensures the same API supports both MVP and Full WPPM mode.</p> Connections <ul> <li>WPPM delegates to task.predict and task.loglik (never re-implements likelihood)</li> <li>noise model is passed through from WPPM so tasks can simulate responses.</li> <li>we can define new tasks by subclassing TaskLikelihood and implementing   predict() and loglik().</li> </ul> <p>Classes:</p> Name Description <code>OddityTask</code> <p>Three-alternative forced-choice oddity task (MVP placeholder) (\"pick the odd-one out).</p> <code>TaskLikelihood</code> <p>Abstract base class for task likelihoods</p> <code>TwoAFC</code> <p>2-alternative forced-choice task (MVP placeholder).</p> <p>Attributes:</p> Name Type Description <code>Stimulus</code>"},{"location":"reference/model/#psyphy.model.task.Stimulus","title":"Stimulus","text":"<pre><code>Stimulus = Tuple[ndarray, ndarray]\n</code></pre>"},{"location":"reference/model/#psyphy.model.task.OddityTask","title":"OddityTask","text":"<pre><code>OddityTask(slope: float = 1.5)\n</code></pre> <p>               Bases: <code>TaskLikelihood</code></p> <p>Three-alternative forced-choice oddity task (MVP placeholder) (\"pick the odd-one out).</p> <p>Methods:</p> Name Description <code>loglik</code> <code>predict</code> <p>Attributes:</p> Name Type Description <code>chance_level</code> <code>float</code> <code>performance_range</code> <code>float</code> <code>slope</code> Source code in <code>src/psyphy/model/task.py</code> <pre><code>def __init__(self, slope: float = 1.5) -&gt; None:\n    self.slope = float(slope)\n    self.chance_level: float = 1.0 / 3.0\n    self.performance_range: float = 1.0 - self.chance_level\n</code></pre>"},{"location":"reference/model/#psyphy.model.task.OddityTask.chance_level","title":"chance_level","text":"<pre><code>chance_level: float = 1.0 / 3.0\n</code></pre>"},{"location":"reference/model/#psyphy.model.task.OddityTask.performance_range","title":"performance_range","text":"<pre><code>performance_range: float = 1.0 - chance_level\n</code></pre>"},{"location":"reference/model/#psyphy.model.task.OddityTask.slope","title":"slope","text":"<pre><code>slope = float(slope)\n</code></pre>"},{"location":"reference/model/#psyphy.model.task.OddityTask.loglik","title":"loglik","text":"<pre><code>loglik(\n    params: Any, data: Any, model: Any, noise: Any\n) -&gt; ndarray\n</code></pre> Source code in <code>src/psyphy/model/task.py</code> <pre><code>def loglik(self, params: Any, data: Any, model: Any, noise: Any) -&gt; jnp.ndarray:\n    refs, probes, responses = data.to_numpy()\n    ps = jnp.array([self.predict(params, (r, p), model, noise) for r, p in zip(refs, probes)])\n    eps = 1e-9\n    return jnp.sum(jnp.where(responses == 1, jnp.log(ps + eps), jnp.log(1.0 - ps + eps)))\n</code></pre>"},{"location":"reference/model/#psyphy.model.task.OddityTask.predict","title":"predict","text":"<pre><code>predict(\n    params: Any, stimuli: Stimulus, model: Any, noise: Any\n) -&gt; ndarray\n</code></pre> Source code in <code>src/psyphy/model/task.py</code> <pre><code>def predict(self, params: Any, stimuli: Stimulus, model: Any, noise: Any) -&gt; jnp.ndarray:\n    d = model.discriminability(params, stimuli)\n    g = 0.5 * (jnp.tanh(self.slope * d) + 1.0)\n    return self.chance_level + self.performance_range * g\n</code></pre>"},{"location":"reference/model/#psyphy.model.task.TaskLikelihood","title":"TaskLikelihood","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for task likelihoods</p> <p>Methods:</p> Name Description <code>loglik</code> <p>Compute log-likelihood of observed responses under this task</p> <code>predict</code> <p>Predict probability of correct response for a stimulus.</p>"},{"location":"reference/model/#psyphy.model.task.TaskLikelihood.loglik","title":"loglik","text":"<pre><code>loglik(\n    params: Any, data: Any, model: Any, noise: Any\n) -&gt; ndarray\n</code></pre> <p>Compute log-likelihood of observed responses under this task</p> Source code in <code>src/psyphy/model/task.py</code> <pre><code>@abstractmethod\ndef loglik(self, params: Any, data: Any, model: Any, noise: Any) -&gt; jnp.ndarray:\n    \"\"\"Compute log-likelihood of observed responses under this task\"\"\"\n    ...\n</code></pre>"},{"location":"reference/model/#psyphy.model.task.TaskLikelihood.predict","title":"predict","text":"<pre><code>predict(\n    params: Any, stimuli: Stimulus, model: Any, noise: Any\n) -&gt; ndarray\n</code></pre> <p>Predict probability of correct response for a stimulus.</p> Source code in <code>src/psyphy/model/task.py</code> <pre><code>@abstractmethod\ndef predict(self, params: Any, stimuli: Stimulus, model: Any, noise: Any) -&gt; jnp.ndarray:\n    \"\"\"Predict probability of correct response for a stimulus.\"\"\"\n    ...\n</code></pre>"},{"location":"reference/model/#psyphy.model.task.TwoAFC","title":"TwoAFC","text":"<pre><code>TwoAFC(slope: float = 2.0)\n</code></pre> <p>               Bases: <code>TaskLikelihood</code></p> <p>2-alternative forced-choice task (MVP placeholder).</p> <p>Methods:</p> Name Description <code>loglik</code> <code>predict</code> <p>Attributes:</p> Name Type Description <code>chance_level</code> <code>float</code> <code>performance_range</code> <code>float</code> <code>slope</code> Source code in <code>src/psyphy/model/task.py</code> <pre><code>def __init__(self, slope: float = 2.0) -&gt; None:\n    self.slope = float(slope)\n    self.chance_level: float = 0.5\n    self.performance_range: float = 1.0 - self.chance_level\n</code></pre>"},{"location":"reference/model/#psyphy.model.task.TwoAFC.chance_level","title":"chance_level","text":"<pre><code>chance_level: float = 0.5\n</code></pre>"},{"location":"reference/model/#psyphy.model.task.TwoAFC.performance_range","title":"performance_range","text":"<pre><code>performance_range: float = 1.0 - chance_level\n</code></pre>"},{"location":"reference/model/#psyphy.model.task.TwoAFC.slope","title":"slope","text":"<pre><code>slope = float(slope)\n</code></pre>"},{"location":"reference/model/#psyphy.model.task.TwoAFC.loglik","title":"loglik","text":"<pre><code>loglik(\n    params: Any, data: Any, model: Any, noise: Any\n) -&gt; ndarray\n</code></pre> Source code in <code>src/psyphy/model/task.py</code> <pre><code>def loglik(self, params: Any, data: Any, model: Any, noise: Any) -&gt; jnp.ndarray:\n    refs, probes, responses = data.to_numpy()\n    ps = jnp.array([self.predict(params, (r, p), model, noise) for r, p in zip(refs, probes)])\n    eps = 1e-9\n    return jnp.sum(jnp.where(responses == 1, jnp.log(ps + eps), jnp.log(1.0 - ps + eps)))\n</code></pre>"},{"location":"reference/model/#psyphy.model.task.TwoAFC.predict","title":"predict","text":"<pre><code>predict(\n    params: Any, stimuli: Stimulus, model: Any, noise: Any\n) -&gt; ndarray\n</code></pre> Source code in <code>src/psyphy/model/task.py</code> <pre><code>def predict(self, params: Any, stimuli: Stimulus, model: Any, noise: Any) -&gt; jnp.ndarray:\n    d = model.discriminability(params, stimuli)\n    return self.chance_level + self.performance_range * jnp.tanh(self.slope * d)\n</code></pre>"},{"location":"reference/overview/","title":"Psyphy","text":""},{"location":"reference/overview/#psyphy","title":"psyphy","text":""},{"location":"reference/overview/#psyphy--psyphy","title":"psyphy","text":"<p>Psychophysical modeling and adaptive trial placement.</p> <p>This package implements the Wishart Process Psychophysical Model (WPPM) with modular components for priors, task likelihoods, and noise models,  which can be fitted to incoming subject data and used to adaptively select new trials to present to the subject next.  This is useful for efficiently estimating psychophysical parameters (e.g. threshold contours) with minimal trials.</p> Workflow Core design <ol> <li>WPPM (model/wppm.py):</li> <li>Structural definition of the psychophysical model.</li> <li>Maintains parameterization of local covariance fields.</li> <li>Computes discriminability between stimuli.</li> <li> <p>Delegates trial likelihoods and predictions to the task.</p> </li> <li> <p>Prior (model/prior.py):</p> </li> <li>Defines the distribution over model parameters.</li> <li>MVP: Gaussian prior over diagonal log-variances.</li> <li> <p>Full WPPM mode: structured prior over basis weights and      lengthscale-controlled covariance fields.</p> </li> <li> <p>TaskLikelihood (model/task.py):</p> </li> <li>Encodes the psychophysical decision rule.</li> <li>MVP: OddityTask (3AFC) and TwoAFC with sigmoid mappings.</li> <li> <p>Full WPPM mode: loglik and predict implemented via Monte Carlo      observer simulations, using the noise model explicitly.</p> </li> <li> <p>NoiseModel (model/noise.py):</p> </li> <li>Defines the distribution of internal representation noise.</li> <li>MVP: GaussianNoise (zero mean, isotropic).</li> <li>Full WPPM mode: add StudentTNoise option and  beyond.</li> </ol> Unified import style <p>Top-level (core models + session):   from psyphy import WPPM, Prior, OddityTask, GaussianNoise, MAPOptimizer   from psyphy import ExperimentSession, ResponseData, TrialBatch</p> <p>Subpackages:   from psyphy.model import WPPM, Prior, OddityTask, TwoAFC, GaussianNoise, StudentTNoise   from psyphy.inference import MAPOptimizer, LangevinSampler, LaplaceApproximation   from psyphy.posterior import Posterior, effective_sample_size, rhat   from psyphy.trial_placement import GridPlacement, GreedyMAPPlacement, InfoGainPlacement, SobolPlacement, StaircasePlacement   from psyphy.utils import grid_candidates, sobol_candidates, custom_candidates, chebyshev_basis</p> Data flow <ul> <li>A ResponseData object (psyphy.data) contains trial stimuli and responses.</li> <li>WPPM.init_params(prior) samples parameter initialization.</li> <li>Inference engines optimize the log posterior:       log_posterior = task.loglik(params, data, model=WPPM, noise=NoiseModel)                     + prior.log_prob(params)</li> <li>Posterior predictions (p(correct), threshold ellipses) are always obtained   through WPPM delegating to TaskLikelihood.</li> </ul> Extensibility <ul> <li>To add a new task: subclass TaskLikelihood, implement predict/loglik.</li> <li>To add a new noise model: subclass NoiseModel, implement logpdf/sample.</li> <li>To upgrade from MVP -&gt; Full WPPM mode: replace local_covariance and   discriminability with basis-expansion Wishart process + MC simulation.</li> </ul> MVP vs Full WPPM mode <ul> <li>MVP is a diagonal-covariance, closed-form scaffold that runs out of the box.</li> <li>Full WPPM mode matches the published research model:</li> <li>Smooth covariance fields (Wishart process priors).</li> <li>Monte Carlo likelihood evaluation.</li> <li>Explicit noise model in predictions.</li> </ul> <p>Classes:</p> Name Description <code>ExperimentSession</code> <p>High-level experiment orchestrator.</p> <code>GaussianNoise</code> <code>LangevinSampler</code> <p>Langevin sampler (stub).</p> <code>LaplaceApproximation</code> <p>Laplace approximation around MAP estimate.</p> <code>MAPOptimizer</code> <p>MAP (Maximum A Posteriori) optimizer.</p> <code>OddityTask</code> <p>Three-alternative forced-choice oddity task (MVP placeholder) (\"pick the odd-one out).</p> <code>Posterior</code> <p>MVP Posterior (MAP only).</p> <code>Prior</code> <p>Prior distribution over WPPM parameters</p> <code>ResponseData</code> <p>Container for psychophysical trial data.</p> <code>StudentTNoise</code> <code>TrialBatch</code> <p>Container for a proposed batch of trials</p> <code>TwoAFC</code> <p>2-alternative forced-choice task (MVP placeholder).</p> <code>WPPM</code> <p>Wishart Process Psychophysical Model (WPPM).</p>"},{"location":"reference/overview/#psyphy.ExperimentSession","title":"ExperimentSession","text":"<pre><code>ExperimentSession(\n    model, inference, placement, init_placement=None\n)\n</code></pre> <p>High-level experiment orchestrator.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>WPPM</code> <p>(Psychophysical) model instance.</p> required <code>inference</code> <code>InferenceEngine</code> <p>Inference engine (MAP, Langevin, etc.).</p> required <code>placement</code> <code>TrialPlacement</code> <p>Adaptive trial placement strategy.</p> required <code>init_placement</code> <code>TrialPlacement</code> <p>Initial placement strategy (e.g., Sobol exploration).</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>data</code> <code>ResponseData</code> <p>Stores all collected trials.</p> <code>posterior</code> <code>Posterior or None</code> <p>Current posterior estimate (None before initialization).</p> <p>Methods:</p> Name Description <code>initialize</code> <p>Fit an initial posterior before any adaptive placement.</p> <code>next_batch</code> <p>Propose the next batch of trials.</p> <code>update</code> <p>Refit posterior with accumulated data.</p> Source code in <code>src/psyphy/session/experiment_session.py</code> <pre><code>def __init__(self, model, inference, placement, init_placement=None):\n    self.model = model\n    self.inference = inference\n    self.placement = placement\n    self.init_placement = init_placement\n\n    # Data store starts empty\n    self.data = ResponseData()\n\n    # Posterior will be set after initialize() or update()\n    self.posterior = None\n</code></pre>"},{"location":"reference/overview/#psyphy.ExperimentSession.data","title":"data","text":"<pre><code>data = ResponseData()\n</code></pre>"},{"location":"reference/overview/#psyphy.ExperimentSession.inference","title":"inference","text":"<pre><code>inference = inference\n</code></pre>"},{"location":"reference/overview/#psyphy.ExperimentSession.init_placement","title":"init_placement","text":"<pre><code>init_placement = init_placement\n</code></pre>"},{"location":"reference/overview/#psyphy.ExperimentSession.model","title":"model","text":"<pre><code>model = model\n</code></pre>"},{"location":"reference/overview/#psyphy.ExperimentSession.placement","title":"placement","text":"<pre><code>placement = placement\n</code></pre>"},{"location":"reference/overview/#psyphy.ExperimentSession.posterior","title":"posterior","text":"<pre><code>posterior = None\n</code></pre>"},{"location":"reference/overview/#psyphy.ExperimentSession.initialize","title":"initialize","text":"<pre><code>initialize()\n</code></pre> <p>Fit an initial posterior before any adaptive placement.</p> <p>Returns:</p> Type Description <code>Posterior</code> <p>Posterior object wrapping fitted parameters.</p> Notes <p>MVP:     Posterior is fitted to empty data (prior only). Full WPPM mode:     Could use pilot data or pre-collected trials along grid etc.</p> Source code in <code>src/psyphy/session/experiment_session.py</code> <pre><code>def initialize(self):\n    \"\"\"\n    Fit an initial posterior before any adaptive placement.\n\n    Returns\n    -------\n    Posterior\n        Posterior object wrapping fitted parameters.\n\n    Notes\n    -----\n    MVP:\n        Posterior is fitted to empty data (prior only).\n    Full WPPM mode:\n        Could use pilot data or pre-collected trials along grid etc.\n    \"\"\"\n    self.posterior = self.inference.fit(self.model, self.data)\n    return self.posterior\n</code></pre>"},{"location":"reference/overview/#psyphy.ExperimentSession.next_batch","title":"next_batch","text":"<pre><code>next_batch(batch_size: int)\n</code></pre> <p>Propose the next batch of trials.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of trials to propose.</p> required <p>Returns:</p> Type Description <code>TrialBatch</code> <p>Batch of proposed (reference, probe) stimuli.</p> Notes <p>MVP:     Always calls placement.propose() on current posterior. Full WPPM mode:     Could support hybrid placement (init strategy -&gt; adaptive strategy).</p> Source code in <code>src/psyphy/session/experiment_session.py</code> <pre><code>def next_batch(self, batch_size: int):\n    \"\"\"\n    Propose the next batch of trials.\n\n    Parameters\n    ----------\n    batch_size : int\n        Number of trials to propose.\n\n    Returns\n    -------\n    TrialBatch\n        Batch of proposed (reference, probe) stimuli.\n\n    Notes\n    -----\n    MVP:\n        Always calls placement.propose() on current posterior.\n    Full WPPM mode:\n        Could support hybrid placement (init strategy -&gt; adaptive strategy).\n    \"\"\"\n    if self.posterior is None:\n        raise RuntimeError(\"Posterior not initialized. Call initialize() first.\")\n    return self.placement.propose(self.posterior, batch_size)\n</code></pre>"},{"location":"reference/overview/#psyphy.ExperimentSession.update","title":"update","text":"<pre><code>update()\n</code></pre> <p>Refit posterior with accumulated data.</p> <p>Returns:</p> Type Description <code>Posterior</code> <p>Updated posterior.</p> Notes <p>MVP:     Re-optimizes from scratch using all data. Full WPPM mode:     Could support warm-start or online parameter updates.</p> Source code in <code>src/psyphy/session/experiment_session.py</code> <pre><code>def update(self):\n    \"\"\"\n    Refit posterior with accumulated data.\n\n    Returns\n    -------\n    Posterior\n        Updated posterior.\n\n    Notes\n    -----\n    MVP:\n        Re-optimizes from scratch using all data.\n    Full WPPM mode:\n        Could support warm-start or online parameter updates.\n    \"\"\"\n    self.posterior = self.inference.fit(self.model, self.data)\n    return self.posterior\n</code></pre>"},{"location":"reference/overview/#psyphy.GaussianNoise","title":"GaussianNoise","text":"<pre><code>GaussianNoise(sigma: float = 1.0)\n</code></pre> <p>Methods:</p> Name Description <code>log_prob</code> <p>Attributes:</p> Name Type Description <code>sigma</code> <code>float</code>"},{"location":"reference/overview/#psyphy.GaussianNoise.sigma","title":"sigma","text":"<pre><code>sigma: float = 1.0\n</code></pre>"},{"location":"reference/overview/#psyphy.GaussianNoise.log_prob","title":"log_prob","text":"<pre><code>log_prob(residual: float) -&gt; float\n</code></pre> Source code in <code>src/psyphy/model/noise.py</code> <pre><code>def log_prob(self, residual: float) -&gt; float:\n    _ = residual\n    return -0.5\n</code></pre>"},{"location":"reference/overview/#psyphy.LangevinSampler","title":"LangevinSampler","text":"<pre><code>LangevinSampler(\n    steps: int = 1000,\n    step_size: float = 0.001,\n    temperature: float = 1.0,\n)\n</code></pre> <p>Langevin sampler (stub).</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of Langevin steps.</p> <code>1000</code> <code>step_size</code> <code>float</code> <p>Integration step size.</p> <code>1e-3</code> <code>temperature</code> <code>float</code> <p>Noise scale (temperature).</p> <code>1.0</code> <p>Methods:</p> Name Description <code>fit</code> <p>Fit model parameters with Langevin dynamics (stub).</p> <p>Attributes:</p> Name Type Description <code>step_size</code> <code>steps</code> <code>temperature</code> Source code in <code>src/psyphy/inference/langevin.py</code> <pre><code>def __init__(self, steps: int = 1000, step_size: float = 1e-3, temperature: float = 1.0):\n    self.steps = steps\n    self.step_size = step_size\n    self.temperature = temperature\n</code></pre>"},{"location":"reference/overview/#psyphy.LangevinSampler.step_size","title":"step_size","text":"<pre><code>step_size = step_size\n</code></pre>"},{"location":"reference/overview/#psyphy.LangevinSampler.steps","title":"steps","text":"<pre><code>steps = steps\n</code></pre>"},{"location":"reference/overview/#psyphy.LangevinSampler.temperature","title":"temperature","text":"<pre><code>temperature = temperature\n</code></pre>"},{"location":"reference/overview/#psyphy.LangevinSampler.fit","title":"fit","text":"<pre><code>fit(model, data) -&gt; Posterior\n</code></pre> <p>Fit model parameters with Langevin dynamics (stub).</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>WPPM</code> <p>Model instance.</p> required <code>data</code> <code>ResponseData</code> <p>Observed trials.</p> required <p>Returns:</p> Type Description <code>Posterior</code> <p>Posterior wrapper (MVP: params from init).</p> Source code in <code>src/psyphy/inference/langevin.py</code> <pre><code>def fit(self, model, data) -&gt; Posterior:\n    \"\"\"\n    Fit model parameters with Langevin dynamics (stub).\n\n    Parameters\n    ----------\n    model : WPPM\n        Model instance.\n    data : ResponseData\n        Observed trials.\n\n    Returns\n    -------\n    Posterior\n        Posterior wrapper (MVP: params from init).\n    \"\"\"\n    return Posterior(params=model.init_params(None), model=model)\n</code></pre>"},{"location":"reference/overview/#psyphy.LaplaceApproximation","title":"LaplaceApproximation","text":"<p>Laplace approximation around MAP estimate.</p> <p>Methods:</p> Name Description <code>from_map</code> <p>Construct a Gaussian approximation centered at MAP.</p>"},{"location":"reference/overview/#psyphy.LaplaceApproximation.from_map","title":"from_map","text":"<pre><code>from_map(map_posterior: Posterior) -&gt; Posterior\n</code></pre> <p>Return posterior approximation from MAP.</p> <p>Parameters:</p> Name Type Description Default <code>map_posterior</code> <code>Posterior</code> <p>Posterior object from MAP optimization.</p> required <p>Returns:</p> Type Description <code>Posterior</code> <p>Same posterior object (MVP).</p> Source code in <code>src/psyphy/inference/laplace.py</code> <pre><code>def from_map(self, map_posterior: Posterior) -&gt; Posterior:\n    \"\"\"\n    Return posterior approximation from MAP.\n\n    Parameters\n    ----------\n    map_posterior : Posterior\n        Posterior object from MAP optimization.\n\n    Returns\n    -------\n    Posterior\n        Same posterior object (MVP).\n    \"\"\"\n    return map_posterior\n</code></pre>"},{"location":"reference/overview/#psyphy.MAPOptimizer","title":"MAPOptimizer","text":"<pre><code>MAPOptimizer(\n    steps: int = 500,\n    optimizer: GradientTransformation | None = None,\n)\n</code></pre> <p>               Bases: <code>InferenceEngine</code></p> <p>MAP (Maximum A Posteriori) optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>Number of optimization steps.</p> <code>500</code> <code>optimizer</code> <code>GradientTransformation</code> <p>Optax optimizer to use. Default: SGD with momentum.</p> <code>None</code> Notes <ul> <li>Loss function = negative log posterior.</li> <li>Gradients computed with jax.grad.</li> </ul> <p>Methods:</p> Name Description <code>fit</code> <p>Fit model parameters with MAP optimization.</p> <p>Attributes:</p> Name Type Description <code>optimizer</code> <code>steps</code> Source code in <code>src/psyphy/inference/map_optimizer.py</code> <pre><code>def __init__(self, steps: int = 500, optimizer: optax.GradientTransformation | None = None):\n    self.steps = steps\n    self.optimizer = optimizer or optax.sgd(learning_rate=5e-5, momentum=0.9)\n</code></pre>"},{"location":"reference/overview/#psyphy.MAPOptimizer.optimizer","title":"optimizer","text":"<pre><code>optimizer = optimizer or sgd(\n    learning_rate=5e-05, momentum=0.9\n)\n</code></pre>"},{"location":"reference/overview/#psyphy.MAPOptimizer.steps","title":"steps","text":"<pre><code>steps = steps\n</code></pre>"},{"location":"reference/overview/#psyphy.MAPOptimizer.fit","title":"fit","text":"<pre><code>fit(model, data) -&gt; Posterior\n</code></pre> <p>Fit model parameters with MAP optimization.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>WPPM</code> <p>Model instance.</p> required <code>data</code> <code>ResponseData</code> <p>Observed trials.</p> required <p>Returns:</p> Type Description <code>Posterior</code> <p>Posterior wrapper around MAP params and model.</p> Source code in <code>src/psyphy/inference/map_optimizer.py</code> <pre><code>def fit(self, model, data) -&gt; Posterior:\n    \"\"\"\n    Fit model parameters with MAP optimization.\n\n    Parameters\n    ----------\n    model : WPPM\n        Model instance.\n    data : ResponseData\n        Observed trials.\n\n    Returns\n    -------\n    Posterior\n        Posterior wrapper around MAP params and model.\n    \"\"\"\n\n    def loss_fn(params):\n        return -model.log_posterior_from_data(params, data)\n\n    params = model.init_params(jax.random.PRNGKey(0))\n    opt_state = self.optimizer.init(params)\n\n    @jax.jit\n    def step(params, opt_state):\n        loss, grads = jax.value_and_grad(loss_fn)(params)\n        updates, opt_state = self.optimizer.update(grads, opt_state, params)\n        params = optax.apply_updates(params, updates)\n        return params, opt_state, loss\n\n    for _ in range(self.steps):\n        params, opt_state, loss = step(params, opt_state)\n\n    return Posterior(params=params, model=model)\n</code></pre>"},{"location":"reference/overview/#psyphy.OddityTask","title":"OddityTask","text":"<pre><code>OddityTask(slope: float = 1.5)\n</code></pre> <p>               Bases: <code>TaskLikelihood</code></p> <p>Three-alternative forced-choice oddity task (MVP placeholder) (\"pick the odd-one out).</p> <p>Methods:</p> Name Description <code>loglik</code> <code>predict</code> <p>Attributes:</p> Name Type Description <code>chance_level</code> <code>float</code> <code>performance_range</code> <code>float</code> <code>slope</code> Source code in <code>src/psyphy/model/task.py</code> <pre><code>def __init__(self, slope: float = 1.5) -&gt; None:\n    self.slope = float(slope)\n    self.chance_level: float = 1.0 / 3.0\n    self.performance_range: float = 1.0 - self.chance_level\n</code></pre>"},{"location":"reference/overview/#psyphy.OddityTask.chance_level","title":"chance_level","text":"<pre><code>chance_level: float = 1.0 / 3.0\n</code></pre>"},{"location":"reference/overview/#psyphy.OddityTask.performance_range","title":"performance_range","text":"<pre><code>performance_range: float = 1.0 - chance_level\n</code></pre>"},{"location":"reference/overview/#psyphy.OddityTask.slope","title":"slope","text":"<pre><code>slope = float(slope)\n</code></pre>"},{"location":"reference/overview/#psyphy.OddityTask.loglik","title":"loglik","text":"<pre><code>loglik(\n    params: Any, data: Any, model: Any, noise: Any\n) -&gt; ndarray\n</code></pre> Source code in <code>src/psyphy/model/task.py</code> <pre><code>def loglik(self, params: Any, data: Any, model: Any, noise: Any) -&gt; jnp.ndarray:\n    refs, probes, responses = data.to_numpy()\n    ps = jnp.array([self.predict(params, (r, p), model, noise) for r, p in zip(refs, probes)])\n    eps = 1e-9\n    return jnp.sum(jnp.where(responses == 1, jnp.log(ps + eps), jnp.log(1.0 - ps + eps)))\n</code></pre>"},{"location":"reference/overview/#psyphy.OddityTask.predict","title":"predict","text":"<pre><code>predict(\n    params: Any, stimuli: Stimulus, model: Any, noise: Any\n) -&gt; ndarray\n</code></pre> Source code in <code>src/psyphy/model/task.py</code> <pre><code>def predict(self, params: Any, stimuli: Stimulus, model: Any, noise: Any) -&gt; jnp.ndarray:\n    d = model.discriminability(params, stimuli)\n    g = 0.5 * (jnp.tanh(self.slope * d) + 1.0)\n    return self.chance_level + self.performance_range * g\n</code></pre>"},{"location":"reference/overview/#psyphy.Posterior","title":"Posterior","text":"<pre><code>Posterior(params, model)\n</code></pre> <p>               Bases: <code>BasePosterior</code></p> <p>MVP Posterior (MAP only).</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>MAP parameter dictionary.</p> required <code>model</code> <code>WPPM</code> <p>Model instance used for predictions.</p> required Notes <ul> <li>This is effectively a MAPPosterior.</li> <li>Future subclasses (LaplacePosterior, MCMCPosterior) will extend   BasePosterior with real sampling logic.</li> </ul> <p>Methods:</p> Name Description <code>MAP_params</code> <p>Return the MAP parameters.</p> <code>predict_prob</code> <p>Predict probability of correct response for a stimulus.</p> <code>predict_thresholds</code> <p>Predict discrimination threshold contour around a reference stimulus.</p> <code>sample</code> <p>Draw parameter samples from the posterior.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>params</code> Source code in <code>src/psyphy/posterior/posterior.py</code> <pre><code>def __init__(self, params, model):\n    self.params = params\n    self.model = model\n</code></pre>"},{"location":"reference/overview/#psyphy.Posterior.model","title":"model","text":"<pre><code>model = model\n</code></pre>"},{"location":"reference/overview/#psyphy.Posterior.params","title":"params","text":"<pre><code>params = params\n</code></pre>"},{"location":"reference/overview/#psyphy.Posterior.MAP_params","title":"MAP_params","text":"<pre><code>MAP_params()\n</code></pre> <p>Return the MAP parameters.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Parameter dictionary.</p> Source code in <code>src/psyphy/posterior/posterior.py</code> <pre><code>def MAP_params(self):\n    \"\"\"\n    Return the MAP parameters.\n\n    Returns\n    -------\n    dict\n        Parameter dictionary.\n    \"\"\"\n    return self.params\n</code></pre>"},{"location":"reference/overview/#psyphy.Posterior.predict_prob","title":"predict_prob","text":"<pre><code>predict_prob(stimulus)\n</code></pre> <p>Predict probability of correct response for a stimulus.</p> <p>Parameters:</p> Name Type Description Default <code>stimulus</code> <code>tuple</code> <p>(reference, probe).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Probability of correct response.</p> Notes <p>Delegates to WPPM.predict_prob(). This is not recursion: Posterior calls WPPM\u2019s method with stored params.</p> Source code in <code>src/psyphy/posterior/posterior.py</code> <pre><code>def predict_prob(self, stimulus):\n    \"\"\"\n    Predict probability of correct response for a stimulus.\n\n    Parameters\n    ----------\n    stimulus : tuple\n        (reference, probe).\n\n    Returns\n    -------\n    jnp.ndarray\n        Probability of correct response.\n\n    Notes\n    -----\n    Delegates to WPPM.predict_prob().\n    This is not recursion: Posterior calls WPPM\u2019s method with stored params.\n    \"\"\"\n    return self.model.predict_prob(self.params, stimulus)\n</code></pre>"},{"location":"reference/overview/#psyphy.Posterior.predict_thresholds","title":"predict_thresholds","text":"<pre><code>predict_thresholds(\n    reference,\n    criterion: float = 0.667,\n    directions: int = 16,\n)\n</code></pre> <p>Predict discrimination threshold contour around a reference stimulus.</p> <p>Parameters:</p> Name Type Description Default <code>reference</code> <code>ndarray</code> <p>Reference point in model space.</p> required <code>criterion</code> <code>float</code> <p>Target performance (e.g., 2/3 for oddity).</p> <code>0.667</code> <code>directions</code> <code>int</code> <p>Number of directions to probe.</p> <code>16</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Contour points (MVP: unit circle).</p> MVP <p>Returns a placeholder unit circle.</p> Future <ul> <li>Search outward in each direction until performance crosses criterion.</li> <li>Average over posterior samples (Laplace, MCMC) to get credible intervals.</li> </ul> Source code in <code>src/psyphy/posterior/posterior.py</code> <pre><code>def predict_thresholds(self, reference, criterion: float = 0.667, directions: int = 16):\n    \"\"\"\n    Predict discrimination threshold contour around a reference stimulus.\n\n    Parameters\n    ----------\n    reference : jnp.ndarray\n        Reference point in model space.\n    criterion : float, default=0.667\n        Target performance (e.g., 2/3 for oddity).\n    directions : int, default=16\n        Number of directions to probe.\n\n    Returns\n    -------\n    jnp.ndarray\n        Contour points (MVP: unit circle).\n\n    MVP\n    ---\n    Returns a placeholder unit circle.\n\n    Future\n    ------\n    - Search outward in each direction until performance crosses criterion.\n    - Average over posterior samples (Laplace, MCMC) to get credible intervals.\n    \"\"\"\n    angles = jnp.linspace(0, 2 * jnp.pi, directions, endpoint=False)\n    contour = jnp.stack([reference + jnp.array([jnp.cos(a), jnp.sin(a)]) for a in angles])\n    return contour\n</code></pre>"},{"location":"reference/overview/#psyphy.Posterior.sample","title":"sample","text":"<pre><code>sample(num_samples: int = 1)\n</code></pre> <p>Draw parameter samples from the posterior.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>Number of samples.</p> <code>1</code> <p>Returns:</p> Type Description <code>list of dict</code> <p>Parameter sets.</p> MVP <p>Returns MAP params repeated n times.</p> Future <ul> <li>LaplacePosterior: draw from N(mean, cov).</li> <li>MCMCPosterior: return stored samples.</li> </ul> Source code in <code>src/psyphy/posterior/posterior.py</code> <pre><code>def sample(self, num_samples: int = 1):\n    \"\"\"\n    Draw parameter samples from the posterior.\n\n    Parameters\n    ----------\n    num_samples : int, default=1\n        Number of samples.\n\n    Returns\n    -------\n    list of dict\n        Parameter sets.\n\n    MVP\n    ---\n    Returns MAP params repeated n times.\n\n    Future\n    ------\n    - LaplacePosterior: draw from N(mean, cov).\n    - MCMCPosterior: return stored samples.\n    \"\"\"\n    return [self.params] * num_samples\n</code></pre>"},{"location":"reference/overview/#psyphy.Prior","title":"Prior","text":"<pre><code>Prior(\n    input_dim: int,\n    scale: float = 0.5,\n    variance_scale: float = 1.0,\n    lengthscale: float = 1.0,\n    extra_embedding_dims: int = 0,\n)\n</code></pre> <p>Prior distribution over WPPM parameters</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimensionality of the model space (same as WPPM.input_dim)</p> required <code>scale</code> <code>float</code> <p>Stddev of Gaussian prior for log_diag entries (MVP only).</p> <code>0.5</code> <code>variance_scale</code> <code>float</code> <p>Forward-compatible stub for Full WPPM mode. Will scale covariance magnitudes</p> <code>1.0</code> <code>lengthscale</code> <code>float</code> <p>Forward-compatible stub for Full WPPM mode; controls smoothness of covariance field: - small lengthscale --&gt; rapid variation across space - large lengthscale --&gt; smoother field, long-range correlations.</p> <code>1.0</code> <code>extra_embedding_dims</code> <code>int</code> <p>Forward-compatible stub for Full WPPM mode. Will expand embedding space.</p> <code>0</code> <p>Methods:</p> Name Description <code>default</code> <p>Convenience constructor with MVP defaults.</p> <code>log_prob</code> <p>Compute log prior density (up to a constant)</p> <code>sample_params</code> <p>Sample initial parameters from the prior.</p> <p>Attributes:</p> Name Type Description <code>extra_embedding_dims</code> <code>int</code> <code>input_dim</code> <code>int</code> <code>lengthscale</code> <code>float</code> <code>scale</code> <code>float</code> <code>variance_scale</code> <code>float</code>"},{"location":"reference/overview/#psyphy.Prior.extra_embedding_dims","title":"extra_embedding_dims","text":"<pre><code>extra_embedding_dims: int = 0\n</code></pre>"},{"location":"reference/overview/#psyphy.Prior.input_dim","title":"input_dim","text":"<pre><code>input_dim: int\n</code></pre>"},{"location":"reference/overview/#psyphy.Prior.lengthscale","title":"lengthscale","text":"<pre><code>lengthscale: float = 1.0\n</code></pre>"},{"location":"reference/overview/#psyphy.Prior.scale","title":"scale","text":"<pre><code>scale: float = 0.5\n</code></pre>"},{"location":"reference/overview/#psyphy.Prior.variance_scale","title":"variance_scale","text":"<pre><code>variance_scale: float = 1.0\n</code></pre>"},{"location":"reference/overview/#psyphy.Prior.default","title":"default","text":"<pre><code>default(input_dim: int, scale: float = 0.5) -&gt; 'Prior'\n</code></pre> <p>Convenience constructor with MVP defaults.</p> Source code in <code>src/psyphy/model/prior.py</code> <pre><code>@classmethod\ndef default(cls, input_dim: int, scale: float = 0.5) -&gt; \"Prior\":\n    \"\"\"Convenience constructor with MVP defaults.\"\"\"\n    return cls(input_dim=input_dim, scale=scale)\n</code></pre>"},{"location":"reference/overview/#psyphy.Prior.log_prob","title":"log_prob","text":"<pre><code>log_prob(params: Params) -&gt; ndarray\n</code></pre> <p>Compute log prior density (up to a constant)</p> <p>MVP:     Isotropic Gaussian on log_diag Full WPPM mode:     Will implement structured prior over basis weights and     lengthscale-regularized covariance fields</p> Source code in <code>src/psyphy/model/prior.py</code> <pre><code>def log_prob(self, params: Params) -&gt; jnp.ndarray:\n    \"\"\"\n    Compute log prior density (up to a constant)\n\n    MVP:\n        Isotropic Gaussian on log_diag\n    Full WPPM mode:\n        Will implement structured prior over basis weights and\n        lengthscale-regularized covariance fields\n    \"\"\"\n    log_diag = params[\"log_diag\"]\n    var = self.scale**2\n    return -0.5 * jnp.sum((log_diag**2) / var)\n</code></pre>"},{"location":"reference/overview/#psyphy.Prior.sample_params","title":"sample_params","text":"<pre><code>sample_params(key: KeyArray) -&gt; Params\n</code></pre> <p>Sample initial parameters from the prior.</p> <p>MVP:     Returns {\"log_diag\": shape (input_dim,)}. Full WPPM mode:     Will also include basis weights, structured covariance params,     and hyperparameters for GP (variance_scale, lengthscale).</p> Source code in <code>src/psyphy/model/prior.py</code> <pre><code>def sample_params(self, key: jr.KeyArray) -&gt; Params:\n    \"\"\"\n    Sample initial parameters from the prior.\n\n    MVP:\n        Returns {\"log_diag\": shape (input_dim,)}.\n    Full WPPM mode:\n        Will also include basis weights, structured covariance params,\n        and hyperparameters for GP (variance_scale, lengthscale).\n    \"\"\"\n    log_diag = jr.normal(key, shape=(self.input_dim,)) * self.scale\n    return {\"log_diag\": log_diag}\n</code></pre>"},{"location":"reference/overview/#psyphy.ResponseData","title":"ResponseData","text":"<pre><code>ResponseData()\n</code></pre> <p>Container for psychophysical trial data.</p> <p>Attributes:</p> Name Type Description <code>refs</code> <code>List[Any]</code> <p>List of reference stimuli.</p> <code>probes</code> <code>List[Any]</code> <p>List of probe stimuli.</p> <code>responses</code> <code>List[int]</code> <p>List of subject responses (e.g., 0/1 or categorical).</p> <p>Methods:</p> Name Description <code>add_batch</code> <p>Append responses for a batch of trials.</p> <code>add_trial</code> <p>append a single trial.</p> <code>to_numpy</code> <p>Return refs, probes, responses as numpy arrays.</p> Source code in <code>src/psyphy/data/dataset.py</code> <pre><code>def __init__(self) -&gt; None:\n    self.refs: List[Any] = []\n    self.probes: List[Any] = []\n    self.responses: List[int] = []\n</code></pre>"},{"location":"reference/overview/#psyphy.ResponseData.probes","title":"probes","text":"<pre><code>probes: List[Any] = []\n</code></pre>"},{"location":"reference/overview/#psyphy.ResponseData.refs","title":"refs","text":"<pre><code>refs: List[Any] = []\n</code></pre>"},{"location":"reference/overview/#psyphy.ResponseData.responses","title":"responses","text":"<pre><code>responses: List[int] = []\n</code></pre>"},{"location":"reference/overview/#psyphy.ResponseData.add_batch","title":"add_batch","text":"<pre><code>add_batch(\n    responses: List[int], trial_batch: TrialBatch\n) -&gt; None\n</code></pre> <p>Append responses for a batch of trials.</p> <p>Parameters:</p> Name Type Description Default <code>responses</code> <code>List[int]</code> <p>Responses corresponding to each (ref, probe) in the trial batch.</p> required <code>trial_batch</code> <code>TrialBatch</code> <p>The batch of proposed trials.</p> required Source code in <code>src/psyphy/data/dataset.py</code> <pre><code>def add_batch(self, responses: List[int], trial_batch: TrialBatch) -&gt; None:\n    \"\"\"\n    Append responses for a batch of trials.\n\n    Parameters\n    ----------\n    responses : List[int]\n        Responses corresponding to each (ref, probe) in the trial batch.\n    trial_batch : TrialBatch\n        The batch of proposed trials.\n    \"\"\"\n    for (ref, probe), resp in zip(trial_batch.stimuli, responses):\n        self.add_trial(ref, probe, resp)\n</code></pre>"},{"location":"reference/overview/#psyphy.ResponseData.add_trial","title":"add_trial","text":"<pre><code>add_trial(ref: Any, probe: Any, resp: int) -&gt; None\n</code></pre> <p>append a single trial.</p> <p>Parameters:</p> Name Type Description Default <code>ref</code> <code>Any</code> <p>Reference stimulus (numpy array, list, etc.)</p> required <code>probe</code> <code>Any</code> <p>Probe stimulus</p> required <code>resp</code> <code>int</code> <p>Subject response (binary or categorical)</p> required Source code in <code>src/psyphy/data/dataset.py</code> <pre><code>def add_trial(self, ref: Any, probe: Any, resp: int) -&gt; None:\n    \"\"\"\n    append a single trial.\n\n    Parameters\n    ----------\n    ref : Any\n        Reference stimulus (numpy array, list, etc.)\n    probe : Any\n        Probe stimulus\n    resp : int\n        Subject response (binary or categorical)\n    \"\"\"\n    self.refs.append(ref)\n    self.probes.append(probe)\n    self.responses.append(resp)\n</code></pre>"},{"location":"reference/overview/#psyphy.ResponseData.to_numpy","title":"to_numpy","text":"<pre><code>to_numpy() -&gt; Tuple[ndarray, ndarray, ndarray]\n</code></pre> <p>Return refs, probes, responses as numpy arrays.</p> <p>Returns:</p> Name Type Description <code>refs</code> <code>ndarray</code> <code>probes</code> <code>ndarray</code> <code>responses</code> <code>ndarray</code> Source code in <code>src/psyphy/data/dataset.py</code> <pre><code>def to_numpy(self) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Return refs, probes, responses as numpy arrays.\n\n    Returns\n    -------\n    refs : np.ndarray\n    probes : np.ndarray\n    responses : np.ndarray\n    \"\"\"\n    return (\n        np.array(self.refs),\n        np.array(self.probes),\n        np.array(self.responses),\n    )\n</code></pre>"},{"location":"reference/overview/#psyphy.StudentTNoise","title":"StudentTNoise","text":"<pre><code>StudentTNoise(df: float = 3.0, scale: float = 1.0)\n</code></pre> <p>Methods:</p> Name Description <code>log_prob</code> <p>Attributes:</p> Name Type Description <code>df</code> <code>float</code> <code>scale</code> <code>float</code>"},{"location":"reference/overview/#psyphy.StudentTNoise.df","title":"df","text":"<pre><code>df: float = 3.0\n</code></pre>"},{"location":"reference/overview/#psyphy.StudentTNoise.scale","title":"scale","text":"<pre><code>scale: float = 1.0\n</code></pre>"},{"location":"reference/overview/#psyphy.StudentTNoise.log_prob","title":"log_prob","text":"<pre><code>log_prob(residual: float) -&gt; float\n</code></pre> Source code in <code>src/psyphy/model/noise.py</code> <pre><code>def log_prob(self, residual: float) -&gt; float:\n    _ = residual\n    return -0.5\n</code></pre>"},{"location":"reference/overview/#psyphy.TrialBatch","title":"TrialBatch","text":"<pre><code>TrialBatch(stimuli: List[Tuple[Any, Any]])\n</code></pre> <p>Container for a proposed batch of trials</p> <p>Attributes:</p> Name Type Description <code>stimuli</code> <code>List[Tuple[Any, Any]]</code> <p>Each trial is a (reference, probe) tuple.</p> <p>Methods:</p> Name Description <code>from_stimuli</code> <p>Construct a TrialBatch from a list of stimuli (ref, probe) pairs.</p> Source code in <code>src/psyphy/data/dataset.py</code> <pre><code>def __init__(self, stimuli: List[Tuple[Any, Any]]) -&gt; None:\n    self.stimuli = list(stimuli)\n</code></pre>"},{"location":"reference/overview/#psyphy.TrialBatch.stimuli","title":"stimuli","text":"<pre><code>stimuli = list(stimuli)\n</code></pre>"},{"location":"reference/overview/#psyphy.TrialBatch.from_stimuli","title":"from_stimuli","text":"<pre><code>from_stimuli(pairs: List[Tuple[Any, Any]]) -&gt; TrialBatch\n</code></pre> <p>Construct a TrialBatch from a list of stimuli (ref, probe) pairs.</p> Source code in <code>src/psyphy/data/dataset.py</code> <pre><code>@classmethod\ndef from_stimuli(cls, pairs: List[Tuple[Any, Any]]) -&gt; TrialBatch:\n    \"\"\"\n    Construct a TrialBatch from a list of stimuli (ref, probe) pairs.\n    \"\"\"\n    return cls(pairs)\n</code></pre>"},{"location":"reference/overview/#psyphy.TwoAFC","title":"TwoAFC","text":"<pre><code>TwoAFC(slope: float = 2.0)\n</code></pre> <p>               Bases: <code>TaskLikelihood</code></p> <p>2-alternative forced-choice task (MVP placeholder).</p> <p>Methods:</p> Name Description <code>loglik</code> <code>predict</code> <p>Attributes:</p> Name Type Description <code>chance_level</code> <code>float</code> <code>performance_range</code> <code>float</code> <code>slope</code> Source code in <code>src/psyphy/model/task.py</code> <pre><code>def __init__(self, slope: float = 2.0) -&gt; None:\n    self.slope = float(slope)\n    self.chance_level: float = 0.5\n    self.performance_range: float = 1.0 - self.chance_level\n</code></pre>"},{"location":"reference/overview/#psyphy.TwoAFC.chance_level","title":"chance_level","text":"<pre><code>chance_level: float = 0.5\n</code></pre>"},{"location":"reference/overview/#psyphy.TwoAFC.performance_range","title":"performance_range","text":"<pre><code>performance_range: float = 1.0 - chance_level\n</code></pre>"},{"location":"reference/overview/#psyphy.TwoAFC.slope","title":"slope","text":"<pre><code>slope = float(slope)\n</code></pre>"},{"location":"reference/overview/#psyphy.TwoAFC.loglik","title":"loglik","text":"<pre><code>loglik(\n    params: Any, data: Any, model: Any, noise: Any\n) -&gt; ndarray\n</code></pre> Source code in <code>src/psyphy/model/task.py</code> <pre><code>def loglik(self, params: Any, data: Any, model: Any, noise: Any) -&gt; jnp.ndarray:\n    refs, probes, responses = data.to_numpy()\n    ps = jnp.array([self.predict(params, (r, p), model, noise) for r, p in zip(refs, probes)])\n    eps = 1e-9\n    return jnp.sum(jnp.where(responses == 1, jnp.log(ps + eps), jnp.log(1.0 - ps + eps)))\n</code></pre>"},{"location":"reference/overview/#psyphy.TwoAFC.predict","title":"predict","text":"<pre><code>predict(\n    params: Any, stimuli: Stimulus, model: Any, noise: Any\n) -&gt; ndarray\n</code></pre> Source code in <code>src/psyphy/model/task.py</code> <pre><code>def predict(self, params: Any, stimuli: Stimulus, model: Any, noise: Any) -&gt; jnp.ndarray:\n    d = model.discriminability(params, stimuli)\n    return self.chance_level + self.performance_range * jnp.tanh(self.slope * d)\n</code></pre>"},{"location":"reference/overview/#psyphy.WPPM","title":"WPPM","text":"<pre><code>WPPM(\n    input_dim: int,\n    prior: Prior,\n    task: TaskLikelihood,\n    noise: Any | None = None,\n    *,\n    extra_dims: int = 0,\n    variance_scale: float = 1.0,\n    lengthscale: float = 1.0,\n    diag_term: float = 1e-06\n)\n</code></pre> <p>Wishart Process Psychophysical Model (WPPM).</p> <p>Parameters:</p> Name Type Description Default <code>input_dim</code> <code>int</code> <p>Dimensionality of the input stimulus space (e.g., 2 for isoluminant plane, 3 for RGB). Both reference and probe live in R^{input_dim}.</p> required <code>prior</code> <code>Prior</code> <p>Prior distribution over model parameters. MVP uses a simple Gaussian prior over diagonal log-variances (see Prior.sample_params()).</p> required <code>task</code> <code>TaskLikelihood</code> <p>Psychophysical task mapping that defines how discriminability translates to p(correct) and how log-likelihood of responses is computed. (e.g., OddityTask, TwoAFC)</p> required <code>noise</code> <code>Any</code> <p>Noise model describing internal representation noise (e.g., GaussianNoise). Not used in MVP mapping but passed to the task interface for future MC sims.</p> <code>None</code> Forward-compatible hyperparameters (MVP stubs) <p>extra_dims : int, default=0     Additional embedding dimensions for basis expansions (unused in MVP). variance_scale : float, default=1.0     Global scaling factor for covariance magnitude (unused in MVP). lengthscale : float, default=1.0     Smoothness/length-scale for spatial covariance variation (unused in MVP).     (formerly \"decay_rate\") diag_term : float, default=1e-6     Small positive value added to the covariance diagonal for numerical stability.     MVP uses this in matrix solves; the research model will also use it.</p> <p>Methods:</p> Name Description <code>discriminability</code> <p>Compute scalar discriminability d &gt;= 0 for a (reference, probe) pair</p> <code>init_params</code> <p>Sample initial parameters from the prior.</p> <code>local_covariance</code> <p>Return local covariance \u03a3(x) at stimulus location x.</p> <code>log_likelihood</code> <p>Compute the log-likelihood for arrays of trials.</p> <code>log_likelihood_from_data</code> <p>Compute log-likelihood directly from a ResponseData object.</p> <code>log_posterior_from_data</code> <p>Convenience helper if you want log posterior in one call (MVP).</p> <code>predict_prob</code> <p>Predict probability of a correct response for a single stimulus.</p> <p>Attributes:</p> Name Type Description <code>diag_term</code> <code>extra_dims</code> <code>input_dim</code> <code>lengthscale</code> <code>noise</code> <code>prior</code> <code>task</code> <code>variance_scale</code> Source code in <code>src/psyphy/model/wppm.py</code> <pre><code>def __init__(\n    self,\n    input_dim: int,\n    prior: Prior,\n    task: TaskLikelihood,\n    noise: Any | None = None,\n    *,\n    extra_dims: int = 0,\n    variance_scale: float = 1.0,\n    lengthscale: float = 1.0,\n    diag_term: float = 1e-6,\n) -&gt; None:\n    # --- core components ---\n    self.input_dim = int(input_dim)   # stimulus-space dimensionality\n    self.prior = prior                # prior over parameter PyTree\n    self.task = task                  # task mapping and likelihood\n    self.noise = noise                # noise model \n\n    # --- forward-compatible hyperparameters (stubs in MVP) ---\n    self.extra_dims = int(extra_dims)\n    self.variance_scale = float(variance_scale)\n    self.lengthscale = float(lengthscale)\n    self.diag_term = float(diag_term)\n</code></pre>"},{"location":"reference/overview/#psyphy.WPPM.diag_term","title":"diag_term","text":"<pre><code>diag_term = float(diag_term)\n</code></pre>"},{"location":"reference/overview/#psyphy.WPPM.extra_dims","title":"extra_dims","text":"<pre><code>extra_dims = int(extra_dims)\n</code></pre>"},{"location":"reference/overview/#psyphy.WPPM.input_dim","title":"input_dim","text":"<pre><code>input_dim = int(input_dim)\n</code></pre>"},{"location":"reference/overview/#psyphy.WPPM.lengthscale","title":"lengthscale","text":"<pre><code>lengthscale = float(lengthscale)\n</code></pre>"},{"location":"reference/overview/#psyphy.WPPM.noise","title":"noise","text":"<pre><code>noise = noise\n</code></pre>"},{"location":"reference/overview/#psyphy.WPPM.prior","title":"prior","text":"<pre><code>prior = prior\n</code></pre>"},{"location":"reference/overview/#psyphy.WPPM.task","title":"task","text":"<pre><code>task = task\n</code></pre>"},{"location":"reference/overview/#psyphy.WPPM.variance_scale","title":"variance_scale","text":"<pre><code>variance_scale = float(variance_scale)\n</code></pre>"},{"location":"reference/overview/#psyphy.WPPM.discriminability","title":"discriminability","text":"<pre><code>discriminability(\n    params: Params, stimulus: Stimulus\n) -&gt; ndarray\n</code></pre> <p>Compute scalar discriminability d &gt;= 0 for a (reference, probe) pair</p> <p>MVP:     d = sqrt( (probe - ref)^T \u03a3(ref)^{-1} (probe - ref) )     with \u03a3(ref) the local covariance at the reference,     - We add <code>diag_term * I</code> for numerical stability before inversion Future (full WPPM mode):     d is implicit via Monte Carlo simulation of internal noisy responses     under the task's decision rule (no closed form). In that case, tasks     will directly implement predict/loglik with MC, and this method may be     used only for diagnostics.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Model parameters.</p> required <code>stimulus</code> <code>tuple</code> <p>(reference, probe) arrays of shape (input_dim,).</p> required <p>Returns:</p> Name Type Description <code>d</code> <code>ndarray</code> <p>Nonnegative scalar discriminability.</p> Source code in <code>src/psyphy/model/wppm.py</code> <pre><code>def discriminability(self, params: Params, stimulus: Stimulus) -&gt; jnp.ndarray:\n    \"\"\"\n    Compute scalar discriminability d &gt;= 0 for a (reference, probe) pair\n\n    MVP:\n        d = sqrt( (probe - ref)^T \u03a3(ref)^{-1} (probe - ref) )\n        with \u03a3(ref) the local covariance at the reference,\n        - We add `diag_term * I` for numerical stability before inversion\n    Future (full WPPM mode):\n        d is implicit via Monte Carlo simulation of internal noisy responses\n        under the task's decision rule (no closed form). In that case, tasks\n        will directly implement predict/loglik with MC, and this method may be\n        used only for diagnostics.\n\n    Parameters\n    ----------\n    params : dict\n        Model parameters.\n    stimulus : tuple\n        (reference, probe) arrays of shape (input_dim,).\n\n    Returns\n    -------\n    d : jnp.ndarray\n        Nonnegative scalar discriminability.\n    \"\"\"\n    ref, probe = stimulus\n    delta = probe - ref                                # difference vector in input space\n    Sigma = self.local_covariance(params, ref)         # local covariance at reference\n    # Add jitter for stable solve; diag_term is configurable\n    jitter = self.diag_term * jnp.eye(self.input_dim)\n    # Solve (\u03a3 + jitter)^{-1} delta using a PD-aware solver\n    x = jax.scipy.linalg.solve(Sigma + jitter, delta, assume_a=\"pos\")\n    d2 = jnp.dot(delta, x)                             # quadratic form\n    # Guard against tiny negative values from numerical error\n    return jnp.sqrt(jnp.maximum(d2, 0.0))\n</code></pre>"},{"location":"reference/overview/#psyphy.WPPM.init_params","title":"init_params","text":"<pre><code>init_params(key: KeyArray) -&gt; Params\n</code></pre> <p>Sample initial parameters from the prior.</p> <p>MVP parameters:     {\"log_diag\": shape (input_dim,)} which defines a constant diagonal covariance across the space.</p> <p>Returns:</p> Name Type Description <code>params</code> <code>dict[str, ndarray]</code> Source code in <code>src/psyphy/model/wppm.py</code> <pre><code>def init_params(self, key: jr.KeyArray) -&gt; Params:\n    \"\"\"\n    Sample initial parameters from the prior.\n\n    MVP parameters:\n        {\"log_diag\": shape (input_dim,)}\n    which defines a constant diagonal covariance across the space.\n\n    Returns\n    -------\n    params : dict[str, jnp.ndarray]\n    \"\"\"\n    return self.prior.sample_params(key)\n</code></pre>"},{"location":"reference/overview/#psyphy.WPPM.local_covariance","title":"local_covariance","text":"<pre><code>local_covariance(params: Params, x: ndarray) -&gt; ndarray\n</code></pre> <p>Return local covariance \u03a3(x) at stimulus location x.</p> <p>MVP:     \u03a3(x) = diag(exp(log_diag)), constant across x.     - Positive-definite because exp(log_diag) &gt; 0. Future (full WPPM mode):     \u03a3(x) varies smoothly with x via basis expansions and a Wishart-process     prior controlled by (extra_dims, variance_scale, lengthscale). Those     hyperparameters are exposed here but not used in MVP.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>model parameters (MVP expects \"log_diag\": (input_dim,)).</p> required <code>x</code> <code>ndarray</code> <p>Stimulus location (unused in MVP because \u03a3 is constant).</p> required <p>Returns:</p> Type Description <code>\u03a3 : jnp.ndarray, shape (input_dim, input_dim)</code> Source code in <code>src/psyphy/model/wppm.py</code> <pre><code>def local_covariance(self, params: Params, x: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"\n    Return local covariance \u03a3(x) at stimulus location x.\n\n    MVP:\n        \u03a3(x) = diag(exp(log_diag)), constant across x.\n        - Positive-definite because exp(log_diag) &gt; 0.\n    Future (full WPPM mode):\n        \u03a3(x) varies smoothly with x via basis expansions and a Wishart-process\n        prior controlled by (extra_dims, variance_scale, lengthscale). Those\n        hyperparameters are exposed here but not used in MVP.\n\n    Parameters\n    ----------\n    params : dict\n        model parameters (MVP expects \"log_diag\": (input_dim,)).\n    x : jnp.ndarray\n        Stimulus location (unused in MVP because \u03a3 is constant).\n\n    Returns\n    -------\n    \u03a3 : jnp.ndarray, shape (input_dim, input_dim)\n    \"\"\"\n    log_diag = params[\"log_diag\"]               # unconstrained diagonal log-variances\n    diag = jnp.exp(log_diag)                    # enforce positivity\n    return jnp.diag(diag)                       # constant diagonal covariance\n</code></pre>"},{"location":"reference/overview/#psyphy.WPPM.log_likelihood","title":"log_likelihood","text":"<pre><code>log_likelihood(\n    params: Params,\n    refs: ndarray,\n    probes: ndarray,\n    responses: ndarray,\n) -&gt; ndarray\n</code></pre> <p>Compute the log-likelihood for arrays of trials.</p> <p>IMPORTANT:     We delegate to the TaskLikelihood to avoid duplicating Bernoulli (MPV)     or MC likelihood logic in multiple places. This keeps responsibilities     clean and makes adding new tasks straightforward.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Model parameters.</p> required <code>refs</code> <code>(ndarray, shape(N, input_dim))</code> required <code>probes</code> <code>(ndarray, shape(N, input_dim))</code> required <code>responses</code> <code>(ndarray, shape(N))</code> <p>Typically 0/1; task may support richer encodings.</p> required <p>Returns:</p> Name Type Description <code>loglik</code> <code>ndarray</code> <p>Scalar log-likelihood (task-only; add prior outside if needed)</p> Source code in <code>src/psyphy/model/wppm.py</code> <pre><code>def log_likelihood(self, params: Params, refs: jnp.ndarray, probes: jnp.ndarray, responses: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"\n    Compute the log-likelihood for arrays of trials.\n\n    IMPORTANT:\n        We delegate to the TaskLikelihood to avoid duplicating Bernoulli (MPV)\n        or MC likelihood logic in multiple places. This keeps responsibilities\n        clean and makes adding new tasks straightforward.\n\n    Parameters\n    ----------\n    params : dict\n        Model parameters.\n    refs : jnp.ndarray, shape (N, input_dim)\n    probes : jnp.ndarray, shape (N, input_dim)\n    responses : jnp.ndarray, shape (N,)\n        Typically 0/1; task may support richer encodings.\n\n    Returns\n    -------\n    loglik : jnp.ndarray\n        Scalar log-likelihood (task-only; add prior outside if needed)\n    \"\"\"\n    # We need a ResponseData-like object. To keep this method usable from\n    # array inputs, we construct one on the fly. If you already have a\n    # ResponseData instance, prefer `log_likelihood_from_data`.\n    from psyphy.data.dataset import ResponseData  # local import to avoid cycles\n    data = ResponseData()\n    # ResponseData.add_trial(ref, probe, resp)\n    for r, p, y in zip(refs, probes, responses):\n        data.add_trial(r, p, int(y))\n    return self.task.loglik(params, data, self, self.noise)\n</code></pre>"},{"location":"reference/overview/#psyphy.WPPM.log_likelihood_from_data","title":"log_likelihood_from_data","text":"<pre><code>log_likelihood_from_data(\n    params: Params, data: Any\n) -&gt; ndarray\n</code></pre> <p>Compute log-likelihood directly from a ResponseData object.</p> <p>Why delegate to the task?     - The task knows the decision rule (oddity, 2AFC, ...).     - The task can use the model (this WPPM) to fetch discriminabilities     - and the task can use the noise model if it needs MC simulation</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Model parameters.</p> required <code>data</code> <code>ResponseData</code> <p>Collected trial data.</p> required <p>Returns:</p> Name Type Description <code>loglik</code> <code>ndarray</code> <p>scalar log-likelihood (task-only; add prior outside if needed)</p> Source code in <code>src/psyphy/model/wppm.py</code> <pre><code>def log_likelihood_from_data(self, params: Params, data: Any) -&gt; jnp.ndarray:\n    \"\"\"\n    Compute log-likelihood directly from a ResponseData object.\n\n    Why delegate to the task?\n        - The task knows the decision rule (oddity, 2AFC, ...).\n        - The task can use the model (this WPPM) to fetch discriminabilities\n        - and the task can use the noise model if it needs MC simulation\n\n    Parameters\n    ----------\n    params : dict\n        Model parameters.\n    data : ResponseData\n        Collected trial data.\n\n    Returns\n    -------\n    loglik : jnp.ndarray\n        scalar log-likelihood (task-only; add prior outside if needed)\n    \"\"\"\n    return self.task.loglik(params, data, self, self.noise)\n</code></pre>"},{"location":"reference/overview/#psyphy.WPPM.log_posterior_from_data","title":"log_posterior_from_data","text":"<pre><code>log_posterior_from_data(\n    params: Params, data: Any\n) -&gt; ndarray\n</code></pre> <p>Convenience helper if you want log posterior in one call (MVP).</p> <p>This simply adds the prior log-probability to the task log-likelihood. Inference engines (e.g., MAP optimizer) typically optimize this quantity.</p> <p>Returns:</p> Type Description <code>jnp.ndarray : scalar log posterior = loglik(params | data) + log_prior(params)</code> Source code in <code>src/psyphy/model/wppm.py</code> <pre><code>def log_posterior_from_data(self, params: Params, data: Any) -&gt; jnp.ndarray:\n    \"\"\"\n    Convenience helper if you want log posterior in one call (MVP).\n\n    This simply adds the prior log-probability to the task log-likelihood.\n    Inference engines (e.g., MAP optimizer) typically optimize this quantity.\n\n    Returns\n    -------\n    jnp.ndarray : scalar log posterior = loglik(params | data) + log_prior(params)\n    \"\"\"\n    return self.log_likelihood_from_data(params, data) + self.prior.log_prob(params)\n</code></pre>"},{"location":"reference/overview/#psyphy.WPPM.predict_prob","title":"predict_prob","text":"<pre><code>predict_prob(params: Params, stimulus: Stimulus) -&gt; ndarray\n</code></pre> <p>Predict probability of a correct response for a single stimulus.</p> <p>Design choice:     WPPM computes discriminability &amp; covariance; the TASK defines how     that translates to performance. We therefore delegate to:         task.predict(params, stimulus, model=self, noise=self.noise)</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> required <code>stimulus</code> <code>(reference, probe)</code> required <p>Returns:</p> Name Type Description <code>p_correct</code> <code>ndarray</code> Source code in <code>src/psyphy/model/wppm.py</code> <pre><code>def predict_prob(self, params: Params, stimulus: Stimulus) -&gt; jnp.ndarray:\n    \"\"\"\n    Predict probability of a correct response for a single stimulus.\n\n    Design choice:\n        WPPM computes discriminability &amp; covariance; the TASK defines how\n        that translates to performance. We therefore delegate to:\n            task.predict(params, stimulus, model=self, noise=self.noise)\n\n    Parameters\n    ----------\n    params : dict\n    stimulus : (reference, probe)\n\n    Returns\n    -------\n    p_correct : jnp.ndarray\n    \"\"\"\n    return self.task.predict(params, stimulus, self, self.noise)\n</code></pre>"},{"location":"reference/posterior/","title":"Posterior","text":""},{"location":"reference/posterior/#package","title":"Package","text":""},{"location":"reference/posterior/#psyphy.posterior","title":"posterior","text":""},{"location":"reference/posterior/#psyphy.posterior--posterior","title":"posterior","text":"<p>Posterior representations and diagnostics.</p> <p>This subpackage provides: - Posterior : wrapper around model parameters, samples, and prediction APIs. - diagnostics : tools for checking posterior quality (ESS, R-hat, etc.).</p> MVP implementation <ul> <li>Posterior: stores MAP parameters and delegates predictions to WPPM.</li> <li>Diagnostics: stubs and simple summaries.</li> </ul> Future extensions <ul> <li>Posterior: store MCMC samples, support predictive intervals.</li> <li>Diagnostics: effective sample size, R-hat, posterior predictive checks.</li> </ul> <p>Classes:</p> Name Description <code>Posterior</code> <p>MVP Posterior (MAP only).</p> <p>Functions:</p> Name Description <code>effective_sample_size</code> <p>Estimate effective sample size (ESS) to calculate the number of independent </p> <code>rhat</code> <p>Compute R-hat convergence diagnostic.</p>"},{"location":"reference/posterior/#psyphy.posterior.Posterior","title":"Posterior","text":"<pre><code>Posterior(params, model)\n</code></pre> <p>               Bases: <code>BasePosterior</code></p> <p>MVP Posterior (MAP only).</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>MAP parameter dictionary.</p> required <code>model</code> <code>WPPM</code> <p>Model instance used for predictions.</p> required Notes <ul> <li>This is effectively a MAPPosterior.</li> <li>Future subclasses (LaplacePosterior, MCMCPosterior) will extend   BasePosterior with real sampling logic.</li> </ul> <p>Methods:</p> Name Description <code>MAP_params</code> <p>Return the MAP parameters.</p> <code>predict_prob</code> <p>Predict probability of correct response for a stimulus.</p> <code>predict_thresholds</code> <p>Predict discrimination threshold contour around a reference stimulus.</p> <code>sample</code> <p>Draw parameter samples from the posterior.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>params</code> Source code in <code>src/psyphy/posterior/posterior.py</code> <pre><code>def __init__(self, params, model):\n    self.params = params\n    self.model = model\n</code></pre>"},{"location":"reference/posterior/#psyphy.posterior.Posterior.model","title":"model","text":"<pre><code>model = model\n</code></pre>"},{"location":"reference/posterior/#psyphy.posterior.Posterior.params","title":"params","text":"<pre><code>params = params\n</code></pre>"},{"location":"reference/posterior/#psyphy.posterior.Posterior.MAP_params","title":"MAP_params","text":"<pre><code>MAP_params()\n</code></pre> <p>Return the MAP parameters.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Parameter dictionary.</p> Source code in <code>src/psyphy/posterior/posterior.py</code> <pre><code>def MAP_params(self):\n    \"\"\"\n    Return the MAP parameters.\n\n    Returns\n    -------\n    dict\n        Parameter dictionary.\n    \"\"\"\n    return self.params\n</code></pre>"},{"location":"reference/posterior/#psyphy.posterior.Posterior.predict_prob","title":"predict_prob","text":"<pre><code>predict_prob(stimulus)\n</code></pre> <p>Predict probability of correct response for a stimulus.</p> <p>Parameters:</p> Name Type Description Default <code>stimulus</code> <code>tuple</code> <p>(reference, probe).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Probability of correct response.</p> Notes <p>Delegates to WPPM.predict_prob(). This is not recursion: Posterior calls WPPM\u2019s method with stored params.</p> Source code in <code>src/psyphy/posterior/posterior.py</code> <pre><code>def predict_prob(self, stimulus):\n    \"\"\"\n    Predict probability of correct response for a stimulus.\n\n    Parameters\n    ----------\n    stimulus : tuple\n        (reference, probe).\n\n    Returns\n    -------\n    jnp.ndarray\n        Probability of correct response.\n\n    Notes\n    -----\n    Delegates to WPPM.predict_prob().\n    This is not recursion: Posterior calls WPPM\u2019s method with stored params.\n    \"\"\"\n    return self.model.predict_prob(self.params, stimulus)\n</code></pre>"},{"location":"reference/posterior/#psyphy.posterior.Posterior.predict_thresholds","title":"predict_thresholds","text":"<pre><code>predict_thresholds(\n    reference,\n    criterion: float = 0.667,\n    directions: int = 16,\n)\n</code></pre> <p>Predict discrimination threshold contour around a reference stimulus.</p> <p>Parameters:</p> Name Type Description Default <code>reference</code> <code>ndarray</code> <p>Reference point in model space.</p> required <code>criterion</code> <code>float</code> <p>Target performance (e.g., 2/3 for oddity).</p> <code>0.667</code> <code>directions</code> <code>int</code> <p>Number of directions to probe.</p> <code>16</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Contour points (MVP: unit circle).</p> MVP <p>Returns a placeholder unit circle.</p> Future <ul> <li>Search outward in each direction until performance crosses criterion.</li> <li>Average over posterior samples (Laplace, MCMC) to get credible intervals.</li> </ul> Source code in <code>src/psyphy/posterior/posterior.py</code> <pre><code>def predict_thresholds(self, reference, criterion: float = 0.667, directions: int = 16):\n    \"\"\"\n    Predict discrimination threshold contour around a reference stimulus.\n\n    Parameters\n    ----------\n    reference : jnp.ndarray\n        Reference point in model space.\n    criterion : float, default=0.667\n        Target performance (e.g., 2/3 for oddity).\n    directions : int, default=16\n        Number of directions to probe.\n\n    Returns\n    -------\n    jnp.ndarray\n        Contour points (MVP: unit circle).\n\n    MVP\n    ---\n    Returns a placeholder unit circle.\n\n    Future\n    ------\n    - Search outward in each direction until performance crosses criterion.\n    - Average over posterior samples (Laplace, MCMC) to get credible intervals.\n    \"\"\"\n    angles = jnp.linspace(0, 2 * jnp.pi, directions, endpoint=False)\n    contour = jnp.stack([reference + jnp.array([jnp.cos(a), jnp.sin(a)]) for a in angles])\n    return contour\n</code></pre>"},{"location":"reference/posterior/#psyphy.posterior.Posterior.sample","title":"sample","text":"<pre><code>sample(num_samples: int = 1)\n</code></pre> <p>Draw parameter samples from the posterior.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>Number of samples.</p> <code>1</code> <p>Returns:</p> Type Description <code>list of dict</code> <p>Parameter sets.</p> MVP <p>Returns MAP params repeated n times.</p> Future <ul> <li>LaplacePosterior: draw from N(mean, cov).</li> <li>MCMCPosterior: return stored samples.</li> </ul> Source code in <code>src/psyphy/posterior/posterior.py</code> <pre><code>def sample(self, num_samples: int = 1):\n    \"\"\"\n    Draw parameter samples from the posterior.\n\n    Parameters\n    ----------\n    num_samples : int, default=1\n        Number of samples.\n\n    Returns\n    -------\n    list of dict\n        Parameter sets.\n\n    MVP\n    ---\n    Returns MAP params repeated n times.\n\n    Future\n    ------\n    - LaplacePosterior: draw from N(mean, cov).\n    - MCMCPosterior: return stored samples.\n    \"\"\"\n    return [self.params] * num_samples\n</code></pre>"},{"location":"reference/posterior/#psyphy.posterior.effective_sample_size","title":"effective_sample_size","text":"<pre><code>effective_sample_size(samples: ndarray) -&gt; float\n</code></pre> <p>Estimate effective sample size (ESS) to calculate the number of independent  samples that a correlated MCMC chain is equivalent to.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>ndarray</code> <p>Posterior samples, shape (n_samples, ...).</p> required <p>Returns:</p> Type Description <code>float</code> <p>Effective sample size (stub).</p> Notes <p>MVP:     Returns the number of samples. Full WPPM mode:     Compute ESS using autocorrelation structure.</p> Source code in <code>src/psyphy/posterior/diagnostics.py</code> <pre><code>def effective_sample_size(samples: jnp.ndarray) -&gt; float:\n    \"\"\"\n    Estimate effective sample size (ESS) to calculate the number of independent \n    samples that a correlated MCMC chain is equivalent to.\n\n    Parameters\n    ----------\n    samples : jnp.ndarray\n        Posterior samples, shape (n_samples, ...).\n\n    Returns\n    -------\n    float\n        Effective sample size (stub).\n\n    Notes\n    -----\n    MVP:\n        Returns the number of samples.\n    Full WPPM mode:\n        Compute ESS using autocorrelation structure.\n    \"\"\"\n    return samples.shape[0]\n</code></pre>"},{"location":"reference/posterior/#psyphy.posterior.rhat","title":"rhat","text":"<pre><code>rhat(chains: ndarray) -&gt; float\n</code></pre> <p>Compute R-hat convergence diagnostic.</p> <p>Parameters:</p> Name Type Description Default <code>chains</code> <code>ndarray</code> <p>Posterior samples across chains, shape (n_chains, n_samples, ...).</p> required <p>Returns:</p> Type Description <code>float</code> <p>R-hat statistic (stub).</p> Notes <p>MVP:     Always returns 1.0. Full WPPM mode:     Implement Gelman-Rubin diagnostic [1]</p> References: <pre><code>[1] https://bookdown.org/rdpeng/advstatcomp/monitoring-convergence.html\n</code></pre> Source code in <code>src/psyphy/posterior/diagnostics.py</code> <pre><code>def rhat(chains: jnp.ndarray) -&gt; float:\n    \"\"\"\n    Compute R-hat convergence diagnostic.\n\n    Parameters\n    ----------\n    chains : jnp.ndarray\n        Posterior samples across chains, shape (n_chains, n_samples, ...).\n\n    Returns\n    -------\n    float\n        R-hat statistic (stub).\n\n    Notes\n    -----\n    MVP:\n        Always returns 1.0.\n    Full WPPM mode:\n        Implement Gelman-Rubin diagnostic [1]\n\n    References:\n    ----------\n        [1] https://bookdown.org/rdpeng/advstatcomp/monitoring-convergence.html\n\n    \"\"\"\n    return 1.0\n</code></pre>"},{"location":"reference/posterior/#posterior-representation","title":"Posterior Representation","text":""},{"location":"reference/posterior/#psyphy.posterior.posterior","title":"posterior","text":"posterior.py <p>Posterior representation for WPPM.</p> <p>MVP implementation: - Posterior = MAPPosterior (point estimate only). - Stores one parameter set and delegates predictions to WPPM.</p> Design note <ul> <li>This class inherits from BasePosterior.</li> <li>In the future, other subclasses (LaplacePosterior, MCMCPosterior) will   also inherit from BasePosterior, each implementing the same interface.</li> </ul> <p>Classes:</p> Name Description <code>Posterior</code> <p>MVP Posterior (MAP only).</p>"},{"location":"reference/posterior/#psyphy.posterior.posterior.Posterior","title":"Posterior","text":"<pre><code>Posterior(params, model)\n</code></pre> <p>               Bases: <code>BasePosterior</code></p> <p>MVP Posterior (MAP only).</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>MAP parameter dictionary.</p> required <code>model</code> <code>WPPM</code> <p>Model instance used for predictions.</p> required Notes <ul> <li>This is effectively a MAPPosterior.</li> <li>Future subclasses (LaplacePosterior, MCMCPosterior) will extend   BasePosterior with real sampling logic.</li> </ul> <p>Methods:</p> Name Description <code>MAP_params</code> <p>Return the MAP parameters.</p> <code>predict_prob</code> <p>Predict probability of correct response for a stimulus.</p> <code>predict_thresholds</code> <p>Predict discrimination threshold contour around a reference stimulus.</p> <code>sample</code> <p>Draw parameter samples from the posterior.</p> <p>Attributes:</p> Name Type Description <code>model</code> <code>params</code> Source code in <code>src/psyphy/posterior/posterior.py</code> <pre><code>def __init__(self, params, model):\n    self.params = params\n    self.model = model\n</code></pre>"},{"location":"reference/posterior/#psyphy.posterior.posterior.Posterior.model","title":"model","text":"<pre><code>model = model\n</code></pre>"},{"location":"reference/posterior/#psyphy.posterior.posterior.Posterior.params","title":"params","text":"<pre><code>params = params\n</code></pre>"},{"location":"reference/posterior/#psyphy.posterior.posterior.Posterior.MAP_params","title":"MAP_params","text":"<pre><code>MAP_params()\n</code></pre> <p>Return the MAP parameters.</p> <p>Returns:</p> Type Description <code>dict</code> <p>Parameter dictionary.</p> Source code in <code>src/psyphy/posterior/posterior.py</code> <pre><code>def MAP_params(self):\n    \"\"\"\n    Return the MAP parameters.\n\n    Returns\n    -------\n    dict\n        Parameter dictionary.\n    \"\"\"\n    return self.params\n</code></pre>"},{"location":"reference/posterior/#psyphy.posterior.posterior.Posterior.predict_prob","title":"predict_prob","text":"<pre><code>predict_prob(stimulus)\n</code></pre> <p>Predict probability of correct response for a stimulus.</p> <p>Parameters:</p> Name Type Description Default <code>stimulus</code> <code>tuple</code> <p>(reference, probe).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Probability of correct response.</p> Notes <p>Delegates to WPPM.predict_prob(). This is not recursion: Posterior calls WPPM\u2019s method with stored params.</p> Source code in <code>src/psyphy/posterior/posterior.py</code> <pre><code>def predict_prob(self, stimulus):\n    \"\"\"\n    Predict probability of correct response for a stimulus.\n\n    Parameters\n    ----------\n    stimulus : tuple\n        (reference, probe).\n\n    Returns\n    -------\n    jnp.ndarray\n        Probability of correct response.\n\n    Notes\n    -----\n    Delegates to WPPM.predict_prob().\n    This is not recursion: Posterior calls WPPM\u2019s method with stored params.\n    \"\"\"\n    return self.model.predict_prob(self.params, stimulus)\n</code></pre>"},{"location":"reference/posterior/#psyphy.posterior.posterior.Posterior.predict_thresholds","title":"predict_thresholds","text":"<pre><code>predict_thresholds(\n    reference,\n    criterion: float = 0.667,\n    directions: int = 16,\n)\n</code></pre> <p>Predict discrimination threshold contour around a reference stimulus.</p> <p>Parameters:</p> Name Type Description Default <code>reference</code> <code>ndarray</code> <p>Reference point in model space.</p> required <code>criterion</code> <code>float</code> <p>Target performance (e.g., 2/3 for oddity).</p> <code>0.667</code> <code>directions</code> <code>int</code> <p>Number of directions to probe.</p> <code>16</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Contour points (MVP: unit circle).</p> MVP <p>Returns a placeholder unit circle.</p> Future <ul> <li>Search outward in each direction until performance crosses criterion.</li> <li>Average over posterior samples (Laplace, MCMC) to get credible intervals.</li> </ul> Source code in <code>src/psyphy/posterior/posterior.py</code> <pre><code>def predict_thresholds(self, reference, criterion: float = 0.667, directions: int = 16):\n    \"\"\"\n    Predict discrimination threshold contour around a reference stimulus.\n\n    Parameters\n    ----------\n    reference : jnp.ndarray\n        Reference point in model space.\n    criterion : float, default=0.667\n        Target performance (e.g., 2/3 for oddity).\n    directions : int, default=16\n        Number of directions to probe.\n\n    Returns\n    -------\n    jnp.ndarray\n        Contour points (MVP: unit circle).\n\n    MVP\n    ---\n    Returns a placeholder unit circle.\n\n    Future\n    ------\n    - Search outward in each direction until performance crosses criterion.\n    - Average over posterior samples (Laplace, MCMC) to get credible intervals.\n    \"\"\"\n    angles = jnp.linspace(0, 2 * jnp.pi, directions, endpoint=False)\n    contour = jnp.stack([reference + jnp.array([jnp.cos(a), jnp.sin(a)]) for a in angles])\n    return contour\n</code></pre>"},{"location":"reference/posterior/#psyphy.posterior.posterior.Posterior.sample","title":"sample","text":"<pre><code>sample(num_samples: int = 1)\n</code></pre> <p>Draw parameter samples from the posterior.</p> <p>Parameters:</p> Name Type Description Default <code>num_samples</code> <code>int</code> <p>Number of samples.</p> <code>1</code> <p>Returns:</p> Type Description <code>list of dict</code> <p>Parameter sets.</p> MVP <p>Returns MAP params repeated n times.</p> Future <ul> <li>LaplacePosterior: draw from N(mean, cov).</li> <li>MCMCPosterior: return stored samples.</li> </ul> Source code in <code>src/psyphy/posterior/posterior.py</code> <pre><code>def sample(self, num_samples: int = 1):\n    \"\"\"\n    Draw parameter samples from the posterior.\n\n    Parameters\n    ----------\n    num_samples : int, default=1\n        Number of samples.\n\n    Returns\n    -------\n    list of dict\n        Parameter sets.\n\n    MVP\n    ---\n    Returns MAP params repeated n times.\n\n    Future\n    ------\n    - LaplacePosterior: draw from N(mean, cov).\n    - MCMCPosterior: return stored samples.\n    \"\"\"\n    return [self.params] * num_samples\n</code></pre>"},{"location":"reference/posterior/#diagnostics","title":"Diagnostics","text":""},{"location":"reference/posterior/#psyphy.posterior.diagnostics","title":"diagnostics","text":"diagnostics.py <p>Posterior diagnostics.</p> <p>Provides functions to check quality of posterior inference.</p> <p>MVP implementation: - Stubs for effective sample size and R-hat.</p> <p>Full WPPM mode: - Implement real diagnostics from posterior chains. - Include posterior predictive checks.</p> <p>Functions:</p> Name Description <code>effective_sample_size</code> <p>Estimate effective sample size (ESS) to calculate the number of independent </p> <code>rhat</code> <p>Compute R-hat convergence diagnostic.</p>"},{"location":"reference/posterior/#psyphy.posterior.diagnostics.effective_sample_size","title":"effective_sample_size","text":"<pre><code>effective_sample_size(samples: ndarray) -&gt; float\n</code></pre> <p>Estimate effective sample size (ESS) to calculate the number of independent  samples that a correlated MCMC chain is equivalent to.</p> <p>Parameters:</p> Name Type Description Default <code>samples</code> <code>ndarray</code> <p>Posterior samples, shape (n_samples, ...).</p> required <p>Returns:</p> Type Description <code>float</code> <p>Effective sample size (stub).</p> Notes <p>MVP:     Returns the number of samples. Full WPPM mode:     Compute ESS using autocorrelation structure.</p> Source code in <code>src/psyphy/posterior/diagnostics.py</code> <pre><code>def effective_sample_size(samples: jnp.ndarray) -&gt; float:\n    \"\"\"\n    Estimate effective sample size (ESS) to calculate the number of independent \n    samples that a correlated MCMC chain is equivalent to.\n\n    Parameters\n    ----------\n    samples : jnp.ndarray\n        Posterior samples, shape (n_samples, ...).\n\n    Returns\n    -------\n    float\n        Effective sample size (stub).\n\n    Notes\n    -----\n    MVP:\n        Returns the number of samples.\n    Full WPPM mode:\n        Compute ESS using autocorrelation structure.\n    \"\"\"\n    return samples.shape[0]\n</code></pre>"},{"location":"reference/posterior/#psyphy.posterior.diagnostics.rhat","title":"rhat","text":"<pre><code>rhat(chains: ndarray) -&gt; float\n</code></pre> <p>Compute R-hat convergence diagnostic.</p> <p>Parameters:</p> Name Type Description Default <code>chains</code> <code>ndarray</code> <p>Posterior samples across chains, shape (n_chains, n_samples, ...).</p> required <p>Returns:</p> Type Description <code>float</code> <p>R-hat statistic (stub).</p> Notes <p>MVP:     Always returns 1.0. Full WPPM mode:     Implement Gelman-Rubin diagnostic [1]</p> References: <pre><code>[1] https://bookdown.org/rdpeng/advstatcomp/monitoring-convergence.html\n</code></pre> Source code in <code>src/psyphy/posterior/diagnostics.py</code> <pre><code>def rhat(chains: jnp.ndarray) -&gt; float:\n    \"\"\"\n    Compute R-hat convergence diagnostic.\n\n    Parameters\n    ----------\n    chains : jnp.ndarray\n        Posterior samples across chains, shape (n_chains, n_samples, ...).\n\n    Returns\n    -------\n    float\n        R-hat statistic (stub).\n\n    Notes\n    -----\n    MVP:\n        Always returns 1.0.\n    Full WPPM mode:\n        Implement Gelman-Rubin diagnostic [1]\n\n    References:\n    ----------\n        [1] https://bookdown.org/rdpeng/advstatcomp/monitoring-convergence.html\n\n    \"\"\"\n    return 1.0\n</code></pre>"},{"location":"reference/session/","title":"Session","text":""},{"location":"reference/session/#package","title":"Package","text":""},{"location":"reference/session/#psyphy.session","title":"session","text":""},{"location":"reference/session/#psyphy.session--session","title":"session","text":"<p>Experiment orchestration.</p> <p>This subpackage provides: - ExperimentSession : a high-level controller that coordinates data collection,   model fitting, posterior updates, and adaptive trial placement.</p> MVP implementation <ul> <li>Wraps model, inference engine, and placement strategy.</li> <li>Stores data in a ResponseData object.</li> <li>Provides initialize(), update(), and next_batch() methods.</li> </ul> Full WPPM mode <ul> <li>Will support richer workflows:</li> <li>Batch vs online updates.</li> <li>Integration with live experimental computers (e.g., resuming sessions after breaks).</li> </ul> <p>Classes:</p> Name Description <code>ExperimentSession</code> <p>High-level experiment orchestrator.</p>"},{"location":"reference/session/#psyphy.session.ExperimentSession","title":"ExperimentSession","text":"<pre><code>ExperimentSession(\n    model, inference, placement, init_placement=None\n)\n</code></pre> <p>High-level experiment orchestrator.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>WPPM</code> <p>(Psychophysical) model instance.</p> required <code>inference</code> <code>InferenceEngine</code> <p>Inference engine (MAP, Langevin, etc.).</p> required <code>placement</code> <code>TrialPlacement</code> <p>Adaptive trial placement strategy.</p> required <code>init_placement</code> <code>TrialPlacement</code> <p>Initial placement strategy (e.g., Sobol exploration).</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>data</code> <code>ResponseData</code> <p>Stores all collected trials.</p> <code>posterior</code> <code>Posterior or None</code> <p>Current posterior estimate (None before initialization).</p> <p>Methods:</p> Name Description <code>initialize</code> <p>Fit an initial posterior before any adaptive placement.</p> <code>next_batch</code> <p>Propose the next batch of trials.</p> <code>update</code> <p>Refit posterior with accumulated data.</p> Source code in <code>src/psyphy/session/experiment_session.py</code> <pre><code>def __init__(self, model, inference, placement, init_placement=None):\n    self.model = model\n    self.inference = inference\n    self.placement = placement\n    self.init_placement = init_placement\n\n    # Data store starts empty\n    self.data = ResponseData()\n\n    # Posterior will be set after initialize() or update()\n    self.posterior = None\n</code></pre>"},{"location":"reference/session/#psyphy.session.ExperimentSession.data","title":"data","text":"<pre><code>data = ResponseData()\n</code></pre>"},{"location":"reference/session/#psyphy.session.ExperimentSession.inference","title":"inference","text":"<pre><code>inference = inference\n</code></pre>"},{"location":"reference/session/#psyphy.session.ExperimentSession.init_placement","title":"init_placement","text":"<pre><code>init_placement = init_placement\n</code></pre>"},{"location":"reference/session/#psyphy.session.ExperimentSession.model","title":"model","text":"<pre><code>model = model\n</code></pre>"},{"location":"reference/session/#psyphy.session.ExperimentSession.placement","title":"placement","text":"<pre><code>placement = placement\n</code></pre>"},{"location":"reference/session/#psyphy.session.ExperimentSession.posterior","title":"posterior","text":"<pre><code>posterior = None\n</code></pre>"},{"location":"reference/session/#psyphy.session.ExperimentSession.initialize","title":"initialize","text":"<pre><code>initialize()\n</code></pre> <p>Fit an initial posterior before any adaptive placement.</p> <p>Returns:</p> Type Description <code>Posterior</code> <p>Posterior object wrapping fitted parameters.</p> Notes <p>MVP:     Posterior is fitted to empty data (prior only). Full WPPM mode:     Could use pilot data or pre-collected trials along grid etc.</p> Source code in <code>src/psyphy/session/experiment_session.py</code> <pre><code>def initialize(self):\n    \"\"\"\n    Fit an initial posterior before any adaptive placement.\n\n    Returns\n    -------\n    Posterior\n        Posterior object wrapping fitted parameters.\n\n    Notes\n    -----\n    MVP:\n        Posterior is fitted to empty data (prior only).\n    Full WPPM mode:\n        Could use pilot data or pre-collected trials along grid etc.\n    \"\"\"\n    self.posterior = self.inference.fit(self.model, self.data)\n    return self.posterior\n</code></pre>"},{"location":"reference/session/#psyphy.session.ExperimentSession.next_batch","title":"next_batch","text":"<pre><code>next_batch(batch_size: int)\n</code></pre> <p>Propose the next batch of trials.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of trials to propose.</p> required <p>Returns:</p> Type Description <code>TrialBatch</code> <p>Batch of proposed (reference, probe) stimuli.</p> Notes <p>MVP:     Always calls placement.propose() on current posterior. Full WPPM mode:     Could support hybrid placement (init strategy -&gt; adaptive strategy).</p> Source code in <code>src/psyphy/session/experiment_session.py</code> <pre><code>def next_batch(self, batch_size: int):\n    \"\"\"\n    Propose the next batch of trials.\n\n    Parameters\n    ----------\n    batch_size : int\n        Number of trials to propose.\n\n    Returns\n    -------\n    TrialBatch\n        Batch of proposed (reference, probe) stimuli.\n\n    Notes\n    -----\n    MVP:\n        Always calls placement.propose() on current posterior.\n    Full WPPM mode:\n        Could support hybrid placement (init strategy -&gt; adaptive strategy).\n    \"\"\"\n    if self.posterior is None:\n        raise RuntimeError(\"Posterior not initialized. Call initialize() first.\")\n    return self.placement.propose(self.posterior, batch_size)\n</code></pre>"},{"location":"reference/session/#psyphy.session.ExperimentSession.update","title":"update","text":"<pre><code>update()\n</code></pre> <p>Refit posterior with accumulated data.</p> <p>Returns:</p> Type Description <code>Posterior</code> <p>Updated posterior.</p> Notes <p>MVP:     Re-optimizes from scratch using all data. Full WPPM mode:     Could support warm-start or online parameter updates.</p> Source code in <code>src/psyphy/session/experiment_session.py</code> <pre><code>def update(self):\n    \"\"\"\n    Refit posterior with accumulated data.\n\n    Returns\n    -------\n    Posterior\n        Updated posterior.\n\n    Notes\n    -----\n    MVP:\n        Re-optimizes from scratch using all data.\n    Full WPPM mode:\n        Could support warm-start or online parameter updates.\n    \"\"\"\n    self.posterior = self.inference.fit(self.model, self.data)\n    return self.posterior\n</code></pre>"},{"location":"reference/session/#experiment-session","title":"Experiment Session","text":""},{"location":"reference/session/#psyphy.session.experiment_session","title":"experiment_session","text":"experiment_session.py <p>ExperimentSession orchestrates the adaptive experiment loop.</p> Responsibilities <ol> <li>Store trial data (ResponseData).</li> <li>Manage inference engine (MAP, Langevin, Laplace).</li> <li>Keep track of the current posterior.</li> <li>Delegate adaptive placement to a TrialPlacement strategy.</li> </ol> <p>MVP implementation: - Data container starts empty and can be appended to. - initialize() fits a posterior once before trials. - update() refits posterior with accumulated data. - next_batch() proposes new trials from the placement strategy.</p> <p>Full WPPM mode: - Will support batch vs online updates. - Integrate with lab software for live trial execution. - Save/load checkpoints for long experiments.</p> <p>Classes:</p> Name Description <code>ExperimentSession</code> <p>High-level experiment orchestrator.</p>"},{"location":"reference/session/#psyphy.session.experiment_session.ExperimentSession","title":"ExperimentSession","text":"<pre><code>ExperimentSession(\n    model, inference, placement, init_placement=None\n)\n</code></pre> <p>High-level experiment orchestrator.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>WPPM</code> <p>(Psychophysical) model instance.</p> required <code>inference</code> <code>InferenceEngine</code> <p>Inference engine (MAP, Langevin, etc.).</p> required <code>placement</code> <code>TrialPlacement</code> <p>Adaptive trial placement strategy.</p> required <code>init_placement</code> <code>TrialPlacement</code> <p>Initial placement strategy (e.g., Sobol exploration).</p> <code>None</code> <p>Attributes:</p> Name Type Description <code>data</code> <code>ResponseData</code> <p>Stores all collected trials.</p> <code>posterior</code> <code>Posterior or None</code> <p>Current posterior estimate (None before initialization).</p> <p>Methods:</p> Name Description <code>initialize</code> <p>Fit an initial posterior before any adaptive placement.</p> <code>next_batch</code> <p>Propose the next batch of trials.</p> <code>update</code> <p>Refit posterior with accumulated data.</p> Source code in <code>src/psyphy/session/experiment_session.py</code> <pre><code>def __init__(self, model, inference, placement, init_placement=None):\n    self.model = model\n    self.inference = inference\n    self.placement = placement\n    self.init_placement = init_placement\n\n    # Data store starts empty\n    self.data = ResponseData()\n\n    # Posterior will be set after initialize() or update()\n    self.posterior = None\n</code></pre>"},{"location":"reference/session/#psyphy.session.experiment_session.ExperimentSession.data","title":"data","text":"<pre><code>data = ResponseData()\n</code></pre>"},{"location":"reference/session/#psyphy.session.experiment_session.ExperimentSession.inference","title":"inference","text":"<pre><code>inference = inference\n</code></pre>"},{"location":"reference/session/#psyphy.session.experiment_session.ExperimentSession.init_placement","title":"init_placement","text":"<pre><code>init_placement = init_placement\n</code></pre>"},{"location":"reference/session/#psyphy.session.experiment_session.ExperimentSession.model","title":"model","text":"<pre><code>model = model\n</code></pre>"},{"location":"reference/session/#psyphy.session.experiment_session.ExperimentSession.placement","title":"placement","text":"<pre><code>placement = placement\n</code></pre>"},{"location":"reference/session/#psyphy.session.experiment_session.ExperimentSession.posterior","title":"posterior","text":"<pre><code>posterior = None\n</code></pre>"},{"location":"reference/session/#psyphy.session.experiment_session.ExperimentSession.initialize","title":"initialize","text":"<pre><code>initialize()\n</code></pre> <p>Fit an initial posterior before any adaptive placement.</p> <p>Returns:</p> Type Description <code>Posterior</code> <p>Posterior object wrapping fitted parameters.</p> Notes <p>MVP:     Posterior is fitted to empty data (prior only). Full WPPM mode:     Could use pilot data or pre-collected trials along grid etc.</p> Source code in <code>src/psyphy/session/experiment_session.py</code> <pre><code>def initialize(self):\n    \"\"\"\n    Fit an initial posterior before any adaptive placement.\n\n    Returns\n    -------\n    Posterior\n        Posterior object wrapping fitted parameters.\n\n    Notes\n    -----\n    MVP:\n        Posterior is fitted to empty data (prior only).\n    Full WPPM mode:\n        Could use pilot data or pre-collected trials along grid etc.\n    \"\"\"\n    self.posterior = self.inference.fit(self.model, self.data)\n    return self.posterior\n</code></pre>"},{"location":"reference/session/#psyphy.session.experiment_session.ExperimentSession.next_batch","title":"next_batch","text":"<pre><code>next_batch(batch_size: int)\n</code></pre> <p>Propose the next batch of trials.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of trials to propose.</p> required <p>Returns:</p> Type Description <code>TrialBatch</code> <p>Batch of proposed (reference, probe) stimuli.</p> Notes <p>MVP:     Always calls placement.propose() on current posterior. Full WPPM mode:     Could support hybrid placement (init strategy -&gt; adaptive strategy).</p> Source code in <code>src/psyphy/session/experiment_session.py</code> <pre><code>def next_batch(self, batch_size: int):\n    \"\"\"\n    Propose the next batch of trials.\n\n    Parameters\n    ----------\n    batch_size : int\n        Number of trials to propose.\n\n    Returns\n    -------\n    TrialBatch\n        Batch of proposed (reference, probe) stimuli.\n\n    Notes\n    -----\n    MVP:\n        Always calls placement.propose() on current posterior.\n    Full WPPM mode:\n        Could support hybrid placement (init strategy -&gt; adaptive strategy).\n    \"\"\"\n    if self.posterior is None:\n        raise RuntimeError(\"Posterior not initialized. Call initialize() first.\")\n    return self.placement.propose(self.posterior, batch_size)\n</code></pre>"},{"location":"reference/session/#psyphy.session.experiment_session.ExperimentSession.update","title":"update","text":"<pre><code>update()\n</code></pre> <p>Refit posterior with accumulated data.</p> <p>Returns:</p> Type Description <code>Posterior</code> <p>Updated posterior.</p> Notes <p>MVP:     Re-optimizes from scratch using all data. Full WPPM mode:     Could support warm-start or online parameter updates.</p> Source code in <code>src/psyphy/session/experiment_session.py</code> <pre><code>def update(self):\n    \"\"\"\n    Refit posterior with accumulated data.\n\n    Returns\n    -------\n    Posterior\n        Updated posterior.\n\n    Notes\n    -----\n    MVP:\n        Re-optimizes from scratch using all data.\n    Full WPPM mode:\n        Could support warm-start or online parameter updates.\n    \"\"\"\n    self.posterior = self.inference.fit(self.model, self.data)\n    return self.posterior\n</code></pre>"},{"location":"reference/trial_placement/","title":"Trial Placement","text":""},{"location":"reference/trial_placement/#package","title":"Package","text":""},{"location":"reference/trial_placement/#psyphy.trial_placement","title":"trial_placement","text":""},{"location":"reference/trial_placement/#psyphy.trial_placement--trial_placement","title":"trial_placement","text":"<p>Strategies for selecting the next set of trials in an experiment.</p> <p>This subpackage provides: - TrialPlacement : abstract base class. - GridPlacement : fixed non-adaptive design. - SobolPlacement : quasi-random low-discrepancy exploration. - StaircasePlacement : classical adaptive rule (1-up-2-down). - GreedyMAPPlacement : adaptive design based on MAP point estimate. - InfoGainPlacement : adaptive design based on expected information gain.</p> MVP implementation <ul> <li>Simple grid, Sobol, and staircase procedures.</li> <li>Greedy placement uses MAP only.</li> <li>InfoGain uses entropy-style heuristic with placeholder logic.</li> </ul> Full WPPM mode <ul> <li>InfoGainPlacement will integrate with posterior.sample() from   LaplacePosterior or MCMCPosterior.</li> <li>StaircasePlacement can be extended to multi-dimensional, task-aware rules.</li> <li>Hybrid strategies: exploration (Sobol) -&gt; exploitation (InfoGain).</li> </ul> <p>Classes:</p> Name Description <code>GreedyMAPPlacement</code> <p>Greedy adaptive placement (MAP-based).</p> <code>GridPlacement</code> <p>Fixed grid placement.</p> <code>InfoGainPlacement</code> <p>Information-gain adaptive placement.</p> <code>SobolPlacement</code> <p>Sobol quasi-random placement.</p> <code>StaircasePlacement</code> <p>Staircase procedure.</p> <code>TrialPlacement</code> <p>Abstract interface for trial placement strategies.</p>"},{"location":"reference/trial_placement/#psyphy.trial_placement.GreedyMAPPlacement","title":"GreedyMAPPlacement","text":"<pre><code>GreedyMAPPlacement(candidate_pool)\n</code></pre> <p>               Bases: <code>TrialPlacement</code></p> <p>Greedy adaptive placement (MAP-based).</p> <p>Parameters:</p> Name Type Description Default <code>candidate_pool</code> <code>list of (ref, probe)</code> <p>Candidate stimuli.</p> required <p>Methods:</p> Name Description <code>propose</code> <p>Select trials based on MAP discriminability.</p> <p>Attributes:</p> Name Type Description <code>pool</code> Source code in <code>src/psyphy/trial_placement/greedy_map.py</code> <pre><code>def __init__(self, candidate_pool):\n    self.pool = candidate_pool\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.GreedyMAPPlacement.pool","title":"pool","text":"<pre><code>pool = candidate_pool\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.GreedyMAPPlacement.propose","title":"propose","text":"<pre><code>propose(posterior, batch_size: int) -&gt; TrialBatch\n</code></pre> <p>Select trials based on MAP discriminability.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>Posterior</code> <p>Posterior object. Provides MAP params.</p> required <code>batch_size</code> <code>int</code> <p>Number of trials to propose.</p> required <p>Returns:</p> Type Description <code>TrialBatch</code> <p>Selected trials.</p> Notes <p>MVP:     Returns first N candidates. Full WPPM mode:     - Score all candidates with _score_candidate().     - Rank candidates by score.     - Select top-N.</p> Source code in <code>src/psyphy/trial_placement/greedy_map.py</code> <pre><code>def propose(self, posterior, batch_size: int) -&gt; TrialBatch:\n    \"\"\"\n    Select trials based on MAP discriminability.\n\n    Parameters\n    ----------\n    posterior : Posterior\n        Posterior object. Provides MAP params.\n    batch_size : int\n        Number of trials to propose.\n\n    Returns\n    -------\n    TrialBatch\n        Selected trials.\n\n    Notes\n    -----\n    MVP:\n        Returns first N candidates.\n    Full WPPM mode:\n        - Score all candidates with _score_candidate().\n        - Rank candidates by score.\n        - Select top-N.\n    \"\"\"\n    # MVP stub\n    selected = self.pool[:batch_size]\n\n    # TODO: Uncomment when scoring is implemented\n    # scores = [self._score_candidate(posterior, cand) for cand in self.pool]\n    # idx = jnp.argsort(jnp.array(scores))[::-1]  # sort descending\n    # selected = [self.pool[i] for i in idx[:batch_size]]\n\n    return TrialBatch.from_stimuli(selected)\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.GridPlacement","title":"GridPlacement","text":"<pre><code>GridPlacement(grid_points)\n</code></pre> <p>               Bases: <code>TrialPlacement</code></p> <p>Fixed grid placement.</p> <p>Parameters:</p> Name Type Description Default <code>grid_points</code> <code>list of (ref, probe)</code> <p>Predefined set of trial stimuli.</p> required Notes <ul> <li>grid = your set of allowable trials.</li> <li>this class simply walks through that set.</li> </ul> <p>Methods:</p> Name Description <code>propose</code> <p>Return the next batch of trials from the grid.</p> <p>Attributes:</p> Name Type Description <code>grid_points</code> Source code in <code>src/psyphy/trial_placement/grid.py</code> <pre><code>def __init__(self, grid_points):\n    self.grid_points = list(grid_points)\n    self._index = 0\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.GridPlacement.grid_points","title":"grid_points","text":"<pre><code>grid_points = list(grid_points)\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.GridPlacement.propose","title":"propose","text":"<pre><code>propose(posterior, batch_size: int) -&gt; TrialBatch\n</code></pre> <p>Return the next batch of trials from the grid.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>Posterior</code> <p>Ignored in MVP (grid is non-adaptive).</p> required <code>batch_size</code> <code>int</code> <p>Number of trials to return.</p> required <p>Returns:</p> Type Description <code>TrialBatch</code> <p>Fixed batch of (ref, probe).</p> Source code in <code>src/psyphy/trial_placement/grid.py</code> <pre><code>def propose(self, posterior, batch_size: int) -&gt; TrialBatch:\n    \"\"\"\n    Return the next batch of trials from the grid.\n\n    Parameters\n    ----------\n    posterior : Posterior\n        Ignored in MVP (grid is non-adaptive).\n    batch_size : int\n        Number of trials to return.\n\n    Returns\n    -------\n    TrialBatch\n        Fixed batch of (ref, probe).\n    \"\"\"\n    start, end = self._index, self._index + batch_size\n    batch = self.grid_points[start:end]\n    self._index = end\n    return TrialBatch.from_stimuli(batch)\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.InfoGainPlacement","title":"InfoGainPlacement","text":"<pre><code>InfoGainPlacement(candidate_pool)\n</code></pre> <p>               Bases: <code>TrialPlacement</code></p> <p>Information-gain adaptive placement.</p> <p>Parameters:</p> Name Type Description Default <code>candidate_pool</code> <code>list of (ref, probe)</code> <p>Candidate stimuli.</p> required <p>Methods:</p> Name Description <code>propose</code> <p>Propose trials that maximize information gain.</p> <p>Attributes:</p> Name Type Description <code>pool</code> Source code in <code>src/psyphy/trial_placement/info_gain.py</code> <pre><code>def __init__(self, candidate_pool):\n    self.pool = candidate_pool\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.InfoGainPlacement.pool","title":"pool","text":"<pre><code>pool = candidate_pool\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.InfoGainPlacement.propose","title":"propose","text":"<pre><code>propose(posterior, batch_size: int) -&gt; TrialBatch\n</code></pre> <p>Propose trials that maximize information gain.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>Posterior</code> <p>Posterior object. Must support sample().</p> required <code>batch_size</code> <code>int</code> <p>Number of trials to propose.</p> required <p>Returns:</p> Type Description <code>TrialBatch</code> <p>Selected trials.</p> Notes <p>MVP:     Returns first N candidates. Full WPPM mode:     - Score all candidates with _score_candidate().     - Rank candidates by score.     - Select top-N.</p> Source code in <code>src/psyphy/trial_placement/info_gain.py</code> <pre><code>def propose(self, posterior, batch_size: int) -&gt; TrialBatch:\n    \"\"\"\n    Propose trials that maximize information gain.\n\n    Parameters\n    ----------\n    posterior : Posterior\n        Posterior object. Must support sample().\n    batch_size : int\n        Number of trials to propose.\n\n    Returns\n    -------\n    TrialBatch\n        Selected trials.\n\n    Notes\n    -----\n    MVP:\n        Returns first N candidates.\n    Full WPPM mode:\n        - Score all candidates with _score_candidate().\n        - Rank candidates by score.\n        - Select top-N.\n    \"\"\"\n    # MVP stub\n    selected = self.pool[:batch_size]\n\n    # TODO: Uncomment when scoring is implemented\n    # scores = [self._score_candidate(posterior, cand) for cand in self.pool]\n    # idx = jnp.argsort(jnp.array(scores))[::-1]\n    # selected = [self.pool[i] for i in idx[:batch_size]]\n\n    return TrialBatch.from_stimuli(selected)\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.SobolPlacement","title":"SobolPlacement","text":"<pre><code>SobolPlacement(dim: int, bounds, seed: int = 0)\n</code></pre> <p>               Bases: <code>TrialPlacement</code></p> <p>Sobol quasi-random placement.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimensionality of stimulus space.</p> required <code>bounds</code> <code>list of (low, high)</code> <p>Bounds per dimension.</p> required <code>seed</code> <code>int</code> <p>RNG seed.</p> <code>0</code> <p>Methods:</p> Name Description <code>propose</code> <p>Propose Sobol points (ignores posterior).</p> <p>Attributes:</p> Name Type Description <code>bounds</code> <code>engine</code> Source code in <code>src/psyphy/trial_placement/sobol.py</code> <pre><code>def __init__(self, dim: int, bounds, seed: int = 0):\n    self.engine = Sobol(d=dim, scramble=True, seed=seed)\n    self.bounds = bounds\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.SobolPlacement.bounds","title":"bounds","text":"<pre><code>bounds = bounds\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.SobolPlacement.engine","title":"engine","text":"<pre><code>engine = Sobol(d=dim, scramble=True, seed=seed)\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.SobolPlacement.propose","title":"propose","text":"<pre><code>propose(posterior, batch_size: int) -&gt; TrialBatch\n</code></pre> <p>Propose Sobol points (ignores posterior).</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>Posterior</code> <p>Ignored in MVP.</p> required <code>batch_size</code> <code>int</code> <p>Number of trials to return.</p> required <p>Returns:</p> Type Description <code>TrialBatch</code> <p>Candidate trials from Sobol sequence.</p> Notes <p>MVP:     Pure exploration of space. Full WPPM mode:     Use Sobol as initialization, then switch to InfoGain.</p> Source code in <code>src/psyphy/trial_placement/sobol.py</code> <pre><code>def propose(self, posterior, batch_size: int) -&gt; TrialBatch:\n    \"\"\"\n    Propose Sobol points (ignores posterior).\n\n    Parameters\n    ----------\n    posterior : Posterior\n        Ignored in MVP.\n    batch_size : int\n        Number of trials to return.\n\n    Returns\n    -------\n    TrialBatch\n        Candidate trials from Sobol sequence.\n\n    Notes\n    -----\n    MVP:\n        Pure exploration of space.\n    Full WPPM mode:\n        Use Sobol as initialization, then switch to InfoGain.\n    \"\"\"\n    raw = self.engine.random(batch_size)\n    scaled = [\n        low + (high - low) * raw[:, i]\n        for i, (low, high) in enumerate(self.bounds)\n    ]\n    # Convert column-wise scaled arrays into list of probe vectors\n    probes = [tuple(vals) for vals in zip(*scaled)]\n    # MVP: use a zero reference vector of matching dimension\n    dim = len(self.bounds)\n    zero_ref = 0.0 if dim == 1 else tuple(0.0 for _ in range(dim))\n    trials = [(zero_ref, p) for p in probes]\n    return TrialBatch.from_stimuli(trials)\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.StaircasePlacement","title":"StaircasePlacement","text":"<pre><code>StaircasePlacement(\n    start_level: float,\n    step_size: float,\n    rule: str = \"1up-2down\",\n)\n</code></pre> <p>               Bases: <code>TrialPlacement</code></p> <p>Staircase procedure.</p> <p>Parameters:</p> Name Type Description Default <code>start_level</code> <code>float</code> <p>Starting stimulus intensity.</p> required <code>step_size</code> <code>float</code> <p>Step increment.</p> required <code>rule</code> <code>str</code> <p>Adaptive rule.</p> <code>\"1up-2down\"</code> <p>Methods:</p> Name Description <code>propose</code> <p>Return next trial(s) based on staircase rule.</p> <code>update</code> <p>Update staircase level given last response.</p> <p>Attributes:</p> Name Type Description <code>correct_counter</code> <code>current_level</code> <code>rule</code> <code>step_size</code> Source code in <code>src/psyphy/trial_placement/staircase.py</code> <pre><code>def __init__(self, start_level: float, step_size: float, rule: str = \"1up-2down\"):\n    self.current_level = start_level\n    self.step_size = step_size\n    self.rule = rule\n    self.correct_counter = 0\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.StaircasePlacement.correct_counter","title":"correct_counter","text":"<pre><code>correct_counter = 0\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.StaircasePlacement.current_level","title":"current_level","text":"<pre><code>current_level = start_level\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.StaircasePlacement.rule","title":"rule","text":"<pre><code>rule = rule\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.StaircasePlacement.step_size","title":"step_size","text":"<pre><code>step_size = step_size\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.StaircasePlacement.propose","title":"propose","text":"<pre><code>propose(posterior, batch_size: int) -&gt; TrialBatch\n</code></pre> <p>Return next trial(s) based on staircase rule.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>Posterior</code> <p>Ignored in MVP (not posterior-aware).</p> required <code>batch_size</code> <code>int</code> <p>Number of trials to propose.</p> required <p>Returns:</p> Type Description <code>TrialBatch</code> <p>Batch of trials with current staircase level.</p> Source code in <code>src/psyphy/trial_placement/staircase.py</code> <pre><code>def propose(self, posterior, batch_size: int) -&gt; TrialBatch:\n    \"\"\"\n    Return next trial(s) based on staircase rule.\n\n    Parameters\n    ----------\n    posterior : Posterior\n        Ignored in MVP (not posterior-aware).\n    batch_size : int\n        Number of trials to propose.\n\n    Returns\n    -------\n    TrialBatch\n        Batch of trials with current staircase level.\n    \"\"\"\n    trials = [(0.0, self.current_level)] * batch_size  # Stub: (ref=0, probe=level)\n    return TrialBatch.from_stimuli(trials)\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.StaircasePlacement.update","title":"update","text":"<pre><code>update(response: int)\n</code></pre> <p>Update staircase level given last response.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>int</code> <p>1 = correct, 0 = incorrect.</p> required Source code in <code>src/psyphy/trial_placement/staircase.py</code> <pre><code>def update(self, response: int):\n    \"\"\"\n    Update staircase level given last response.\n\n    Parameters\n    ----------\n    response : int\n        1 = correct, 0 = incorrect.\n    \"\"\"\n    if response == 1:\n        self.correct_counter += 1\n        if self.rule == \"1up-2down\" and self.correct_counter &gt;= 2:\n            self.current_level -= self.step_size\n            self.correct_counter = 0\n    else:\n        self.current_level += self.step_size\n        self.correct_counter = 0\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.TrialPlacement","title":"TrialPlacement","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract interface for trial placement strategies.</p> <p>Methods:</p> Name Description <code>propose</code> <p>Propose the next batch of trials.</p> <code>All trial placement strategies</code>"},{"location":"reference/trial_placement/#psyphy.trial_placement.TrialPlacement.propose","title":"propose","text":"<pre><code>propose(posterior: Any, batch_size: int)\n</code></pre> <p>Propose the next batch of trials.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>Posterior</code> <p>Posterior distribution (MAP, Laplace, or MCMC).</p> required <code>batch_size</code> <code>int</code> <p>Number of trials to propose.</p> required <p>Returns:</p> Type Description <code>TrialBatch</code> <p>Proposed batch of (reference, probe) stimuli.</p> Source code in <code>src/psyphy/trial_placement/base.py</code> <pre><code>@abstractmethod\ndef propose(self, posterior: Any, batch_size: int):\n    \"\"\"\n    Propose the next batch of trials.\n\n    Parameters\n    ----------\n    posterior : Posterior\n        Posterior distribution (MAP, Laplace, or MCMC).\n    batch_size : int\n        Number of trials to propose.\n\n    Returns\n    -------\n    TrialBatch\n        Proposed batch of (reference, probe) stimuli.\n    \"\"\"\n    return NotImplementedError()\n</code></pre>"},{"location":"reference/trial_placement/#base","title":"Base","text":""},{"location":"reference/trial_placement/#psyphy.trial_placement.base","title":"base","text":"base.py <p>Abstract base class for trial placement strategies.</p> <p>Classes:</p> Name Description <code>TrialPlacement</code> <p>Abstract interface for trial placement strategies.</p>"},{"location":"reference/trial_placement/#psyphy.trial_placement.base.TrialPlacement","title":"TrialPlacement","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract interface for trial placement strategies.</p> <p>Methods:</p> Name Description <code>propose</code> <p>Propose the next batch of trials.</p> <code>All trial placement strategies</code>"},{"location":"reference/trial_placement/#psyphy.trial_placement.base.TrialPlacement.propose","title":"propose","text":"<pre><code>propose(posterior: Any, batch_size: int)\n</code></pre> <p>Propose the next batch of trials.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>Posterior</code> <p>Posterior distribution (MAP, Laplace, or MCMC).</p> required <code>batch_size</code> <code>int</code> <p>Number of trials to propose.</p> required <p>Returns:</p> Type Description <code>TrialBatch</code> <p>Proposed batch of (reference, probe) stimuli.</p> Source code in <code>src/psyphy/trial_placement/base.py</code> <pre><code>@abstractmethod\ndef propose(self, posterior: Any, batch_size: int):\n    \"\"\"\n    Propose the next batch of trials.\n\n    Parameters\n    ----------\n    posterior : Posterior\n        Posterior distribution (MAP, Laplace, or MCMC).\n    batch_size : int\n        Number of trials to propose.\n\n    Returns\n    -------\n    TrialBatch\n        Proposed batch of (reference, probe) stimuli.\n    \"\"\"\n    return NotImplementedError()\n</code></pre>"},{"location":"reference/trial_placement/#grid","title":"Grid","text":""},{"location":"reference/trial_placement/#psyphy.trial_placement.grid","title":"grid","text":"grid.py <p>Grid-based placement strategy.</p> <p>MVP: - Iterates through a fixed list of grid points. - Ignores the posterior (non-adaptive).</p> <p>Full WPPM mode: - Could refine the grid adaptively around regions of high posterior uncertainty.</p> <p>Classes:</p> Name Description <code>GridPlacement</code> <p>Fixed grid placement.</p>"},{"location":"reference/trial_placement/#psyphy.trial_placement.grid.GridPlacement","title":"GridPlacement","text":"<pre><code>GridPlacement(grid_points)\n</code></pre> <p>               Bases: <code>TrialPlacement</code></p> <p>Fixed grid placement.</p> <p>Parameters:</p> Name Type Description Default <code>grid_points</code> <code>list of (ref, probe)</code> <p>Predefined set of trial stimuli.</p> required Notes <ul> <li>grid = your set of allowable trials.</li> <li>this class simply walks through that set.</li> </ul> <p>Methods:</p> Name Description <code>propose</code> <p>Return the next batch of trials from the grid.</p> <p>Attributes:</p> Name Type Description <code>grid_points</code> Source code in <code>src/psyphy/trial_placement/grid.py</code> <pre><code>def __init__(self, grid_points):\n    self.grid_points = list(grid_points)\n    self._index = 0\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.grid.GridPlacement.grid_points","title":"grid_points","text":"<pre><code>grid_points = list(grid_points)\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.grid.GridPlacement.propose","title":"propose","text":"<pre><code>propose(posterior, batch_size: int) -&gt; TrialBatch\n</code></pre> <p>Return the next batch of trials from the grid.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>Posterior</code> <p>Ignored in MVP (grid is non-adaptive).</p> required <code>batch_size</code> <code>int</code> <p>Number of trials to return.</p> required <p>Returns:</p> Type Description <code>TrialBatch</code> <p>Fixed batch of (ref, probe).</p> Source code in <code>src/psyphy/trial_placement/grid.py</code> <pre><code>def propose(self, posterior, batch_size: int) -&gt; TrialBatch:\n    \"\"\"\n    Return the next batch of trials from the grid.\n\n    Parameters\n    ----------\n    posterior : Posterior\n        Ignored in MVP (grid is non-adaptive).\n    batch_size : int\n        Number of trials to return.\n\n    Returns\n    -------\n    TrialBatch\n        Fixed batch of (ref, probe).\n    \"\"\"\n    start, end = self._index, self._index + batch_size\n    batch = self.grid_points[start:end]\n    self._index = end\n    return TrialBatch.from_stimuli(batch)\n</code></pre>"},{"location":"reference/trial_placement/#sobol","title":"Sobol","text":""},{"location":"reference/trial_placement/#psyphy.trial_placement.sobol","title":"sobol","text":"sobol.py <p>Sobol quasi-random placement.</p> <p>MVP: - Uses a Sobol engine to generate low-discrepancy points. - Ignores the posterior (pure exploration).</p> <p>Full WPPM mode: - Could combine Sobol exploration (early) with posterior-aware exploitation (later).</p> <p>Classes:</p> Name Description <code>SobolPlacement</code> <p>Sobol quasi-random placement.</p>"},{"location":"reference/trial_placement/#psyphy.trial_placement.sobol.SobolPlacement","title":"SobolPlacement","text":"<pre><code>SobolPlacement(dim: int, bounds, seed: int = 0)\n</code></pre> <p>               Bases: <code>TrialPlacement</code></p> <p>Sobol quasi-random placement.</p> <p>Parameters:</p> Name Type Description Default <code>dim</code> <code>int</code> <p>Dimensionality of stimulus space.</p> required <code>bounds</code> <code>list of (low, high)</code> <p>Bounds per dimension.</p> required <code>seed</code> <code>int</code> <p>RNG seed.</p> <code>0</code> <p>Methods:</p> Name Description <code>propose</code> <p>Propose Sobol points (ignores posterior).</p> <p>Attributes:</p> Name Type Description <code>bounds</code> <code>engine</code> Source code in <code>src/psyphy/trial_placement/sobol.py</code> <pre><code>def __init__(self, dim: int, bounds, seed: int = 0):\n    self.engine = Sobol(d=dim, scramble=True, seed=seed)\n    self.bounds = bounds\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.sobol.SobolPlacement.bounds","title":"bounds","text":"<pre><code>bounds = bounds\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.sobol.SobolPlacement.engine","title":"engine","text":"<pre><code>engine = Sobol(d=dim, scramble=True, seed=seed)\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.sobol.SobolPlacement.propose","title":"propose","text":"<pre><code>propose(posterior, batch_size: int) -&gt; TrialBatch\n</code></pre> <p>Propose Sobol points (ignores posterior).</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>Posterior</code> <p>Ignored in MVP.</p> required <code>batch_size</code> <code>int</code> <p>Number of trials to return.</p> required <p>Returns:</p> Type Description <code>TrialBatch</code> <p>Candidate trials from Sobol sequence.</p> Notes <p>MVP:     Pure exploration of space. Full WPPM mode:     Use Sobol as initialization, then switch to InfoGain.</p> Source code in <code>src/psyphy/trial_placement/sobol.py</code> <pre><code>def propose(self, posterior, batch_size: int) -&gt; TrialBatch:\n    \"\"\"\n    Propose Sobol points (ignores posterior).\n\n    Parameters\n    ----------\n    posterior : Posterior\n        Ignored in MVP.\n    batch_size : int\n        Number of trials to return.\n\n    Returns\n    -------\n    TrialBatch\n        Candidate trials from Sobol sequence.\n\n    Notes\n    -----\n    MVP:\n        Pure exploration of space.\n    Full WPPM mode:\n        Use Sobol as initialization, then switch to InfoGain.\n    \"\"\"\n    raw = self.engine.random(batch_size)\n    scaled = [\n        low + (high - low) * raw[:, i]\n        for i, (low, high) in enumerate(self.bounds)\n    ]\n    # Convert column-wise scaled arrays into list of probe vectors\n    probes = [tuple(vals) for vals in zip(*scaled)]\n    # MVP: use a zero reference vector of matching dimension\n    dim = len(self.bounds)\n    zero_ref = 0.0 if dim == 1 else tuple(0.0 for _ in range(dim))\n    trials = [(zero_ref, p) for p in probes]\n    return TrialBatch.from_stimuli(trials)\n</code></pre>"},{"location":"reference/trial_placement/#staircase","title":"Staircase","text":""},{"location":"reference/trial_placement/#psyphy.trial_placement.staircase","title":"staircase","text":"staircase.py <p>Classical staircase placement (1-up, 2-down).</p> <p>MVP: - Purely response-driven, 1D only. - Ignores posterior.</p> <p>Full WPPM mode: - Extend to multi-D tasks, integrate  with WPPM-based discriminability thresholds.</p> <p>Classes:</p> Name Description <code>StaircasePlacement</code> <p>Staircase procedure.</p>"},{"location":"reference/trial_placement/#psyphy.trial_placement.staircase.StaircasePlacement","title":"StaircasePlacement","text":"<pre><code>StaircasePlacement(\n    start_level: float,\n    step_size: float,\n    rule: str = \"1up-2down\",\n)\n</code></pre> <p>               Bases: <code>TrialPlacement</code></p> <p>Staircase procedure.</p> <p>Parameters:</p> Name Type Description Default <code>start_level</code> <code>float</code> <p>Starting stimulus intensity.</p> required <code>step_size</code> <code>float</code> <p>Step increment.</p> required <code>rule</code> <code>str</code> <p>Adaptive rule.</p> <code>\"1up-2down\"</code> <p>Methods:</p> Name Description <code>propose</code> <p>Return next trial(s) based on staircase rule.</p> <code>update</code> <p>Update staircase level given last response.</p> <p>Attributes:</p> Name Type Description <code>correct_counter</code> <code>current_level</code> <code>rule</code> <code>step_size</code> Source code in <code>src/psyphy/trial_placement/staircase.py</code> <pre><code>def __init__(self, start_level: float, step_size: float, rule: str = \"1up-2down\"):\n    self.current_level = start_level\n    self.step_size = step_size\n    self.rule = rule\n    self.correct_counter = 0\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.staircase.StaircasePlacement.correct_counter","title":"correct_counter","text":"<pre><code>correct_counter = 0\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.staircase.StaircasePlacement.current_level","title":"current_level","text":"<pre><code>current_level = start_level\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.staircase.StaircasePlacement.rule","title":"rule","text":"<pre><code>rule = rule\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.staircase.StaircasePlacement.step_size","title":"step_size","text":"<pre><code>step_size = step_size\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.staircase.StaircasePlacement.propose","title":"propose","text":"<pre><code>propose(posterior, batch_size: int) -&gt; TrialBatch\n</code></pre> <p>Return next trial(s) based on staircase rule.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>Posterior</code> <p>Ignored in MVP (not posterior-aware).</p> required <code>batch_size</code> <code>int</code> <p>Number of trials to propose.</p> required <p>Returns:</p> Type Description <code>TrialBatch</code> <p>Batch of trials with current staircase level.</p> Source code in <code>src/psyphy/trial_placement/staircase.py</code> <pre><code>def propose(self, posterior, batch_size: int) -&gt; TrialBatch:\n    \"\"\"\n    Return next trial(s) based on staircase rule.\n\n    Parameters\n    ----------\n    posterior : Posterior\n        Ignored in MVP (not posterior-aware).\n    batch_size : int\n        Number of trials to propose.\n\n    Returns\n    -------\n    TrialBatch\n        Batch of trials with current staircase level.\n    \"\"\"\n    trials = [(0.0, self.current_level)] * batch_size  # Stub: (ref=0, probe=level)\n    return TrialBatch.from_stimuli(trials)\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.staircase.StaircasePlacement.update","title":"update","text":"<pre><code>update(response: int)\n</code></pre> <p>Update staircase level given last response.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>int</code> <p>1 = correct, 0 = incorrect.</p> required Source code in <code>src/psyphy/trial_placement/staircase.py</code> <pre><code>def update(self, response: int):\n    \"\"\"\n    Update staircase level given last response.\n\n    Parameters\n    ----------\n    response : int\n        1 = correct, 0 = incorrect.\n    \"\"\"\n    if response == 1:\n        self.correct_counter += 1\n        if self.rule == \"1up-2down\" and self.correct_counter &gt;= 2:\n            self.current_level -= self.step_size\n            self.correct_counter = 0\n    else:\n        self.current_level += self.step_size\n        self.correct_counter = 0\n</code></pre>"},{"location":"reference/trial_placement/#greedy-map","title":"Greedy MAP","text":""},{"location":"reference/trial_placement/#psyphy.trial_placement.greedy_map","title":"greedy_map","text":"greedy_map.py <p>Greedy placement using MAP estimate.</p> <p>MVP: - Returns first N candidates (stub). - Ignores posterior discriminability.</p> <p>Full WPPM mode: - Score each candidate using posterior.MAP_params(). - Rank candidates by informativeness (e.g., discriminability).</p> <p>Classes:</p> Name Description <code>GreedyMAPPlacement</code> <p>Greedy adaptive placement (MAP-based).</p>"},{"location":"reference/trial_placement/#psyphy.trial_placement.greedy_map.GreedyMAPPlacement","title":"GreedyMAPPlacement","text":"<pre><code>GreedyMAPPlacement(candidate_pool)\n</code></pre> <p>               Bases: <code>TrialPlacement</code></p> <p>Greedy adaptive placement (MAP-based).</p> <p>Parameters:</p> Name Type Description Default <code>candidate_pool</code> <code>list of (ref, probe)</code> <p>Candidate stimuli.</p> required <p>Methods:</p> Name Description <code>propose</code> <p>Select trials based on MAP discriminability.</p> <p>Attributes:</p> Name Type Description <code>pool</code> Source code in <code>src/psyphy/trial_placement/greedy_map.py</code> <pre><code>def __init__(self, candidate_pool):\n    self.pool = candidate_pool\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.greedy_map.GreedyMAPPlacement.pool","title":"pool","text":"<pre><code>pool = candidate_pool\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.greedy_map.GreedyMAPPlacement.propose","title":"propose","text":"<pre><code>propose(posterior, batch_size: int) -&gt; TrialBatch\n</code></pre> <p>Select trials based on MAP discriminability.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>Posterior</code> <p>Posterior object. Provides MAP params.</p> required <code>batch_size</code> <code>int</code> <p>Number of trials to propose.</p> required <p>Returns:</p> Type Description <code>TrialBatch</code> <p>Selected trials.</p> Notes <p>MVP:     Returns first N candidates. Full WPPM mode:     - Score all candidates with _score_candidate().     - Rank candidates by score.     - Select top-N.</p> Source code in <code>src/psyphy/trial_placement/greedy_map.py</code> <pre><code>def propose(self, posterior, batch_size: int) -&gt; TrialBatch:\n    \"\"\"\n    Select trials based on MAP discriminability.\n\n    Parameters\n    ----------\n    posterior : Posterior\n        Posterior object. Provides MAP params.\n    batch_size : int\n        Number of trials to propose.\n\n    Returns\n    -------\n    TrialBatch\n        Selected trials.\n\n    Notes\n    -----\n    MVP:\n        Returns first N candidates.\n    Full WPPM mode:\n        - Score all candidates with _score_candidate().\n        - Rank candidates by score.\n        - Select top-N.\n    \"\"\"\n    # MVP stub\n    selected = self.pool[:batch_size]\n\n    # TODO: Uncomment when scoring is implemented\n    # scores = [self._score_candidate(posterior, cand) for cand in self.pool]\n    # idx = jnp.argsort(jnp.array(scores))[::-1]  # sort descending\n    # selected = [self.pool[i] for i in idx[:batch_size]]\n\n    return TrialBatch.from_stimuli(selected)\n</code></pre>"},{"location":"reference/trial_placement/#info-gain","title":"Info Gain","text":""},{"location":"reference/trial_placement/#psyphy.trial_placement.info_gain","title":"info_gain","text":"info_gain.py <p>Information-gain placement (e.g., entropy, expected Absolute Volume Change (EAVC)).</p> <p>MVP: - Returns first N candidates (stub)</p> <p>Full WPPM mode: - Requires posterior.sample() (Laplace/MCMC). - For each candidate:     * Draw posterior samples.     * Compute predictive distribution of responses.     * Compute expected entropy or EAVC.     * Rank candidates by information gain.</p> <p>Classes:</p> Name Description <code>InfoGainPlacement</code> <p>Information-gain adaptive placement.</p>"},{"location":"reference/trial_placement/#psyphy.trial_placement.info_gain.InfoGainPlacement","title":"InfoGainPlacement","text":"<pre><code>InfoGainPlacement(candidate_pool)\n</code></pre> <p>               Bases: <code>TrialPlacement</code></p> <p>Information-gain adaptive placement.</p> <p>Parameters:</p> Name Type Description Default <code>candidate_pool</code> <code>list of (ref, probe)</code> <p>Candidate stimuli.</p> required <p>Methods:</p> Name Description <code>propose</code> <p>Propose trials that maximize information gain.</p> <p>Attributes:</p> Name Type Description <code>pool</code> Source code in <code>src/psyphy/trial_placement/info_gain.py</code> <pre><code>def __init__(self, candidate_pool):\n    self.pool = candidate_pool\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.info_gain.InfoGainPlacement.pool","title":"pool","text":"<pre><code>pool = candidate_pool\n</code></pre>"},{"location":"reference/trial_placement/#psyphy.trial_placement.info_gain.InfoGainPlacement.propose","title":"propose","text":"<pre><code>propose(posterior, batch_size: int) -&gt; TrialBatch\n</code></pre> <p>Propose trials that maximize information gain.</p> <p>Parameters:</p> Name Type Description Default <code>posterior</code> <code>Posterior</code> <p>Posterior object. Must support sample().</p> required <code>batch_size</code> <code>int</code> <p>Number of trials to propose.</p> required <p>Returns:</p> Type Description <code>TrialBatch</code> <p>Selected trials.</p> Notes <p>MVP:     Returns first N candidates. Full WPPM mode:     - Score all candidates with _score_candidate().     - Rank candidates by score.     - Select top-N.</p> Source code in <code>src/psyphy/trial_placement/info_gain.py</code> <pre><code>def propose(self, posterior, batch_size: int) -&gt; TrialBatch:\n    \"\"\"\n    Propose trials that maximize information gain.\n\n    Parameters\n    ----------\n    posterior : Posterior\n        Posterior object. Must support sample().\n    batch_size : int\n        Number of trials to propose.\n\n    Returns\n    -------\n    TrialBatch\n        Selected trials.\n\n    Notes\n    -----\n    MVP:\n        Returns first N candidates.\n    Full WPPM mode:\n        - Score all candidates with _score_candidate().\n        - Rank candidates by score.\n        - Select top-N.\n    \"\"\"\n    # MVP stub\n    selected = self.pool[:batch_size]\n\n    # TODO: Uncomment when scoring is implemented\n    # scores = [self._score_candidate(posterior, cand) for cand in self.pool]\n    # idx = jnp.argsort(jnp.array(scores))[::-1]\n    # selected = [self.pool[i] for i in idx[:batch_size]]\n\n    return TrialBatch.from_stimuli(selected)\n</code></pre>"},{"location":"reference/utils/","title":"Utils","text":""},{"location":"reference/utils/#package","title":"Package","text":""},{"location":"reference/utils/#psyphy.utils","title":"utils","text":""},{"location":"reference/utils/#psyphy.utils--utils","title":"utils","text":"<p>Shared utility functions and helpers for psyphy.</p> <p>This subpackage provides: - candidates : functions for generating candidate stimulus pools. - math : mathematical utilities (basis functions, distances, kernels). - rng : random number handling for reproducibility.</p> MVP implementation <ul> <li>candidates: grid, Sobol, custom pools.</li> <li>math: Chebyshev basis, Mahalanobis distance, RBF kernel.</li> <li>rng: seed() and split() for JAX PRNG keys.</li> </ul> Full WPPM mode <ul> <li>candidates: adaptive refinement around posterior uncertainty.</li> <li>math: richer kernels and basis expansions for Wishart processes.</li> <li>rng: experiment-wide RNG registry.</li> </ul> <p>Functions:</p> Name Description <code>chebyshev_basis</code> <p>Construct the Chebyshev polynomial basis matrix T_0..T_degree evaluated at x.</p> <code>custom_candidates</code> <p>Wrap a user-defined list of probes into candidate pairs.</p> <code>grid_candidates</code> <p>Generate grid-based candidate probes around a reference.</p> <code>mahalanobis_distance</code> <p>Compute squared Mahalanobis distance between x and mean.</p> <code>rbf_kernel</code> <p>Radial Basis Function (RBF) kernel between two sets of points.</p> <code>seed</code> <p>Create a new PRNG key from an integer seed.</p> <code>sobol_candidates</code> <p>Generate Sobol quasi-random candidates within bounds.</p> <code>split</code> <p>Split a PRNG key into multiple independent keys.</p>"},{"location":"reference/utils/#psyphy.utils.chebyshev_basis","title":"chebyshev_basis","text":"<pre><code>chebyshev_basis(x: ndarray, degree: int) -&gt; ndarray\n</code></pre> <p>Construct the Chebyshev polynomial basis matrix T_0..T_degree evaluated at x.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input points of shape (N,). For best numerical properties, values should lie in [-1, 1].</p> required <code>degree</code> <code>int</code> <p>Maximum polynomial degree (&gt;= 0). The output includes columns for T_0 through T_degree.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of shape (N, degree + 1) where column j contains T_j(x).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>degree</code> is negative or <code>x</code> is not 1-D.</p> Notes <p>Uses the three-term recurrence:     T_0(x) = 1     T_1(x) = x     T_{n+1}(x) = 2 x T_n(x) - T_{n-1}(x) The Chebyshev polynomials are orthogonal on [-1, 1] with weight (1 / sqrt(1 - x^2)).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; x = jnp.linspace(-1, 1, 5)\n&gt;&gt;&gt; B = chebyshev_basis(x, degree=3)  # columns: T0, T1, T2, T3\n</code></pre> Source code in <code>src/psyphy/utils/math.py</code> <pre><code>def chebyshev_basis(x: jnp.ndarray, degree: int) -&gt; jnp.ndarray:\n    \"\"\"\n    Construct the Chebyshev polynomial basis matrix T_0..T_degree evaluated at x.\n\n    Parameters\n    ----------\n    x : jnp.ndarray\n        Input points of shape (N,). For best numerical properties, values should lie in [-1, 1].\n    degree : int\n        Maximum polynomial degree (&gt;= 0). The output includes columns for T_0 through T_degree.\n\n    Returns\n    -------\n    jnp.ndarray\n        Array of shape (N, degree + 1) where column j contains T_j(x).\n\n    Raises\n    ------\n    ValueError\n        If `degree` is negative or `x` is not 1-D.\n\n    Notes\n    -----\n    Uses the three-term recurrence:\n        T_0(x) = 1\n        T_1(x) = x\n        T_{n+1}(x) = 2 x T_n(x) - T_{n-1}(x)\n    The Chebyshev polynomials are orthogonal on [-1, 1] with weight (1 / sqrt(1 - x^2)).\n\n    Examples\n    --------\n    &gt;&gt;&gt; import jax.numpy as jnp\n    &gt;&gt;&gt; x = jnp.linspace(-1, 1, 5)\n    &gt;&gt;&gt; B = chebyshev_basis(x, degree=3)  # columns: T0, T1, T2, T3\n    \"\"\"\n    if degree &lt; 0:\n        raise ValueError(\"degree must be &gt;= 0\")\n    if x.ndim != 1:\n        raise ValueError(\"x must be 1-D (shape (N,))\")\n\n    # Ensure a floating dtype (Chebyshev recurrences are polynomial in x)\n    x = x.astype(jnp.result_type(x, 0.0))\n\n    N = x.shape[0]\n\n    # Handle small degrees explicitly.\n    if degree == 0:\n        return jnp.ones((N, 1), dtype=x.dtype)\n    if degree == 1:\n        return jnp.stack([jnp.ones_like(x), x], axis=1)\n\n    # Initialize T0 and T1 columns.\n    T0 = jnp.ones_like(x)\n    T1 = x\n\n    # Scan to generate T2..T_degree in a JIT-friendly way (avoids Python-side loops).\n    def step(carry, _):\n        # compute next Chebyshev polynomial\n        Tm1, Tm = carry\n        Tnext = 2.0 * x * Tm - Tm1\n        return (Tm, Tnext), Tnext # new carry, plus an output to collect\n\n    # Jax friendly loop\n    (final_Tm1_ignored, final_Tm_ignored), Ts = lax.scan(step, (T0, T1), xs=None, length=degree - 1)\n    # Ts has shape (degree-1, N) and holds [T2, T3, ..., T_degree]\n    B = jnp.concatenate(\n        [T0[:, None], T1[:, None], jnp.swapaxes(Ts, 0, 1)],\n        axis=1\n    )\n    return B\n</code></pre>"},{"location":"reference/utils/#psyphy.utils.custom_candidates","title":"custom_candidates","text":"<pre><code>custom_candidates(\n    reference: ndarray, probe_list: List[ndarray]\n) -&gt; List[Stimulus]\n</code></pre> <p>Wrap a user-defined list of probes into candidate pairs.</p> <p>Parameters:</p> Name Type Description Default <code>reference</code> <code>(ndarray, shape(D))</code> <p>Reference stimulus.</p> required <code>probe_list</code> <code>list of jnp.ndarray</code> <p>Explicitly chosen probe vectors.</p> required <p>Returns:</p> Type Description <code>list of Stimulus</code> <p>Candidate (reference, probe) pairs.</p> Notes <ul> <li>Useful when hardware constraints (monitor gamut, auditory frequencies)   restrict the set of valid stimuli.</li> <li>Full WPPM mode: this pool could be pruned or expanded dynamically   depending on posterior fit quality.</li> </ul> Source code in <code>src/psyphy/utils/candidates.py</code> <pre><code>def custom_candidates(reference: jnp.ndarray, probe_list: List[jnp.ndarray]) -&gt; List[Stimulus]:\n    \"\"\"\n    Wrap a user-defined list of probes into candidate pairs.\n\n    Parameters\n    ----------\n    reference : jnp.ndarray, shape (D,)\n        Reference stimulus.\n    probe_list : list of jnp.ndarray\n        Explicitly chosen probe vectors.\n\n    Returns\n    -------\n    list of Stimulus\n        Candidate (reference, probe) pairs.\n\n    Notes\n    -----\n    - Useful when hardware constraints (monitor gamut, auditory frequencies)\n      restrict the set of valid stimuli.\n    - Full WPPM mode: this pool could be pruned or expanded dynamically\n      depending on posterior fit quality.\n    \"\"\"\n    return [(reference, probe) for probe in probe_list]\n</code></pre>"},{"location":"reference/utils/#psyphy.utils.grid_candidates","title":"grid_candidates","text":"<pre><code>grid_candidates(\n    reference: ndarray,\n    radii: List[float],\n    directions: int = 16,\n) -&gt; List[Stimulus]\n</code></pre> <p>Generate grid-based candidate probes around a reference.</p> <p>Parameters:</p> Name Type Description Default <code>reference</code> <code>(ndarray, shape(D))</code> <p>Reference stimulus in model space.</p> required <code>radii</code> <code>list of float</code> <p>Distances from reference to probe.</p> required <code>directions</code> <code>int</code> <p>Number of angular directions.</p> <code>16</code> <p>Returns:</p> Type Description <code>list of Stimulus</code> <p>Candidate (reference, probe) pairs.</p> Notes <ul> <li>MVP: probes lie on concentric circles around reference.</li> <li>Full WPPM mode: could adaptively refine grid around regions of   high posterior uncertainty.</li> </ul> Source code in <code>src/psyphy/utils/candidates.py</code> <pre><code>def grid_candidates(reference: jnp.ndarray, radii: List[float], directions: int = 16) -&gt; List[Stimulus]:\n    \"\"\"\n    Generate grid-based candidate probes around a reference.\n\n    Parameters\n    ----------\n    reference : jnp.ndarray, shape (D,)\n        Reference stimulus in model space.\n    radii : list of float\n        Distances from reference to probe.\n    directions : int, default=16\n        Number of angular directions.\n\n    Returns\n    -------\n    list of Stimulus\n        Candidate (reference, probe) pairs.\n\n    Notes\n    -----\n    - MVP: probes lie on concentric circles around reference.\n    - Full WPPM mode: could adaptively refine grid around regions of\n      high posterior uncertainty.\n    \"\"\"\n    candidates = []\n    angles = jnp.linspace(0, 2 * jnp.pi, directions, endpoint=False)\n    for r in radii:\n        probes = [reference + r * jnp.array([jnp.cos(a), jnp.sin(a)]) for a in angles]\n        candidates.extend([(reference, p) for p in probes])\n    return candidates\n</code></pre>"},{"location":"reference/utils/#psyphy.utils.mahalanobis_distance","title":"mahalanobis_distance","text":"<pre><code>mahalanobis_distance(\n    x: ndarray, mean: ndarray, cov_inv: ndarray\n) -&gt; ndarray\n</code></pre> <p>Compute squared Mahalanobis distance between x and mean.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Data vector, shape (D,).</p> required <code>mean</code> <code>ndarray</code> <p>Mean vector, shape (D,).</p> required <code>cov_inv</code> <code>ndarray</code> <p>Inverse covariance matrix, shape (D, D).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Scalar squared Mahalanobis distance.</p> Notes <ul> <li>Formula: d^2 = (x - mean)^T \u03a3^{-1} (x - mean)</li> <li>Used in WPPM discriminability calculations.</li> </ul> Source code in <code>src/psyphy/utils/math.py</code> <pre><code>def mahalanobis_distance(x: jnp.ndarray, mean: jnp.ndarray, cov_inv: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"\n    Compute squared Mahalanobis distance between x and mean.\n\n    Parameters\n    ----------\n    x : jnp.ndarray\n        Data vector, shape (D,).\n    mean : jnp.ndarray\n        Mean vector, shape (D,).\n    cov_inv : jnp.ndarray\n        Inverse covariance matrix, shape (D, D).\n\n    Returns\n    -------\n    jnp.ndarray\n        Scalar squared Mahalanobis distance.\n\n    Notes\n    -----\n    - Formula: d^2 = (x - mean)^T \u03a3^{-1} (x - mean)\n    - Used in WPPM discriminability calculations.\n    \"\"\"\n    delta = x - mean\n    return jnp.dot(delta, cov_inv @ delta)\n</code></pre>"},{"location":"reference/utils/#psyphy.utils.rbf_kernel","title":"rbf_kernel","text":"<pre><code>rbf_kernel(\n    x1: ndarray, x2: ndarray, lengthscale: float = 1.0\n) -&gt; ndarray\n</code></pre> <p>Radial Basis Function (RBF) kernel between two sets of points.</p> <p>Parameters:</p> Name Type Description Default <code>x1</code> <code>ndarray</code> <p>First set of points, shape (N, D).</p> required <code>x2</code> <code>ndarray</code> <p>Second set of points, shape (M, D).</p> required <code>lengthscale</code> <code>float</code> <p>Length-scale parameter controlling smoothness.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Kernel matrix of shape (N, M).</p> Notes <ul> <li>RBF kernel: k(x, x') = exp(-||x - x'||^2 / (2 * lengthscale^2))</li> <li>Default used for Gaussian processes for smooth covariance priors in Full WPPM mode.</li> </ul> Source code in <code>src/psyphy/utils/math.py</code> <pre><code>def rbf_kernel(x1: jnp.ndarray, x2: jnp.ndarray, lengthscale: float = 1.0) -&gt; jnp.ndarray:\n    \"\"\"\n    Radial Basis Function (RBF) kernel between two sets of points.\n\n\n    Parameters\n    ----------\n    x1 : jnp.ndarray\n        First set of points, shape (N, D).\n    x2 : jnp.ndarray\n        Second set of points, shape (M, D).\n    lengthscale : float, default=1.0\n        Length-scale parameter controlling smoothness.\n\n    Returns\n    -------\n    jnp.ndarray\n        Kernel matrix of shape (N, M).\n\n    Notes\n    -----\n    - RBF kernel: k(x, x') = exp(-||x - x'||^2 / (2 * lengthscale^2))\n    - Default used for Gaussian processes for smooth covariance priors in Full WPPM mode.\n    \"\"\"\n    sqdist = jnp.sum((x1[:, None, :] - x2[None, :, :])**2, axis=-1)\n    return jnp.exp(-0.5 * sqdist / (lengthscale**2))\n</code></pre>"},{"location":"reference/utils/#psyphy.utils.seed","title":"seed","text":"<pre><code>seed(seed_value: int) -&gt; KeyArray\n</code></pre> <p>Create a new PRNG key from an integer seed.</p> <p>Parameters:</p> Name Type Description Default <code>seed_value</code> <code>int</code> <p>Seed for random number generation.</p> required <p>Returns:</p> Type Description <code>KeyArray</code> <p>New PRNG key.</p> Source code in <code>src/psyphy/utils/rng.py</code> <pre><code>def seed(seed_value: int) -&gt; jax.random.KeyArray:\n    \"\"\"\n    Create a new PRNG key from an integer seed.\n\n    Parameters\n    ----------\n    seed_value : int\n        Seed for random number generation.\n\n    Returns\n    -------\n    jax.random.KeyArray\n        New PRNG key.\n    \"\"\"\n    return jr.PRNGKey(seed_value)\n</code></pre>"},{"location":"reference/utils/#psyphy.utils.sobol_candidates","title":"sobol_candidates","text":"<pre><code>sobol_candidates(\n    reference: ndarray,\n    n: int,\n    bounds: List[Tuple[float, float]],\n    seed: int = 0,\n) -&gt; List[Stimulus]\n</code></pre> <p>Generate Sobol quasi-random candidates within bounds.</p> <p>Parameters:</p> Name Type Description Default <code>reference</code> <code>(ndarray, shape(D))</code> <p>Reference stimulus.</p> required <code>n</code> <code>int</code> <p>Number of candidates to generate.</p> required <code>bounds</code> <code>list of (low, high)</code> <p>Bounds per dimension.</p> required <code>seed</code> <code>int</code> <p>Random seed.</p> <code>0</code> <p>Returns:</p> Type Description <code>list of Stimulus</code> <p>Candidate (reference, probe) pairs.</p> Notes <ul> <li>MVP: uniform coverage of space using low-discrepancy Sobol sequence.</li> <li>Full WPPM mode: Sobol could be used for initialization,   then hand off to posterior-aware strategies.</li> </ul> Source code in <code>src/psyphy/utils/candidates.py</code> <pre><code>def sobol_candidates(reference: jnp.ndarray, n: int, bounds: List[Tuple[float, float]], seed: int = 0) -&gt; List[Stimulus]:\n    \"\"\"\n    Generate Sobol quasi-random candidates within bounds.\n\n    Parameters\n    ----------\n    reference : jnp.ndarray, shape (D,)\n        Reference stimulus.\n    n : int\n        Number of candidates to generate.\n    bounds : list of (low, high)\n        Bounds per dimension.\n    seed : int, default=0\n        Random seed.\n\n    Returns\n    -------\n    list of Stimulus\n        Candidate (reference, probe) pairs.\n\n    Notes\n    -----\n    - MVP: uniform coverage of space using low-discrepancy Sobol sequence.\n    - Full WPPM mode: Sobol could be used for initialization,\n      then hand off to posterior-aware strategies.\n    \"\"\"\n    from scipy.stats.qmc import Sobol\n    dim = len(bounds)\n    engine = Sobol(d=dim, scramble=True, seed=seed)\n    raw = engine.random(n)\n    scaled = [low + (high - low) * raw[:, i] for i, (low, high) in enumerate(bounds)]\n    probes = np.stack(scaled, axis=-1)\n    return [(reference, jnp.array(p)) for p in probes]\n</code></pre>"},{"location":"reference/utils/#psyphy.utils.split","title":"split","text":"<pre><code>split(key: KeyArray, num: int = 2) -&gt; Tuple[KeyArray, ...]\n</code></pre> <p>Split a PRNG key into multiple independent keys.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>KeyArray</code> <p>RNG key to split.</p> required <code>num</code> <code>int</code> <p>Number of new keys to return.</p> <code>2</code> <p>Returns:</p> Type Description <code>tuple of jax.random.KeyArray</code> <p>Independent new PRNG keys.</p> Source code in <code>src/psyphy/utils/rng.py</code> <pre><code>def split(key: jax.random.KeyArray, num: int = 2) -&gt; Tuple[jax.random.KeyArray, ...]:\n    \"\"\"\n    Split a PRNG key into multiple independent keys.\n\n    Parameters\n    ----------\n    key : jax.random.KeyArray\n        RNG key to split.\n    num : int, default=2\n        Number of new keys to return.\n\n    Returns\n    -------\n    tuple of jax.random.KeyArray\n        Independent new PRNG keys.\n    \"\"\"\n    return jr.split(key, num=num)\n</code></pre>"},{"location":"reference/utils/#rng","title":"RNG","text":""},{"location":"reference/utils/#psyphy.utils.rng","title":"rng","text":"rng.py <p>Random number utilities for psyphy.</p> <p>This module standardizes RNG handling across the package, especially important when mixing NumPy and JAX.</p> <p>MVP implementation: - Wrappers around JAX PRNG keys. - Helpers for reproducibility.</p> <p>Future extensions: - Experiment-wide RNG registry. - Splitting strategies for parallel adaptive placement.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import jax\n&gt;&gt;&gt; from psyphy.utils.rng import seed, split\n&gt;&gt;&gt; key = seed(0)\n&gt;&gt;&gt; k1, k2 = split(key)\n</code></pre> <p>Functions:</p> Name Description <code>seed</code> <p>Create a new PRNG key from an integer seed.</p> <code>split</code> <p>Split a PRNG key into multiple independent keys.</p>"},{"location":"reference/utils/#psyphy.utils.rng.seed","title":"seed","text":"<pre><code>seed(seed_value: int) -&gt; KeyArray\n</code></pre> <p>Create a new PRNG key from an integer seed.</p> <p>Parameters:</p> Name Type Description Default <code>seed_value</code> <code>int</code> <p>Seed for random number generation.</p> required <p>Returns:</p> Type Description <code>KeyArray</code> <p>New PRNG key.</p> Source code in <code>src/psyphy/utils/rng.py</code> <pre><code>def seed(seed_value: int) -&gt; jax.random.KeyArray:\n    \"\"\"\n    Create a new PRNG key from an integer seed.\n\n    Parameters\n    ----------\n    seed_value : int\n        Seed for random number generation.\n\n    Returns\n    -------\n    jax.random.KeyArray\n        New PRNG key.\n    \"\"\"\n    return jr.PRNGKey(seed_value)\n</code></pre>"},{"location":"reference/utils/#psyphy.utils.rng.split","title":"split","text":"<pre><code>split(key: KeyArray, num: int = 2) -&gt; Tuple[KeyArray, ...]\n</code></pre> <p>Split a PRNG key into multiple independent keys.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>KeyArray</code> <p>RNG key to split.</p> required <code>num</code> <code>int</code> <p>Number of new keys to return.</p> <code>2</code> <p>Returns:</p> Type Description <code>tuple of jax.random.KeyArray</code> <p>Independent new PRNG keys.</p> Source code in <code>src/psyphy/utils/rng.py</code> <pre><code>def split(key: jax.random.KeyArray, num: int = 2) -&gt; Tuple[jax.random.KeyArray, ...]:\n    \"\"\"\n    Split a PRNG key into multiple independent keys.\n\n    Parameters\n    ----------\n    key : jax.random.KeyArray\n        RNG key to split.\n    num : int, default=2\n        Number of new keys to return.\n\n    Returns\n    -------\n    tuple of jax.random.KeyArray\n        Independent new PRNG keys.\n    \"\"\"\n    return jr.split(key, num=num)\n</code></pre>"},{"location":"reference/utils/#math","title":"Math","text":""},{"location":"reference/utils/#psyphy.utils.math","title":"math","text":"math.py <p>Math utilities for psyphy.</p> <p>Includes: - chebyshev_basis : compute Chebyshev polynomial basis. - mahalanobis_distance : discriminability metric used in WPPM MVP. - rbf_kernel : kernel function, useful in Full WPPM mode covariance priors.</p> <p>All functions use JAX (jax.numpy) for compatibility with autodiff.</p> Notes <ul> <li>math.chebyshev_basis is relevant when implementing Full WPPM mode,   where covariance fields are expressed in a basis expansion.</li> <li>math.mahalanobis_distance is directly used in WPPM MVP discriminability.</li> <li>math.rbf_kernel is a placeholder for Gaussian-process-style covariance priors.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; from psyphy.utils import math\n&gt;&gt;&gt; x = jnp.linspace(-1, 1, 5)\n&gt;&gt;&gt; math.chebyshev_basis(x, degree=3).shape\n(5, 4)\n</code></pre> <p>Functions:</p> Name Description <code>chebyshev_basis</code> <p>Construct the Chebyshev polynomial basis matrix T_0..T_degree evaluated at x.</p> <code>mahalanobis_distance</code> <p>Compute squared Mahalanobis distance between x and mean.</p> <code>rbf_kernel</code> <p>Radial Basis Function (RBF) kernel between two sets of points.</p>"},{"location":"reference/utils/#psyphy.utils.math.chebyshev_basis","title":"chebyshev_basis","text":"<pre><code>chebyshev_basis(x: ndarray, degree: int) -&gt; ndarray\n</code></pre> <p>Construct the Chebyshev polynomial basis matrix T_0..T_degree evaluated at x.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Input points of shape (N,). For best numerical properties, values should lie in [-1, 1].</p> required <code>degree</code> <code>int</code> <p>Maximum polynomial degree (&gt;= 0). The output includes columns for T_0 through T_degree.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of shape (N, degree + 1) where column j contains T_j(x).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>degree</code> is negative or <code>x</code> is not 1-D.</p> Notes <p>Uses the three-term recurrence:     T_0(x) = 1     T_1(x) = x     T_{n+1}(x) = 2 x T_n(x) - T_{n-1}(x) The Chebyshev polynomials are orthogonal on [-1, 1] with weight (1 / sqrt(1 - x^2)).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; x = jnp.linspace(-1, 1, 5)\n&gt;&gt;&gt; B = chebyshev_basis(x, degree=3)  # columns: T0, T1, T2, T3\n</code></pre> Source code in <code>src/psyphy/utils/math.py</code> <pre><code>def chebyshev_basis(x: jnp.ndarray, degree: int) -&gt; jnp.ndarray:\n    \"\"\"\n    Construct the Chebyshev polynomial basis matrix T_0..T_degree evaluated at x.\n\n    Parameters\n    ----------\n    x : jnp.ndarray\n        Input points of shape (N,). For best numerical properties, values should lie in [-1, 1].\n    degree : int\n        Maximum polynomial degree (&gt;= 0). The output includes columns for T_0 through T_degree.\n\n    Returns\n    -------\n    jnp.ndarray\n        Array of shape (N, degree + 1) where column j contains T_j(x).\n\n    Raises\n    ------\n    ValueError\n        If `degree` is negative or `x` is not 1-D.\n\n    Notes\n    -----\n    Uses the three-term recurrence:\n        T_0(x) = 1\n        T_1(x) = x\n        T_{n+1}(x) = 2 x T_n(x) - T_{n-1}(x)\n    The Chebyshev polynomials are orthogonal on [-1, 1] with weight (1 / sqrt(1 - x^2)).\n\n    Examples\n    --------\n    &gt;&gt;&gt; import jax.numpy as jnp\n    &gt;&gt;&gt; x = jnp.linspace(-1, 1, 5)\n    &gt;&gt;&gt; B = chebyshev_basis(x, degree=3)  # columns: T0, T1, T2, T3\n    \"\"\"\n    if degree &lt; 0:\n        raise ValueError(\"degree must be &gt;= 0\")\n    if x.ndim != 1:\n        raise ValueError(\"x must be 1-D (shape (N,))\")\n\n    # Ensure a floating dtype (Chebyshev recurrences are polynomial in x)\n    x = x.astype(jnp.result_type(x, 0.0))\n\n    N = x.shape[0]\n\n    # Handle small degrees explicitly.\n    if degree == 0:\n        return jnp.ones((N, 1), dtype=x.dtype)\n    if degree == 1:\n        return jnp.stack([jnp.ones_like(x), x], axis=1)\n\n    # Initialize T0 and T1 columns.\n    T0 = jnp.ones_like(x)\n    T1 = x\n\n    # Scan to generate T2..T_degree in a JIT-friendly way (avoids Python-side loops).\n    def step(carry, _):\n        # compute next Chebyshev polynomial\n        Tm1, Tm = carry\n        Tnext = 2.0 * x * Tm - Tm1\n        return (Tm, Tnext), Tnext # new carry, plus an output to collect\n\n    # Jax friendly loop\n    (final_Tm1_ignored, final_Tm_ignored), Ts = lax.scan(step, (T0, T1), xs=None, length=degree - 1)\n    # Ts has shape (degree-1, N) and holds [T2, T3, ..., T_degree]\n    B = jnp.concatenate(\n        [T0[:, None], T1[:, None], jnp.swapaxes(Ts, 0, 1)],\n        axis=1\n    )\n    return B\n</code></pre>"},{"location":"reference/utils/#psyphy.utils.math.mahalanobis_distance","title":"mahalanobis_distance","text":"<pre><code>mahalanobis_distance(\n    x: ndarray, mean: ndarray, cov_inv: ndarray\n) -&gt; ndarray\n</code></pre> <p>Compute squared Mahalanobis distance between x and mean.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Data vector, shape (D,).</p> required <code>mean</code> <code>ndarray</code> <p>Mean vector, shape (D,).</p> required <code>cov_inv</code> <code>ndarray</code> <p>Inverse covariance matrix, shape (D, D).</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Scalar squared Mahalanobis distance.</p> Notes <ul> <li>Formula: d^2 = (x - mean)^T \u03a3^{-1} (x - mean)</li> <li>Used in WPPM discriminability calculations.</li> </ul> Source code in <code>src/psyphy/utils/math.py</code> <pre><code>def mahalanobis_distance(x: jnp.ndarray, mean: jnp.ndarray, cov_inv: jnp.ndarray) -&gt; jnp.ndarray:\n    \"\"\"\n    Compute squared Mahalanobis distance between x and mean.\n\n    Parameters\n    ----------\n    x : jnp.ndarray\n        Data vector, shape (D,).\n    mean : jnp.ndarray\n        Mean vector, shape (D,).\n    cov_inv : jnp.ndarray\n        Inverse covariance matrix, shape (D, D).\n\n    Returns\n    -------\n    jnp.ndarray\n        Scalar squared Mahalanobis distance.\n\n    Notes\n    -----\n    - Formula: d^2 = (x - mean)^T \u03a3^{-1} (x - mean)\n    - Used in WPPM discriminability calculations.\n    \"\"\"\n    delta = x - mean\n    return jnp.dot(delta, cov_inv @ delta)\n</code></pre>"},{"location":"reference/utils/#psyphy.utils.math.rbf_kernel","title":"rbf_kernel","text":"<pre><code>rbf_kernel(\n    x1: ndarray, x2: ndarray, lengthscale: float = 1.0\n) -&gt; ndarray\n</code></pre> <p>Radial Basis Function (RBF) kernel between two sets of points.</p> <p>Parameters:</p> Name Type Description Default <code>x1</code> <code>ndarray</code> <p>First set of points, shape (N, D).</p> required <code>x2</code> <code>ndarray</code> <p>Second set of points, shape (M, D).</p> required <code>lengthscale</code> <code>float</code> <p>Length-scale parameter controlling smoothness.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Kernel matrix of shape (N, M).</p> Notes <ul> <li>RBF kernel: k(x, x') = exp(-||x - x'||^2 / (2 * lengthscale^2))</li> <li>Default used for Gaussian processes for smooth covariance priors in Full WPPM mode.</li> </ul> Source code in <code>src/psyphy/utils/math.py</code> <pre><code>def rbf_kernel(x1: jnp.ndarray, x2: jnp.ndarray, lengthscale: float = 1.0) -&gt; jnp.ndarray:\n    \"\"\"\n    Radial Basis Function (RBF) kernel between two sets of points.\n\n\n    Parameters\n    ----------\n    x1 : jnp.ndarray\n        First set of points, shape (N, D).\n    x2 : jnp.ndarray\n        Second set of points, shape (M, D).\n    lengthscale : float, default=1.0\n        Length-scale parameter controlling smoothness.\n\n    Returns\n    -------\n    jnp.ndarray\n        Kernel matrix of shape (N, M).\n\n    Notes\n    -----\n    - RBF kernel: k(x, x') = exp(-||x - x'||^2 / (2 * lengthscale^2))\n    - Default used for Gaussian processes for smooth covariance priors in Full WPPM mode.\n    \"\"\"\n    sqdist = jnp.sum((x1[:, None, :] - x2[None, :, :])**2, axis=-1)\n    return jnp.exp(-0.5 * sqdist / (lengthscale**2))\n</code></pre>"},{"location":"reference/utils/#stimulus-candidates","title":"Stimulus candidates","text":""},{"location":"reference/utils/#psyphy.utils.candidates","title":"candidates","text":"candidates.py <p>Utilities for generating candidate stimulus pools.</p> Definition <p>A candidate pool is the set of all possible (reference, probe) pairs that an adaptive placement strategy may select from.</p> Separation of concerns <ul> <li>Candidate generation (this module) defines what stimuli are possible.</li> <li>Trial placement strategies (e.g., GreedyMAPPlacement, InfoGainPlacement)   define which of those candidates to present next.</li> </ul> Why this matters <ul> <li>Researchers: think of the candidate pool as the \"menu\" of allowable trials.</li> <li>Developers: placement strategies should not generate candidates   but only select from a given pool.</li> </ul> MVP implementation <ul> <li>Grid-based candidates (probes on circles around a reference).</li> <li>Sobol sequence candidates (low-discrepancy exploration).</li> <li>Custom user-defined candidate pools.</li> </ul> Full WPPM mode <ul> <li>Candidate generation could adaptively refine itself based on posterior   uncertainty (e.g., dynamic grids).</li> <li>Candidate pools could be constrained by device gamut or subject-specific calibration.</li> </ul> <p>Functions:</p> Name Description <code>custom_candidates</code> <p>Wrap a user-defined list of probes into candidate pairs.</p> <code>grid_candidates</code> <p>Generate grid-based candidate probes around a reference.</p> <code>sobol_candidates</code> <p>Generate Sobol quasi-random candidates within bounds.</p> <p>Attributes:</p> Name Type Description <code>Stimulus</code>"},{"location":"reference/utils/#psyphy.utils.candidates.Stimulus","title":"Stimulus","text":"<pre><code>Stimulus = Tuple[ndarray, ndarray]\n</code></pre>"},{"location":"reference/utils/#psyphy.utils.candidates.custom_candidates","title":"custom_candidates","text":"<pre><code>custom_candidates(\n    reference: ndarray, probe_list: List[ndarray]\n) -&gt; List[Stimulus]\n</code></pre> <p>Wrap a user-defined list of probes into candidate pairs.</p> <p>Parameters:</p> Name Type Description Default <code>reference</code> <code>(ndarray, shape(D))</code> <p>Reference stimulus.</p> required <code>probe_list</code> <code>list of jnp.ndarray</code> <p>Explicitly chosen probe vectors.</p> required <p>Returns:</p> Type Description <code>list of Stimulus</code> <p>Candidate (reference, probe) pairs.</p> Notes <ul> <li>Useful when hardware constraints (monitor gamut, auditory frequencies)   restrict the set of valid stimuli.</li> <li>Full WPPM mode: this pool could be pruned or expanded dynamically   depending on posterior fit quality.</li> </ul> Source code in <code>src/psyphy/utils/candidates.py</code> <pre><code>def custom_candidates(reference: jnp.ndarray, probe_list: List[jnp.ndarray]) -&gt; List[Stimulus]:\n    \"\"\"\n    Wrap a user-defined list of probes into candidate pairs.\n\n    Parameters\n    ----------\n    reference : jnp.ndarray, shape (D,)\n        Reference stimulus.\n    probe_list : list of jnp.ndarray\n        Explicitly chosen probe vectors.\n\n    Returns\n    -------\n    list of Stimulus\n        Candidate (reference, probe) pairs.\n\n    Notes\n    -----\n    - Useful when hardware constraints (monitor gamut, auditory frequencies)\n      restrict the set of valid stimuli.\n    - Full WPPM mode: this pool could be pruned or expanded dynamically\n      depending on posterior fit quality.\n    \"\"\"\n    return [(reference, probe) for probe in probe_list]\n</code></pre>"},{"location":"reference/utils/#psyphy.utils.candidates.grid_candidates","title":"grid_candidates","text":"<pre><code>grid_candidates(\n    reference: ndarray,\n    radii: List[float],\n    directions: int = 16,\n) -&gt; List[Stimulus]\n</code></pre> <p>Generate grid-based candidate probes around a reference.</p> <p>Parameters:</p> Name Type Description Default <code>reference</code> <code>(ndarray, shape(D))</code> <p>Reference stimulus in model space.</p> required <code>radii</code> <code>list of float</code> <p>Distances from reference to probe.</p> required <code>directions</code> <code>int</code> <p>Number of angular directions.</p> <code>16</code> <p>Returns:</p> Type Description <code>list of Stimulus</code> <p>Candidate (reference, probe) pairs.</p> Notes <ul> <li>MVP: probes lie on concentric circles around reference.</li> <li>Full WPPM mode: could adaptively refine grid around regions of   high posterior uncertainty.</li> </ul> Source code in <code>src/psyphy/utils/candidates.py</code> <pre><code>def grid_candidates(reference: jnp.ndarray, radii: List[float], directions: int = 16) -&gt; List[Stimulus]:\n    \"\"\"\n    Generate grid-based candidate probes around a reference.\n\n    Parameters\n    ----------\n    reference : jnp.ndarray, shape (D,)\n        Reference stimulus in model space.\n    radii : list of float\n        Distances from reference to probe.\n    directions : int, default=16\n        Number of angular directions.\n\n    Returns\n    -------\n    list of Stimulus\n        Candidate (reference, probe) pairs.\n\n    Notes\n    -----\n    - MVP: probes lie on concentric circles around reference.\n    - Full WPPM mode: could adaptively refine grid around regions of\n      high posterior uncertainty.\n    \"\"\"\n    candidates = []\n    angles = jnp.linspace(0, 2 * jnp.pi, directions, endpoint=False)\n    for r in radii:\n        probes = [reference + r * jnp.array([jnp.cos(a), jnp.sin(a)]) for a in angles]\n        candidates.extend([(reference, p) for p in probes])\n    return candidates\n</code></pre>"},{"location":"reference/utils/#psyphy.utils.candidates.sobol_candidates","title":"sobol_candidates","text":"<pre><code>sobol_candidates(\n    reference: ndarray,\n    n: int,\n    bounds: List[Tuple[float, float]],\n    seed: int = 0,\n) -&gt; List[Stimulus]\n</code></pre> <p>Generate Sobol quasi-random candidates within bounds.</p> <p>Parameters:</p> Name Type Description Default <code>reference</code> <code>(ndarray, shape(D))</code> <p>Reference stimulus.</p> required <code>n</code> <code>int</code> <p>Number of candidates to generate.</p> required <code>bounds</code> <code>list of (low, high)</code> <p>Bounds per dimension.</p> required <code>seed</code> <code>int</code> <p>Random seed.</p> <code>0</code> <p>Returns:</p> Type Description <code>list of Stimulus</code> <p>Candidate (reference, probe) pairs.</p> Notes <ul> <li>MVP: uniform coverage of space using low-discrepancy Sobol sequence.</li> <li>Full WPPM mode: Sobol could be used for initialization,   then hand off to posterior-aware strategies.</li> </ul> Source code in <code>src/psyphy/utils/candidates.py</code> <pre><code>def sobol_candidates(reference: jnp.ndarray, n: int, bounds: List[Tuple[float, float]], seed: int = 0) -&gt; List[Stimulus]:\n    \"\"\"\n    Generate Sobol quasi-random candidates within bounds.\n\n    Parameters\n    ----------\n    reference : jnp.ndarray, shape (D,)\n        Reference stimulus.\n    n : int\n        Number of candidates to generate.\n    bounds : list of (low, high)\n        Bounds per dimension.\n    seed : int, default=0\n        Random seed.\n\n    Returns\n    -------\n    list of Stimulus\n        Candidate (reference, probe) pairs.\n\n    Notes\n    -----\n    - MVP: uniform coverage of space using low-discrepancy Sobol sequence.\n    - Full WPPM mode: Sobol could be used for initialization,\n      then hand off to posterior-aware strategies.\n    \"\"\"\n    from scipy.stats.qmc import Sobol\n    dim = len(bounds)\n    engine = Sobol(d=dim, scramble=True, seed=seed)\n    raw = engine.random(n)\n    scaled = [low + (high - low) * raw[:, i] for i, (low, high) in enumerate(bounds)]\n    probes = np.stack(scaled, axis=-1)\n    return [(reference, jnp.array(p)) for p in probes]\n</code></pre>"}]}